
@article{fu_sample-size_2021,
	title = {Sample-size determination for the {Bayesian} t test and {Welch}‚Äôs test using the approximate adjusted fractional {Bayes} factor},
	volume = {53},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-020-01408-1},
	doi = {10.3758/s13428-020-01408-1},
	abstract = {When two independent means Œº1 and Œº2 are compared, H0 : Œº1 = Œº2, H1 : Œº1‚â†Œº2, and H2 : Œº1 {\textgreater} Œº2 are the hypotheses of interest. This paper introduces the R package SSDbain, which can be used to determine the sample size needed to evaluate these hypotheses using the approximate adjusted fractional Bayes factor (AAFBF) implemented in the R package bain. Both the Bayesian t test and the Bayesian Welch‚Äôs test are available in this R package. The sample size required will be calculated such that the probability that the Bayes factor is larger than a threshold value is at least Œ∑ if either the null or alternative hypothesis is true. Using the R package SSDbain and/or the tables provided in this paper, psychological researchers can easily determine the required sample size for their experiments.},
	language = {en},
	number = {1},
	urldate = {2024-11-06},
	journal = {Behavior Research Methods},
	author = {Fu, Qianrao and Hoijtink, Herbert and Moerbeek, Mirjam},
	month = feb,
	year = {2021},
	keywords = {Bayes factor, Bayesian Welch‚Äôs test, Bayesian t test, SSDbain, Sample-size determination},
	pages = {139--152},
}

@article{voelkl_irise_2024,
	title = {The {iRISE} {Reproducibility} {Glossary}},
	url = {https://osf.io/ewybt},
	doi = {https://doi.org/10.17605/OSF.IO/BR9SP},
	abstract = {Presented by OSF},
	language = {eng},
	urldate = {2024-08-05},
	author = {Voelkl, Bernhard and Heyard, Rachel and Fanelli, Daniele and Wever, Kimberley and Held, Leonhard and Maniadis, Zacharias and McCann, Sarah and Zellers, Stephanie and W√ºrbel, Hanno},
	month = jun,
	year = {2024},
	note = {Publisher: Open Science Framework},
}

@article{held_assessment_2024,
	title = {The assessment of replicability using the sum of p-values},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.240149},
	doi = {10.1098/rsos.240149},
	abstract = {Statistical significance of both the original and the replication study is a commonly used criterion to assess replication attempts, also known as the two-trials rule in drug development. However, replication studies are sometimes conducted although the original study is non-significant, in which case Type-I error rate control across both studies is no longer guaranteed. We propose an alternative method to assess replicability using the sum of ùëùp-values from the two studies. The approach provides a combined ùëùp-value and can be calibrated to control the overall Type-I error rate at the same level as the two-trials rule but allows for replication success even if the original study is non-significant. The unweighted version requires a less restrictive level of significance at replication if the original study is already convincing which facilitates sample size reductions of up to 10\%. Downweighting the original study accounts for possible bias and requires a more stringent significance level and larger sample sizes at replication. Data from four large-scale replication projects are used to illustrate and compare the proposed method with the two-trials rule, meta-analysis and Fisher‚Äôs combination method.},
	number = {8},
	urldate = {2024-10-02},
	journal = {Royal Society Open Science},
	author = {Held, Leonhard and Pawel, Samuel and Micheloud, Charlotte},
	month = aug,
	year = {2024},
	note = {Publisher: Royal Society},
	keywords = {Edgington‚Äôs method, Type-I error rate, p-values, replication studies, sample size planning, two-trials rule},
	pages = {240149},
}

@article{micheloud_assessing_2023,
	title = {Assessing replicability with the sceptical \$p\$‚Äêvalue: {Type}‚Äê{I} error control and sample size planning},
	volume = {77},
	issn = {0039-0402, 1467-9574},
	shorttitle = {Assessing replicability with the sceptical p\$\$ p \$\$‚Äêvalue},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/stan.12312},
	doi = {10.1111/stan.12312},
	abstract = {We study a statistical framework for replicability based on a recently proposed quantitative measure of replication success, the sceptical ‚Äêvalue. A recalibration is proposed to obtain exact overall Type‚ÄêI error control if the effect is null in both studies and additional bounds on the partial and conditional Type‚ÄêI error rate, which represent the case where only one study has a null effect. The approach avoids the double dichotomization for significance of the two‚Äêtrials rule and has larger project power to detect existing effects over both studies in combination. It can also be used for power calculations and requires a smaller replication sample size than the two‚Äêtrials rule for already convincing original studies. We illustrate the performance of the proposed methodology in an application to data from the Experimental Economics Replication Project.},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Statistica Neerlandica},
	author = {Micheloud, Charlotte and Balabdaoui, Fadoua and Held, Leonhard},
	month = nov,
	year = {2023},
	pages = {573--591},
}

@article{anderson_sample_2022,
	title = {Sample size planning for replication studies: {The} devil is in the design},
	issn = {1939-1463},
	shorttitle = {Sample size planning for replication studies},
	doi = {10.1037/met0000520},
	abstract = {Replication is central to scientific progress. Because of widely reported replication failures, replication has received increased attention in psychology, sociology, education, management, and related fields in recent years. Replication studies have generally been assessed dichotomously, designated either a ‚Äúsuccess‚Äù or ‚Äúfailure‚Äù based entirely on the outcome of a null hypothesis significance test (i.e., pp {\textgreater} .05, respectively). However, alternative definitions of success depend on researchers‚Äô goals for the replication. Previous work on alternative definitions for success has focused on the analysis phase of replication. However, the design of the replication is also important, as emphasized with the adage, ‚Äúan ounce of prevention is better than a pound of cure.‚Äù One critical component of design often ignored or oversimplified in replication studies is sample size planning, indeed, the details here are crucial. Sample size planning for replication studies should correspond to the method by which success will be evaluated. Researchers have received little guidance, some of which is misguided, on sample size planning for replication goals other than the aforementioned dichotomous null hypothesis significance testing approach. In this article, we describe four different replication goals. Then, we formalize sample size planning methods for each of the four goals. This article aims to provide clarity on the procedures for sample size planning for each goal, with examples and syntax provided to show how each procedure can be used in practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	journal = {Psychological Methods},
	author = {Anderson, Samantha F. and Kelley, Ken},
	year = {2022},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Experimental Design, Experimental Replication, Null Hypothesis Testing, Prevention, Psychology Education, Sample Size, Sociology, Statistical Estimation, Statistical Power},
	pages = {No Pagination Specified--No Pagination Specified},
}

@article{xu_epistemic_2022,
	title = {Epistemic diversity and cross-cultural comparative research: ontology, challenges, and outcomes},
	volume = {20},
	issn = {1476-7724},
	shorttitle = {Epistemic diversity and cross-cultural comparative research},
	url = {https://www.tandfonline.com/doi/full/10.1080/14767724.2021.1932438},
	doi = {10.1080/14767724.2021.1932438},
	number = {1},
	urldate = {2024-09-22},
	journal = {Globalisation, Societies and Education},
	author = {Xu, Xin},
	month = jan,
	year = {2022},
	note = {Publisher: Routledge},
	keywords = {Chinese higher education, Chinese tradition, Epistemic diversity, Western tradition, cross-cultural comparison, global research},
	pages = {36--48},
}

@article{chang_is_2022,
	title = {Is {Economics} {Research} {Replicable}? {Sixty} {Published} {Papers} {From} {Thirteen} {Journals} {Say} ‚Äú{Often} {Not}‚Äù},
	volume = {11},
	issn = {2164-5744, 2164-5760},
	shorttitle = {Is {Economics} {Research} {Replicable}?},
	url = {http://www.nowpublishers.com/article/Details/CFR-0053},
	doi = {10.1561/104.00000053},
	language = {en},
	number = {1},
	urldate = {2024-02-11},
	journal = {Critical Finance Review},
	author = {Chang, Andrew C and Li, Phillip},
	year = {2022},
	pages = {185--206},
}

@article{bland_statistical_1986,
	title = {Statistical methods for assessing agreement between two methods of clinical measurement},
	volume = {327},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673686908378},
	doi = {10.1016/S0140-6736(86)90837-8},
	language = {en},
	number = {8476},
	urldate = {2024-08-08},
	journal = {The Lancet},
	author = {Bland, Martin J. and Altman, DouglasG.},
	month = feb,
	year = {1986},
	pages = {307--310},
}

@article{brandt_replication_2014,
	title = {The {Replication} {Recipe}: {What} makes for a convincing replication?},
	volume = {50},
	issn = {0022-1031},
	shorttitle = {The {Replication} {Recipe}},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103113001819},
	doi = {10.1016/j.jesp.2013.10.005},
	abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
	urldate = {2024-09-16},
	journal = {Journal of Experimental Social Psychology},
	author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and Giner-Sorolla, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and van 't Veer, Anna},
	month = jan,
	year = {2014},
	keywords = {Pre-registration, Replication, Research method, Solid Science, Statistical power},
	pages = {217--224},
}

@article{mathur_new_2019,
	title = {New metrics for meta-analyses of heterogeneous effects},
	volume = {38},
	issn = {1097-0258},
	doi = {10.1002/sim.8057},
	abstract = {We provide two simple metrics that could be reported routinely in random-effects meta-analyses to convey evidence strength for scientifically meaningful effects under effect heterogeneity (ie, a nonzero estimated variance of the true effect distribution). First, given a chosen threshold of meaningful effect size, meta-analyses could report the estimated proportion of true effect sizes above this threshold. Second, meta-analyses could estimate the proportion of effect sizes below a second, possibly symmetric, threshold in the opposite direction from the estimated mean. These metrics could help identify if (1) there are few effects of scientifically meaningful size despite a "statistically significant" pooled point estimate, (2) there are some large effects despite an apparently null point estimate, or (3) strong effects in the direction opposite the pooled estimate also regularly occur (and thus, potential effect modifiers should be examined). These metrics should be presented with confidence intervals, which can be obtained analytically or, under weaker assumptions, using bias-corrected and accelerated bootstrapping. Additionally, these metrics inform relative comparison of evidence strength across related meta-analyses. We illustrate with applied examples and provide an R function to compute the metrics and confidence intervals.},
	language = {eng},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	month = apr,
	year = {2019},
	pmid = {30513552},
	pmcid = {PMC6519385},
	keywords = {Algorithms, Benchmarking, Bias, Meta-Analysis as Topic, Sample Size, effect sizes, heterogeneity, reporting},
	pages = {1336--1342},
}

@article{held_reversebayes_2022,
	title = {Reverse‚Äê{Bayes} methods for evidence assessment and research synthesis},
	volume = {13},
	issn = {1759-2879, 1759-2887},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1538},
	doi = {10.1002/jrsm.1538},
	abstract = {Abstract
            It is now widely accepted that the standard inferential toolkit used by the scientific research community‚Äînull‚Äêhypothesis significance testing (NHST)‚Äîis not fit for purpose. Yet despite the threat posed to the scientific enterprise, there is no agreement concerning alternative approaches for evidence assessment. This lack of consensus reflects long‚Äêstanding issues concerning Bayesian methods, the principal alternative to NHST. We report on recent work that builds on an approach to inference put forward over 70‚Äâyears ago to address the well‚Äêknown ‚ÄúProblem of Priors‚Äù in Bayesian analysis, by reversing the conventional prior‚Äêlikelihood‚Äêposterior (‚Äúforward‚Äù) use of Bayes' theorem. Such Reverse‚ÄêBayes analysis allows priors to be deduced from the likelihood by requiring that the posterior achieve a specified level of credibility. We summarise the technical underpinning of this approach, and show how it opens up new approaches to common inferential challenges, such as assessing the credibility of scientific findings, setting them in appropriate context, estimating the probability of successful replications, and extracting more insight from NHST while reducing the risk of misinterpretation. We argue that Reverse‚ÄêBayes methods have a key role to play in making Bayesian methods more accessible and attractive for evidence assessment and research synthesis. As a running example we consider a recently published meta‚Äêanalysis from several randomised controlled trials (RCTs) investigating the association between corticosteroids and mortality in hospitalised patients with COVID‚Äê19.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Research Synthesis Methods},
	author = {Held, Leonhard and Matthews, Robert and Ott, Manuela and Pawel, Samuel},
	month = may,
	year = {2022},
	pages = {295--314},
}

@article{rosenthal_replication_1990,
	title = {Replication in behavioral research},
	volume = {5},
	issn = {0886-1641},
	abstract = {Addresses evaluation of the importance of 1 or more replications (RPCs) and how to define the success of RPCs. Variables affecting the assessment of whether an RPC is important include when, how, and by whom the RPC was conducted. Conducting RPCs in batteries varying in degree of similarity to the original study may tell more about the external validity of the result being replicated. A useful view of RPC focuses on effect size as the more important summary statistic of a study and evaluates the success of an RPC in a continuous fashion. Concepts of a successful RPC are presented, some metrics of RPC success are described, and suggestions are offered as to what should be reported in an RPC study. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Journal of Social Behavior \& Personality},
	author = {Rosenthal, Robert},
	year = {1990},
	note = {Place: US
Publisher: Select Press},
	keywords = {Experimental Replication, Methodology, Social Sciences},
	pages = {1--30},
}

@article{walsh_statistical_2014,
	title = {The statistical significance of randomized controlled trial results is frequently fragile: a case for a {Fragility} {Index}},
	volume = {67},
	issn = {0895-4356, 1878-5921},
	shorttitle = {The statistical significance of randomized controlled trial results is frequently fragile},
	url = {https://www.jclinepi.com/article/S0895-4356(13)00466-6/fulltext},
	doi = {10.1016/j.jclinepi.2013.10.019},
	language = {English},
	number = {6},
	urldate = {2024-09-15},
	journal = {Journal of Clinical Epidemiology},
	author = {Walsh, Michael and Srinathan, Sadeesh K. and McAuley, Daniel F. and Mrkobrada, Marko and Levine, Oren and Ribic, Christine and Molnar, Amber O. and Dattani, Neil D. and Burke, Andrew and Guyatt, Gordon and Thabane, Lehana and Walter, Stephen D. and Pogue, Janice and Devereaux, P. J.},
	month = jun,
	year = {2014},
	pmid = {24508144},
	note = {Publisher: Elsevier},
	keywords = {Lost to follow-up, Randomized controlled trials, Research methodology},
	pages = {622--628},
}

@article{manolov_assessing_2020,
	title = {Assessing {Consistency} in {Single}-{Case} {Data} {Features} {Using} {Modified} {Brinley} {Plots}},
	volume = {46},
	issn = {0145-4455, 1552-4167},
	url = {http://journals.sagepub.com/doi/10.1177/0145445520982969},
	doi = {10.1177/0145445520982969},
	abstract = {The current text deals with the assessment of consistency of data features from experimentally similar phases and consistency of effects in single-case experimental designs. Although consistency is frequently mentioned as a critical feature, few quantifications have been proposed so far: namely, under the acronyms CONDAP (consistency of data patterns in similar phases) and CONEFF (consistency of effects). Whereas CONDAP allows assessing the consistency of data patterns, the proposals made here focus on the consistency of data features such as level, trend, and variability, as represented by summary measures (mean, ordinary least squares slope, and standard deviation, respectively). The assessment of consistency of effect is also made in terms of these three data features, while also including the study of the consistency of an immediate effect (if expected). The summary measures are represented as points on a modified Brinley plot and their similarity is assessed via quantifications of distance. Both absolute and relative measures of consistency are proposed: the former expressed in the same measurement units as the outcome variable and the latter as a percentage. Illustrations with real data sets (multiple baseline, ABAB, and alternating treatments designs) show the wide applicability of the proposals. We developed a user-friendly website to offer both the graphical representations and the quantifications.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Behavior Modification},
	author = {Manolov, Rumen and Tanious, Ren√©},
	month = may,
	year = {2020},
	pages = {581--627},
}

@article{yang_estimating_2020,
	title = {Estimating the deep replicability of scientific findings using human and artificial intelligence},
	volume = {117},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1909046117},
	doi = {10.1073/pnas.1909046117},
	abstract = {Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study‚Äôs replicability. Here, we trained an artificial intelligence model to estimate a paper‚Äôs replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model‚Äôs generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model‚Äôs predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like ‚Äúremarkable‚Äù or ‚Äúunexpected.‚Äù We did find that the model‚Äôs accuracy is higher when trained on a paper‚Äôs text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications‚Äîa task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
	number = {20},
	urldate = {2024-09-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
	month = may,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {10762--10768},
}

@article{lakens_equivalence_2018,
	title = {Equivalence {Testing} for {Psychological} {Research}: {A} {Tutorial}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Equivalence {Testing} for {Psychological} {Research}},
	url = {https://doi.org/10.1177/2515245918770963},
	doi = {10.1177/2515245918770963},
	abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
	number = {2},
	urldate = {2024-09-13},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Dani√´l and Scheel, Anne M. and Isager, Peder M.},
	month = jun,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {259--269},
}

@article{cumming_confidence_2006,
	title = {Confidence intervals and replication: where will the next mean fall?},
	volume = {11},
	issn = {1082-989X},
	shorttitle = {Confidence intervals and replication},
	doi = {10.1037/1082-989X.11.3.217},
	abstract = {Confidence intervals (CIs) give information about replication, but many researchers have misconceptions about this information. One problem is that the percentage of future replication means captured by a particular CI varies markedly, depending on where in relation to the population mean that CI falls. The authors investigated the distribution of this percentage for varsigma known and unknown, for various sample sizes, and for robust CIs. The distribution has strong negative skew: Most 95\% CIs will capture around 90\% or more of replication means, but some will capture a much lower proportion. On average, a 95\% CI will include just 83.4\% of future replication means. The authors present figures designed to assist understanding of what CIs say about replication, and they also extend the discussion to explain how p values give information about replication.},
	language = {eng},
	number = {3},
	journal = {Psychological Methods},
	author = {Cumming, Geoff and Maillardet, Robert},
	month = sep,
	year = {2006},
	pmid = {16953701},
	keywords = {Confidence Intervals, Humans, Models, Psychological, Psychology},
	pages = {217--227},
}

@article{pawel_replication_2024,
	title = {Replication of null results: {Absence} of evidence or evidence of absence?},
	volume = {12},
	issn = {2050-084X},
	shorttitle = {Replication of null results},
	url = {https://doi.org/10.7554/eLife.92311},
	doi = {10.7554/eLife.92311},
	abstract = {In several large-scale replication projects, statistically non-significant results in both the original and the replication study have been interpreted as a ‚Äòreplication success.‚Äô Here, we discuss the logical problems with this approach: Non-significance in both studies does not ensure that the studies provide evidence for the absence of an effect and ‚Äòreplication success‚Äô can virtually always be achieved if the sample sizes are small enough. In addition, the relevant error rates are not controlled. We show how methods, such as equivalence testing and Bayes factors, can be used to adequately quantify the evidence for the absence of an effect and how they can be applied in the replication setting. Using data from the Reproducibility Project: Cancer Biology, the Experimental Philosophy Replicability Project, and the Reproducibility Project: Psychology we illustrate that many original and replication studies with ‚Äònull results‚Äô are in fact inconclusive. We conclude that it is important to also replicate studies with statistically non-significant results, but that they should be designed, analyzed, and interpreted appropriately.},
	urldate = {2024-09-13},
	journal = {eLife},
	author = {Pawel, Samuel and Heyard, Rachel and Micheloud, Charlotte and Held, Leonhard},
	editor = {Boonstra, Philip and Rodgers, Peter},
	month = may,
	year = {2024},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {Bayesian hypothesis testing, equivalence testing, meta-research, null hypothesis, replication success},
	pages = {RP92311},
}

@article{asendorpf_recommendations_2013,
	title = {Recommendations for {Increasing} {Replicability} in {Psychology}},
	volume = {27},
	issn = {0890-2070},
	url = {https://doi.org/10.1002/per.1919},
	doi = {10.1002/per.1919},
	abstract = {Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. Copyright ? 2013 John Wiley \& Sons, Ltd.},
	number = {2},
	urldate = {2024-09-13},
	journal = {European Journal of Personality},
	author = {Asendorpf, Jens B. and Conner, Mark and De Fruyt, Filip and De Houwer, Jan and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and Van Aken, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
	month = mar,
	year = {2013},
	note = {Publisher: SAGE Publications Ltd},
	pages = {108--119},
}

@misc{noauthor_recommendations_nodate,
	title = {Recommendations for {Increasing} {Replicability} in {Psychology} - {Jens} {B}. {Asendorpf}, {Mark} {Conner}, {Filip} {De} {Fruyt}, {Jan} {De} {Houwer}, {Jaap} {J}. {A}. {Denissen}, {Klaus} {Fiedler}, {Susann} {Fiedler}, {David} {C}. {Funder}, {Reinhold} {Kliegl}, {Brian} {A}. {Nosek}, {Marco} {Perugini}, {Brent} {W}. {Roberts}, {Manfred} {Schmitt}, {Marcel} {A}. {G}. {Van} {Aken}, {Hannelore} {Weber}, {Jelte} {M}. {Wicherts}, 2013},
	url = {https://journals.sagepub.com/doi/10.1002/per.1919},
	urldate = {2024-09-13},
}

@article{nieuwenhuis_erroneous_2011,
	title = {Erroneous analyses of interactions in neuroscience: a problem of significance},
	volume = {14},
	copyright = {2011 Springer Nature America, Inc.},
	issn = {1546-1726},
	shorttitle = {Erroneous analyses of interactions in neuroscience},
	url = {https://www.nature.com/articles/nn.2886},
	doi = {10.1038/nn.2886},
	abstract = {The authors analyze a large corpus of the neuroscience literature and demonstrate that nearly half of the published studies considered incorrectly compared effect sizes by comparing their significance levels.},
	language = {en},
	number = {9},
	urldate = {2024-09-13},
	journal = {Nature Neuroscience},
	author = {Nieuwenhuis, Sander and Forstmann, Birte U. and Wagenmakers, Eric-Jan},
	month = sep,
	year = {2011},
	note = {Publisher: Nature Publishing Group},
	keywords = {Statistics},
	pages = {1105--1107},
}

@article{gelman_difference_2006,
	title = {The {Difference} {Between} ‚Äú{Significant}‚Äù and ‚Äú{Not} {Significant}‚Äù is not {Itself} {Statistically} {Significant}},
	volume = {60},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313006X152649},
	doi = {10.1198/000313006X152649},
	abstract = {It is common to summarize statistical comparisons by declarations of statistical significance or nonsignificance. Here we discuss one problem with such declarations, namely that changes in statistical significance are often not themselves statistically significant. By this, we are not merely making the commonplace observation that any particular threshold is arbitrary‚Äîfor example, only a small change is required to move an estimate from a 5.1\% significance level to 4.9\%, thus moving it into statistical significance. Rather, we are pointing out that even large changes in significance levels can correspond to small, nonsignificant changes in the underlying quantities. The error we describe is conceptually different from other oft-cited problems‚Äîthat statistical significance is not the same as practical importance, that dichotomization into significant and nonsignificant results encourages the dismissal of observed differences in favor of the usually less interesting null hypothesis of no difference, and that any particular threshold for declaring significance is arbitrary. We are troubled by all of these concerns and do not intend to minimize their importance. Rather, our goal is to bring attention to this additional error of interpretation. We illustrate with a theoretical example and two applied examples. The ubiquity of this statistical error leads us to suggest that students and practitioners be made more aware that the difference between ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù is not itself statistically significant.},
	number = {4},
	urldate = {2024-09-13},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Stern, Hal},
	month = nov,
	year = {2006},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/000313006X152649},
	keywords = {Hypothesis testing, Meta-analysis, Pairwise comparison, Replication},
	pages = {328--331},
}

@article{bayarri_bayesian_2002,
	title = {Bayesian {Design} of ‚Äú{Successful}‚Äù {Replications}},
	volume = {56},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313002155},
	doi = {10.1198/000313002155},
	abstract = {Replication of experiments is commonin applied research. However, systematic studies of the goals and motivations of a ‚Äúreplication‚Äù are rare. As a consequence, there does not seem to be a precise notion of what a ‚Äúsuccess‚Äù when replicating means. This article discusses some of the possible goals for replication; this leads to different (but precise) notions of ‚Äúsuccess‚Äù when replicating. Bayesian hierarchical models allow for a flexible and explicit incorporation of the assumed relationship among the experiments. Bayesian predictive distributions are a natural tool to compute the probability of the replication being successful, and hence to design the replication so that the probability of success is high enough. Derivations are exemplified with data coming from a noncentral t distribution.},
	number = {3},
	urldate = {2024-09-13},
	journal = {The American Statistician},
	author = {Bayarri, M. J and Mayoral, A. M},
	month = aug,
	year = {2002},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/000313002155},
	keywords = {Bayesian analysis, Effect size, Hypothesis testing, Meta-analysis, Noncentral t, Predictive distributions},
	pages = {207--214},
}

@article{rouder_default_2012,
	title = {Default {Bayes} {Factors} for {Model} {Selection} in {Regression}},
	volume = {47},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2012.734737},
	doi = {10.1080/00273171.2012.734737},
	abstract = {In this article, we present a Bayes factor solution for inference in multiple regression. Bayes factors are principled measures of the relative evidence from data for various models or positions, including models that embed null hypotheses. In this regard, they may be used to state positive evidence for a lack of an effect, which is not possible in conventional significance testing. One obstacle to the adoption of Bayes factor in psychological science is a lack of guidance and software. Recently, Liang, Paulo, Molina, Clyde, and Berger (2008) developed computationally attractive default Bayes factors for multiple regression designs. We provide a web applet for convenient computation and guidance and context for use of these priors. We discuss the interpretation and advantages of the advocated Bayes factor evidence measures.},
	number = {6},
	urldate = {2024-09-13},
	journal = {Multivariate Behavioral Research},
	author = {Rouder, Jeffrey N. and Morey, Richard D.},
	month = nov,
	year = {2012},
	pmid = {26735007},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2012.734737},
	pages = {877--903},
}

@book{dear_revolutionizing_2019,
	address = {Oxford},
	edition = {Third edition},
	title = {Revolutionizing the sciences: {European} knowledge in transition, 1500-1700},
	isbn = {978-1-352-00313-0 978-1-352-00325-3},
	shorttitle = {Revolutionizing the sciences},
	language = {eng},
	publisher = {macmillan international, Higher Education},
	author = {Dear, Peter Robert},
	year = {2019},
}

@book{dear_revolutionizing_nodate,
	title = {Revolutionizing the {Sciences}},
	url = {https://www.bloomsbury.com/uk/revolutionizing-the-sciences-9781352003130/},
	abstract = {This heavily revised third edition of an award-winning text offers a keen insight into the development of scientific thought in early modern Europe. Including c‚Ä¶},
	language = {en},
	urldate = {2024-09-09},
	author = {Dear, Peter},
}

@incollection{tetens_reproducibility_2016,
	title = {Reproducibility, {Objectivity}, {Invariance}},
	copyright = {Copyright ¬© 2016 John Wiley \& Sons, Inc.},
	isbn = {978-1-118-86506-4},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118865064.ch1},
	abstract = {The independent reproducibility of experiments and their results, for instance, the determination of fundamental constants of nature, belongs to the most important conditions for the possibility of nomological empirical science. Two aspects are particularly significant: laws of nature could not be empirically tested and confirmed if experiments and measurements were not reproducible; and the independent reproducibility of experiments and measurements guarantees the objectivity of scientific results that otherwise would amount to mere subjective beliefs of individual researchers, neither comprehensible nor verifiable by others. The nomological character and the intersubjectivity of propositions in science share a common basic root, namely, the technical applicability of research, and thereby control over nature as one of the most significant goals of modern occidental science. This goal hinges on the requirement of reproducibility. The chapter also discusses which structural features of our theories actually are consequences of the demand that research results be reproducible.},
	language = {en},
	urldate = {2024-09-09},
	booktitle = {Reproducibility},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Tetens, Holm},
	year = {2016},
	doi = {10.1002/9781118865064.ch1},
	note = {Section: 1
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118865064.ch1},
	keywords = {invariance, modern occidental science, nomological empirical science, objectivity, reproducibility},
	pages = {13--20},
}

@incollection{steinle_stability_2016,
	title = {Stability and {Replication} of {Experimental} {Results}: {A} {Historical} {Perspective}},
	copyright = {Copyright ¬© 2016 John Wiley \& Sons, Inc.},
	isbn = {978-1-118-86506-4},
	shorttitle = {Stability and {Replication} of {Experimental} {Results}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118865064.ch3},
	abstract = {The debate about the replication and replicability of experimental results has a long and diverse history since the Early Modern period. This chapter presents an historical analysis which shows important aspects and differentiations of the debate that might enrich contemporary discussion. Before dealing with historical material, it is necessary to address some fundamental distinctions that are instrumental in selecting, organizing, and analyzing the material and have been proposed in the philosophy of science. A different type of variation in repetition or replication is the deliberate variation of apparatus, of materials, or of procedures. Experimenters of all times and branches have been aware of this point, and the historical development of the sciences shows a plethora of strategies to achieve credibility. Questions of replication and its use, necessity, possibility, and reliability in experimental science came into the focus of history and sociology of science in the 1980s.},
	language = {en},
	urldate = {2024-09-09},
	booktitle = {Reproducibility},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Steinle, Friedrich},
	year = {2016},
	doi = {10.1002/9781118865064.ch3},
	note = {Section: 3
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118865064.ch3},
	keywords = {Early Modern period, historical analysis, philosophy, replication, sociology, stability},
	pages = {39--63},
}

@article{pawel_replication_2024-1,
	title = {Replication of ‚Äúnull results‚Äù ‚Äì {Absence} of evidence or evidence of absence?},
	volume = {12},
	url = {https://elifesciences.org/reviewed-preprints/92311},
	doi = {10.7554/eLife.92311.2},
	abstract = {In several large-scale replication projects, statistically non-significant results in both the original and the replication study have been interpreted as a ‚Äúreplication success‚Äù. Here we discuss the logical problems with this approach: Non-significance in both studies does not ensure that the studies provide evidence for the absence of an effect and ‚Äúreplication success‚Äù can virtually always be achieved if the sample sizes are small enough. In addition, the relevant error rates are not controlled. We show how methods, such as equivalence testing and Bayes factors, can be used to adequately quantify the evidence for the absence of an effect and how they can be applied in the replication setting. Using data from the Reproducibility Project: Cancer Biology, the Experimental Philosophy Replicability Project, and the Reproducibility Project: Psychology we illustrate that many original and replication studies with ‚Äúnull results‚Äù are in fact inconclusive. We conclude that it is important to also replicate studies with statistically non-significant results, but that they should be designed, analyzed, and interpreted appropriately.},
	language = {en},
	urldate = {2024-08-26},
	journal = {eLife},
	author = {Pawel, Samuel and Heyard, Rachel and Micheloud, Charlotte and Held, Leonhard},
	month = feb,
	year = {2024},
	note = {Publisher: eLife Sciences Publications Limited},
}

@article{fraser_predicting_2023,
	title = {Predicting reliability through structured expert elicitation with the {repliCATS} ({Collaborative} {Assessments} for {Trustworthy} {Science}) process},
	volume = {18},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0274429},
	doi = {10.1371/journal.pone.0274429},
	abstract = {As replications of individual studies are resource intensive, techniques for predicting the replicability are required. We introduce the repliCATS (Collaborative Assessments for Trustworthy Science) process, a new method for eliciting expert predictions about the replicability of research. This process is a structured expert elicitation approach based on a modified Delphi technique applied to the evaluation of research claims in social and behavioural sciences. The utility of processes to predict replicability is their capacity to test scientific claims without the costs of full replication. Experimental data supports the validity of this process, with a validation study producing a classification accuracy of 84\% and an Area Under the Curve of 0.94, meeting or exceeding the accuracy of other techniques used to predict replicability. The repliCATS process provides other benefits. It is highly scalable, able to be deployed for both rapid assessment of small numbers of claims, and assessment of high volumes of claims over an extended period through an online elicitation platform, having been used to assess 3000 research claims over an 18 month period. It is available to be implemented in a range of ways and we describe one such implementation. An important advantage of the repliCATS process is that it collects qualitative data that has the potential to provide insight in understanding the limits of generalizability of scientific claims. The primary limitation of the repliCATS process is its reliance on human-derived predictions with consequent costs in terms of participant fatigue although careful design can minimise these costs. The repliCATS process has potential applications in alternative peer review and in the allocation of effort for replication studies.},
	language = {en},
	number = {1},
	urldate = {2024-08-26},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Bush, Martin and Wintle, Bonnie C. and Mody, Fallon and Smith, Eden T. and Hanea, Anca M. and Gould, Elliot and Hemming, Victoria and Hamilton, Daniel G. and Rumpff, Libby and Wilkinson, David P. and Pearson, Ross and Thorn, Felix Singleton and Ashton, Raquel and Willcox, Aaron and Gray, Charles T. and Head, Andrew and Ross, Melissa and Groenewegen, Rebecca and Marcoci, Alexandru and Vercammen, Ans and Parker, Timothy H. and Hoekstra, Rink and Nakagawa, Shinichi and Mandel, David R. and Ravenzwaaij, Don van and McBride, Marissa and Sinnott, Richard O. and Vesk, Peter and Burgman, Mark and Fidler, Fiona},
	month = jan,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Peer review, Replication studies, Research assessment, Research design, Research quality assessment, Social sciences, Workshops},
	pages = {e0274429},
}

@book{tukey_exploratory_1977,
	address = {Reading (Mass.) Menlo Park (Calif.) London [etc.]},
	series = {Addison-{Wesley} series in behavioral science},
	title = {Exploratory data analysis},
	isbn = {978-0-201-07616-5},
	language = {eng},
	publisher = {Addison-Wesley publ},
	author = {Tukey, John Wilder},
	year = {1977},
}

@article{liu_boba_2021,
	title = {Boba: {Authoring} and {Visualizing} {Multiverse} {Analyses}},
	volume = {27},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {Boba},
	url = {http://arxiv.org/abs/2007.05551},
	doi = {10.1109/TVCG.2020.3028985},
	abstract = {Multiverse analysis is an approach to data analysis in which all "reasonable" analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Boba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.},
	number = {2},
	urldate = {2024-08-23},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Yang and Kale, Alex and Althoff, Tim and Heer, Jeffrey},
	month = feb,
	year = {2021},
	note = {arXiv:2007.05551 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	pages = {1753--1763},
}

@misc{heyard_reproducibility_2023,
	title = {Reproducibility {Metrics} - {Study} {Protocol}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://osf.io/j65wb},
	doi = {10.17605/OSF.IO/7VC4Z},
	abstract = {Part of the objectives of iRISE is to better understand what metrics have been suggested and/or used to quantify different type of "Reproducibility".},
	language = {en},
	urldate = {2024-08-23},
	publisher = {OSF},
	author = {Heyard, Rachel and Pawel, Samuel and Wever, Kimberley and W√ºrbel, Hanno and Voelkl, Bernhard and Held, Leonhard},
	collaborator = {{Center For Open Science}},
	year = {2023},
	keywords = {literature review, metrics, replication success, reproducibility},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	urldate = {2024-02-13},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {aac4716},
}

@article{patil_what_2016,
	title = {What {Should} {Researchers} {Expect} {When} {They} {Replicate} {Studies}? {A} {Statistical} {View} of {Replicability} in {Psychological} {Science}},
	volume = {11},
	issn = {1745-6916},
	shorttitle = {What {Should} {Researchers} {Expect} {When} {They} {Replicate} {Studies}?},
	url = {https://www.jstor.org/stable/26358643},
	abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers' intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals‚Äîand a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
	number = {4},
	urldate = {2024-08-19},
	journal = {Perspectives on Psychological Science},
	author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
	year = {2016},
	note = {Publisher: [Association for Psychological Science, Sage Publications, Inc.]},
	pages = {539--544},
}

@article{cologna_trust_2024,
	title = {Trust in scientists and their role in society across 67 countries},
	url = {https://osf.io/6ay7s},
	doi = {10.31219/osf.io/6ay7s},
	abstract = {Scientific information is crucial for evidence-based decision-making. Public trust in science can help decision-makers act based on the best available evidence, especially during crises such as climate change or the COVID-19 pandemic. However, in recent years the epistemic authority of science has been challenged, causing concerns about low public trust in scientists. Here we interrogated these concerns with a pre-registered 67-country survey of 71,417 respondents on all inhabited continents and find that in most countries, a majority of the public trust scientists and think that scientists should be more engaged in policymaking. We further show that there is a discrepancy between the public‚Äôs perceived and desired priorities of scientific research. Moreover, we find variations between and within countries, which we explain with individual- and country-level variables, including political orientation. While these results do not show widespread lack of trust in scientists, we cannot discount the concern that lack of trust in scientists by even a small minority may affect considerations of scientific evidence in policymaking. These findings have implications for scientists and policymakers seeking to maintain and increase trust in scientists.},
	language = {en-us},
	urldate = {2024-03-18},
	author = {Cologna, Viktoria and Mede, Niels G. and Berger, Sebastian and Besley, John and Brick, Cameron and Joubert, Marina and Maibach, Edward and Mihelj, Sabina and Oreskes, Naomi and Sch√§fer, Mike S. and Linden, Dr Sander van der},
	month = jan,
	year = {2024},
	note = {Publisher: OSF},
}

@article{naudet_data_2018,
	title = {Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy: survey of studies published in {The} {BMJ} and {PLOS} {Medicine}},
	volume = {360},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {0959-8138, 1756-1833},
	shorttitle = {Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy},
	url = {https://www.bmj.com/content/360/bmj.k400},
	doi = {10.1136/bmj.k400},
	abstract = {Objectives To explore the effectiveness of data sharing by randomized controlled trials (RCTs) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes.
Design Survey of published RCTs.
Setting PubMed/Medline.
Eligibility criteria RCTs that had been submitted and published by The BMJ and PLOS Medicine subsequent to the adoption of data sharing policies by these journals.
Main outcome measure The primary outcome was data availability, defined as the eventual receipt of complete data with clear labelling. Primary outcomes were reanalyzed to assess to what extent studies were reproduced. Difficulties encountered were described.
Results 37 RCTs (21 from The BMJ and 16 from PLOS Medicine) published between 2013 and 2016 met the eligibility criteria. 17/37 (46\%, 95\% confidence interval 30\% to 62\%) satisfied the definition of data availability and 14 of the 17 (82\%, 59\% to 94\%) were fully reproduced on all their primary outcomes. Of the remaining RCTs, errors were identified in two but reached similar conclusions and one paper did not provide enough information in the Methods section to reproduce the analyses. Difficulties identified included problems in contacting corresponding authors and lack of resources on their behalf in preparing the datasets. In addition, there was a range of different data sharing practices across study groups.
Conclusions Data availability was not optimal in two journals with a strong policy for data sharing. When investigators shared data, most reanalyses largely reproduced the original results. Data sharing practices need to become more widespread and streamlined to allow meaningful reanalyses and reuse of data.
Trial registration Open Science Framework osf.io/c4zke.},
	language = {en},
	urldate = {2024-03-14},
	journal = {BMJ},
	author = {Naudet, Florian and Sakarovitch, Charlotte and Janiaud, Perrine and Cristea, Ioana and Fanelli, Daniele and Moher, David and Ioannidis, John P. A.},
	month = feb,
	year = {2018},
	pmid = {29440066},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research},
	pages = {k400},
}

@article{cheung_registered_2016,
	title = {Registered {Replication} {Report}: {Study} 1 {From} {Finkel}, {Rusbult}, {Kumashiro}, \& {Hannon} (2002)},
	volume = {11},
	issn = {1745-6916},
	shorttitle = {Registered {Replication} {Report}},
	url = {https://doi.org/10.1177/1745691616664694},
	doi = {10.1177/1745691616664694},
	abstract = {Finkel, Rusbult, Kumashiro, and Hannon (2002, Study 1) demonstrated a causal link between subjective commitment to a relationship and how people responded to hypothetical betrayals of that relationship. Participants primed to think about their commitment to their partner (high commitment) reacted to the betrayals with reduced exit and neglect responses relative to those primed to think about their independence from their partner (low commitment). The priming manipulation did not affect constructive voice and loyalty responses. Although other studies have demonstrated a correlation between subjective commitment and responses to betrayal, this study provides the only experimental evidence that inducing changes to subjective commitment can causally affect forgiveness responses. This Registered Replication Report (RRR) meta-analytically combines the results of 16 new direct replications of the original study, all of which followed a standardized, vetted, and preregistered protocol. The results showed little effect of the priming manipulation on the forgiveness outcome measures, but it also did not observe an effect of priming on subjective commitment, so the manipulation did not work as it had in the original study. We discuss possible explanations for the discrepancy between the findings from this RRR and the original study.},
	language = {en},
	number = {5},
	urldate = {2024-02-13},
	journal = {Perspectives on Psychological Science},
	author = {Cheung, I. and Campbell, L. and LeBel, E. P. and Ackerman, R. A. and Aykutoƒülu, B. and Bahn√≠k, ≈†. and Bowen, J. D. and Bredow, C. A. and Bromberg, C. and Caprariello, P. A. and Carcedo, R. J. and Carson, K. J. and Cobb, R. J. and Collins, N. L. and Corretti, C. A. and DiDonato, T. E. and Ellithorpe, C. and Fern√°ndez-Rouco, N. and Fuglestad, P. T. and Goldberg, R. M. and Golom, F. D. and G√ºndoƒüdu-Akt√ºrk, E. and Hoplock, L. B. and Houdek, P. and Kane, H. S. and Kim, J. S. and Kraus, S. and Leone, C. T. and Li, N. P. and Logan, J. M. and Millman, R. D. and Morry, M. M. and Pink, J. C. and Ritchey, T. and Root Luna, L. M. and Sinclair, H. C. and Stinson, D. A. and Sucharyna, T. A. and Tidwell, N. D. and Uysal, A. and Vranka, M. and Winczewski, L. A. and Yong, J. C.},
	month = sep,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {750--764},
}

@article{bouwmeester_registered_2017,
	title = {Registered {Replication} {Report}: {Rand}, {Greene}, and {Nowak} (2012)},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Registered {Replication} {Report}},
	url = {https://doi.org/10.1177/1745691617693624},
	doi = {10.1177/1745691617693624},
	abstract = {In an anonymous 4-person economic game, participants contributed more money to a common project (i.e., cooperated) when required to decide quickly than when forced to delay their decision (Rand, Greene \& Nowak, 2012), a pattern consistent with the social heuristics hypothesis proposed by Rand and colleagues. The results of studies using time pressure have been mixed, with some replication attempts observing similar patterns (e.g., Rand et al., 2014) and others observing null effects (e.g., Tingh√∂g et al., 2013; Verkoeijen \& Bouwmeester, 2014). This Registered Replication Report (RRR) assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate, preregistered replications of the critical conditions from Study 7 of the original article (Rand et al., 2012). The primary planned analysis used data from all participants who were randomly assigned to conditions and who met the protocol inclusion criteria (an intent-to-treat approach that included the 65.9\% of participants in the time-pressure condition and 7.5\% in the forced-delay condition who did not adhere to the time constraints), and we observed a difference in contributions of ‚àí0.37 percentage points compared with an 8.6 percentage point difference calculated from the original data. Analyzing the data as the original article did, including data only for participants who complied with the time constraints, the RRR observed a 10.37 percentage point difference in contributions compared with a 15.31 percentage point difference in the original study. In combination, the results of the intent-to-treat analysis and the compliant-only analysis are consistent with the presence of selection biases and the absence of a causal effect of time pressure on cooperation.},
	language = {en},
	number = {3},
	urldate = {2024-02-13},
	journal = {Perspectives on Psychological Science},
	author = {Bouwmeester, S. and Verkoeijen, P. P. J. L. and Aczel, B. and Barbosa, F. and B√®gue, L. and Bra√±as-Garza, P. and Chmura, T. G. H. and Cornelissen, G. and D√∏ssing, F. S. and Esp√≠n, A. M. and Evans, A. M. and Ferreira-Santos, F. and Fiedler, S. and Flegr, J. and Ghaffari, M. and Gl√∂ckner, A. and Goeschl, T. and Guo, L. and Hauser, O. P. and Hernan-Gonzalez, R. and Herrero, A. and Horne, Z. and Houdek, P. and Johannesson, M. and Koppel, L. and Kujal, P. and Laine, T. and Lohse, J. and Martins, E. C. and Mauro, C. and Mischkowski, D. and Mukherjee, S. and Myrseth, K. O. R. and Navarro-Mart√≠nez, D. and Neal, T. M. S. and Novakova, J. and Pag√†, R. and Paiva, T. O. and Palfi, B. and Piovesan, M. and Rahal, R.-M. and Salomon, E. and Srinivasan, N. and Srivastava, A. and Szaszi, B. and Szollosi, A. and Thor, K. √ò. and Tingh√∂g, G. and Trueblood, J. S. and Van Bavel, J. J. and van ‚Äòt Veer, A. E. and V√§stfj√§ll, D. and Warner, M. and Wengstr√∂m, E. and Wills, J. and Wollbrant, C. E.},
	month = may,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {527--542},
}

@article{boyce_eleven_2023,
	title = {Eleven years of student replication projects provide evidence on the correlates of replicability in psychology},
	volume = {10},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.231240},
	doi = {10.1098/rsos.231240},
	abstract = {Cumulative scientific progress requires empirical results that are robust enough to support theory construction and extension. Yet in psychology, some prominent findings have failed to replicate, and large-scale studies suggest replicability issues are widespread. The identification of predictors of replication success is limited by the difficulty of conducting large samples of independent replication experiments, however: most investigations reanalyse the same set of 
170
‚Äâ
replications
. We introduce a new dataset of 176 replications from students in a graduate-level methods course. Replication results were judged to be successful in 49\% of replications; of the 136 where effect sizes could be numerically compared, 46\% had point estimates within the prediction interval of the original outcome (versus the expected 95\%). Larger original effect sizes and within-participants designs were especially related to replication success. Our results indicate that, consistent with prior reports, the robustness of the psychology literature is low enough to limit cumulative progress by student investigators.},
	number = {11},
	urldate = {2024-03-06},
	journal = {Royal Society Open Science},
	author = {Boyce, Veronica and Mathur, Maya and Frank, Michael C.},
	month = nov,
	year = {2023},
	note = {Publisher: Royal Society},
	keywords = {cognitive psychology, large-scale replication project, metascience, pedagogical replication, replication, social psychology},
	pages = {231240},
}

@article{wagenmakers_registered_2016,
	title = {Registered {Replication} {Report}: {Strack}, {Martin}, \& {Stepper} (1988)},
	volume = {11},
	issn = {1745-6916},
	shorttitle = {Registered {Replication} {Report}},
	url = {https://doi.org/10.1177/1745691616674458},
	doi = {10.1177/1745691616674458},
	abstract = {According to the facial feedback hypothesis, people‚Äôs affective responses can be influenced by their own facial expression (e.g., smiling, pouting), even when their expression did not result from their emotional experiences. For example, Strack, Martin, and Stepper (1988) instructed participants to rate the funniness of cartoons using a pen that they held in their mouth. In line with the facial feedback hypothesis, when participants held the pen with their teeth (inducing a ‚Äúsmile‚Äù), they rated the cartoons as funnier than when they held the pen with their lips (inducing a ‚Äúpout‚Äù). This seminal study of the facial feedback hypothesis has not been replicated directly. This Registered Replication Report describes the results of 17 independent direct replications of Study 1 from Strack et al. (1988), all of which followed the same vetted protocol. A meta-analysis of these studies examined the difference in funniness ratings between the ‚Äúsmile‚Äù and ‚Äúpout‚Äù conditions. The original Strack et al. (1988) study reported a rating difference of 0.82 units on a 10-point Likert scale. Our meta-analysis revealed a rating difference of 0.03 units with a 95\% confidence interval ranging from ‚àí0.11 to 0.16.},
	language = {en},
	number = {6},
	urldate = {2024-02-13},
	journal = {Perspectives on Psychological Science},
	author = {Wagenmakers, E.-J. and Beek, T. and Dijkhoff, L. and Gronau, Q. F. and Acosta, A. and Adams, R. B. and Albohn, D. N. and Allard, E. S. and Benning, S. D. and Blouin-Hudon, E.-M. and Bulnes, L. C. and Caldwell, T. L. and Calin-Jageman, R. J. and Capaldi, C. A. and Carfagno, N. S. and Chasten, K. T. and Cleeremans, A. and Connell, L. and DeCicco, J. M. and Dijkstra, K. and Fischer, A. H. and Foroni, F. and Hess, U. and Holmes, K. J. and Jones, J. L. H. and Klein, O. and Koch, C. and Korb, S. and Lewinski, P. and Liao, J. D. and Lund, S. and Lupianez, J. and Lynott, D. and Nance, C. N. and Oosterwijk, S. and Ozdoƒüru, A. A. and Pacheco-Unguetti, A. P. and Pearson, B. and Powis, C. and Riding, S. and Roberts, T.-A. and Rumiati, R. I. and Senden, M. and Shea-Shumsky, N. B. and Sobocko, K. and Soto, J. A. and Steiner, T. G. and Talarico, J. M. and van Allen, Z. M. and Vandekerckhove, M. and Wainwright, B. and Wayand, J. F. and Zeelenberg, R. and Zetzer, E. E. and Zwaan, R. A.},
	month = nov,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {917--928},
}

@article{hagger_multilab_2016,
	title = {A {Multilab} {Preregistered} {Replication} of the {Ego}-{Depletion} {Effect}},
	volume = {11},
	issn = {1745-6924},
	doi = {10.1177/1745691616652873},
	abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals (CIs) that encompassed zero (d = 0.04, 95\% CI [-0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
	language = {eng},
	number = {4},
	journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
	author = {Hagger, Martin S. and Chatzisarantis, Nikos L. D. and Alberts, Hugo and Anggono, Calvin Octavianus and Batailler, C√©dric and Birt, Angela R. and Brand, Ralf and Brandt, Mark J. and Brewer, Gene and Bruyneel, Sabrina and Calvillo, Dustin P. and Campbell, W. Keith and Cannon, Peter R. and Carlucci, Marianna and Carruth, Nicholas P. and Cheung, Tracy and Crowell, Adrienne and De Ridder, Denise T. D. and Dewitte, Siegfried and Elson, Malte and Evans, Jacqueline R. and Fay, Benjamin A. and Fennis, Bob M. and Finley, Anna and Francis, Zo√´ and Heise, Elke and Hoemann, Henrik and Inzlicht, Michael and Koole, Sander L. and Koppel, Lina and Kroese, Floor and Lange, Florian and Lau, Kevin and Lynch, Bridget P. and Martijn, Carolien and Merckelbach, Harald and Mills, Nicole V. and Michirev, Alexej and Miyake, Akira and Mosser, Alexandra E. and Muise, Megan and Muller, Dominique and Muzi, Milena and Nalis, Dario and Nurwanti, Ratri and Otgaar, Henry and Philipp, Michael C. and Primoceri, Pierpaolo and Rentzsch, Katrin and Ringos, Lara and Schlinkert, Caroline and Schmeichel, Brandon J. and Schoch, Sarah F. and Schrama, Michel and Sch√ºtz, Astrid and Stamos, Angelos and Tingh√∂g, Gustav and Ullrich, Johannes and vanDellen, Michelle and Wimbarti, Supra and Wolff, Wanja and Yusainy, Cleoputri and Zerhouni, Oulmann and Zwienenberg, Maria},
	month = jul,
	year = {2016},
	pmid = {27474142},
	keywords = {Adult, Humans, Meta-Analysis as Topic, Reproducibility of Results, Research Design, Self-Control, Task Performance and Analysis, Young Adult, energy model, meta-analysis, resource depletion, self-regulation, strength model},
	pages = {546--573},
}

@article{low_comparison_2017,
	title = {Comparison of two independent systematic reviews of trials of recombinant human bone morphogenetic protein-2 ({rhBMP}-2): the {Yale} {Open} {Data} {Access} {Medtronic} {Project}},
	volume = {6},
	issn = {2046-4053},
	shorttitle = {Comparison of two independent systematic reviews of trials of recombinant human bone morphogenetic protein-2 ({rhBMP}-2)},
	url = {https://doi.org/10.1186/s13643-017-0422-x},
	doi = {10.1186/s13643-017-0422-x},
	abstract = {It is uncertain whether the replication of systematic reviews, particularly those with the same objectives and resources, would employ similar methods and/or arrive at identical findings. We compared the results and conclusions of two concurrent systematic reviews undertaken by two independent research teams provided with the same objectives, resources, and individual participant-level data.},
	number = {1},
	urldate = {2024-02-13},
	journal = {Systematic Reviews},
	author = {Low, Jeffrey and Ross, Joseph S. and Ritchie, Jessica D. and Gross, Cary P. and Lehman, Richard and Lin, Haiqun and Fu, Rongwei and Stewart, Lesley A. and Krumholz, Harlan M.},
	month = feb,
	year = {2017},
	keywords = {Data interpretation, Data sharing, Meta-analysis, Systematic review},
	pages = {28},
}

@article{page_reprise_2021,
	title = {The {REPRISE} project: protocol for an evaluation of {REProducibility} and {Replicability} {In} {Syntheses} of {Evidence}},
	volume = {10},
	issn = {2046-4053},
	shorttitle = {The {REPRISE} project},
	url = {https://doi.org/10.1186/s13643-021-01670-0},
	doi = {10.1186/s13643-021-01670-0},
	abstract = {Investigations of transparency, reproducibility and replicability in science have been directed largely at individual studies. It is just as critical to explore these issues in syntheses of studies, such as systematic reviews, given their influence on decision-making and future research. We aim to explore various aspects relating to the transparency, reproducibility and replicability of several components of systematic reviews with meta-analysis of the effects of health, social, behavioural and educational interventions.},
	number = {1},
	urldate = {2024-02-13},
	journal = {Systematic Reviews},
	author = {Page, Matthew J. and Moher, David and Fidler, Fiona M. and Higgins, Julian P. T. and Brennan, Sue E. and Haddaway, Neal R. and Hamilton, Daniel G. and Kanukula, Raju and Karunananthan, Sathya and Maxwell, Lara J. and McDonald, Steve and Nakagawa, Shinichi and Nunan, David and Tugwell, Peter and Welch, Vivian A. and McKenzie, Joanne E.},
	month = apr,
	year = {2021},
	keywords = {Meta-analysis, Methodology, Quality, Replication, Reproducibility of Results, Systematic reviews, Transparency},
	pages = {112},
}

@article{veronese_reproducibility_2021,
	title = {Reproducibility of findings in modern {PET} neuroimaging: insight from the {NRM2018} grand challenge},
	volume = {41},
	issn = {1559-7016},
	shorttitle = {Reproducibility of findings in modern {PET} neuroimaging},
	doi = {10.1177/0271678X211015101},
	abstract = {The reproducibility of findings is a compelling methodological problem that the neuroimaging community is facing these days. The lack of standardized pipelines for image processing, quantification and statistics plays a major role in the variability and interpretation of results, even when the same data are analysed. This problem is well-known in MRI studies, where the indisputable value of the method has been complicated by a number of studies that produce discrepant results. However, any research domain with complex data and flexible analytical procedures can experience a similar lack of reproducibility. In this paper we investigate this issue for brain PET imaging. During the 2018 NeuroReceptor Mapping conference, the brain PET community was challenged with a computational contest involving a simulated neurotransmitter release experiment. Fourteen international teams analysed the same imaging dataset, for which the ground-truth was known. Despite a plurality of methods, the solutions were consistent across participants, although not identical. These results should create awareness that the increased sharing of PET data alone will only be one component of enhancing confidence in neuroimaging results and that it will be important to complement this with full details of the analysis pipelines and procedures that have been used to quantify data.},
	language = {eng},
	number = {10},
	journal = {Journal of Cerebral Blood Flow and Metabolism: Official Journal of the International Society of Cerebral Blood Flow and Metabolism},
	author = {Veronese, Mattia and Rizzo, Gaia and Belzunce, Martin and Schubert, Julia and Searle, Graham and Whittington, Alex and Mansur, Ayla and Dunn, Joel and Reader, Andrew and Gunn, Roger N. and {Grand Challenge Participants\#}},
	month = oct,
	year = {2021},
	pmid = {33993794},
	pmcid = {PMC8504414},
	keywords = {Congresses as Topic, Female, History, 21st Century, Humans, Male, Neuroimaging, PET, Positron-Emission Tomography, Reproducibility of Results, data analysis, data sharing, reproducibility crisis, ‚ÄúNRM2018 PET Grand Challenge‚Äù},
	pages = {2778--2796},
}

@article{van_dongen_multiple_2019,
	title = {Multiple {Perspectives} on {Inference} for {Two} {Simple} {Statistical} {Scenarios}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2019.1565553},
	doi = {10.1080/00031305.2019.1565553},
	abstract = {When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data? To study this question empirically we selected from the literature two simple scenarios‚Äîinvolving a comparison of two proportions and a Pearson correlation‚Äîand asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome. The results showed considerable overall agreement; nevertheless, this agreement did not appear to diminish the intensity of the subsequent debate over which statistical framework is more appropriate to address the questions at hand.},
	number = {sup1},
	urldate = {2024-02-13},
	journal = {The American Statistician},
	author = {van Dongen, Noah N. N. and van Doorn, Johnny B. and Gronau, Quentin F. and van Ravenzwaaij, Don and Hoekstra, Rink and Haucke, Matthias N. and Lakens, Daniel and Hennig, Christian and Morey, Richard D. and Homer, Saskia and Gelman, Andrew and Sprenger, Jan and Wagenmakers, Eric-Jan},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2019.1565553},
	keywords = {Frequentist or Bayesian, Multilab analysis, Statistical paradigms, Testing or estimation},
	pages = {328--339},
}

@article{silberzahn_many_2018,
	title = {Many {Analysts}, {One} {Data} {Set}: {Making} {Transparent} {How} {Variations} in {Analytic} {Choices} {Affect} {Results}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Many {Analysts}, {One} {Data} {Set}},
	url = {https://doi.org/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts? prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
	number = {3},
	urldate = {2024-02-13},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn√≠k, ≈†. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and H√∂gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl√ºter, E. and Sch√∂nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp√∂rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	month = sep,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {337--356},
}

@article{schweinsberg_same_2021,
	title = {Same data, different conclusions: {Radical} dispersion in empirical results when independent analysts operationalize and test the same hypothesis},
	volume = {165},
	issn = {0749-5978},
	shorttitle = {Same data, different conclusions},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597821000200},
	doi = {10.1016/j.obhdp.2021.02.003},
	abstract = {In this crowdsourced initiative, independent analysts used the same dataset to test two hypotheses regarding the effects of scientists‚Äô gender and professional status on verbosity during group meetings. Not only the analytic approach but also the operationalizations of key variables were left unconstrained and up to individual analysts. For instance, analysts could choose to operationalize status as job title, institutional ranking, citation counts, or some combination. To maximize transparency regarding the process by which analytic choices are made, the analysts used a platform we developed called DataExplained to justify both preferred and rejected analytic paths in real time. Analyses lacking sufficient detail, reproducible code, or with statistical errors were excluded, resulting in 29 analyses in the final sample. Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. A Boba multiverse analysis demonstrates that decisions about how to operationalize variables explain variability in outcomes above and beyond statistical choices (e.g., covariates). Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. Implications for organizations and leaders, whose decision making relies in part on scientific findings, consulting reports, and internal analyses by data scientists, are discussed.},
	urldate = {2024-02-13},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Schweinsberg, Martin and Feldman, Michael and Staub, Nicola and van den Akker, Olmo R. and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Liu, Yang and Althoff, Tim and Heer, Jeffrey and Kale, Alex and Mohamed, Zainab and Amireh, Hashem and Venkatesh Prasad, Vaishali and Bernstein, Abraham and Robinson, Emily and Snellman, Kaisa and Amy Sommer, S. and Otner, Sarah M. G. and Robinson, David and Madan, Nikhil and Silberzahn, Raphael and Goldstein, Pavel and Tierney, Warren and Murase, Toshio and Mandl, Benjamin and Viganola, Domenico and Strobl, Carolin and Schaumans, Catherine B. C. and Kelchtermans, Stijn and Naseeb, Chan and Mason Garrison, S. and Yarkoni, Tal and Richard Chan, C. S. and Adie, Prestone and Alaburda, Paulius and Albers, Casper and Alspaugh, Sara and Alstott, Jeff and Nelson, Andrew A. and Ari√±o de la Rubia, Eduardo and Arzi, Adbi and Bahn√≠k, ≈†tƒõp√°n and Baik, Jason and Winther Balling, Laura and Banker, Sachin and AA Baranger, David and Barr, Dale J. and Barros-Rivera, Brenda and Bauer, Matt and Blaise, Enuh and Boelen, Lisa and Bohle Carbonell, Katerina and Briers, Robert A. and Burkhard, Oliver and Canela, Miguel-Angel and Castrillo, Laura and Catlett, Timothy and Chen, Olivia and Clark, Michael and Cohn, Brent and Coppock, Alex and Cuguer√≥-Escofet, Nat√†lia and Curran, Paul G. and Cyrus-Lai, Wilson and Dai, David and Valentino Dalla Riva, Giulio and Danielsson, Henrik and Russo, Rosaria de F. S. M. and de Silva, Niko and Derungs, Curdin and Dondelinger, Frank and Duarte de Souza, Carolina and Tyson Dube, B. and Dubova, Marina and Mark Dunn, Ben and Adriaan Edelsbrunner, Peter and Finley, Sara and Fox, Nick and Gnambs, Timo and Gong, Yuanyuan and Grand, Erin and Greenawalt, Brandon and Han, Dan and Hanel, Paul H. P. and Hong, Antony B. and Hood, David and Hsueh, Justin and Huang, Lilian and Hui, Kent N. and Hultman, Keith A. and Javaid, Azka and Ji Jiang, Lily and Jong, Jonathan and Kamdar, Jash and Kane, David and Kappler, Gregor and Kaszubowski, Erikson and Kavanagh, Christopher M. and Khabsa, Madian and Kleinberg, Bennett and Kouros, Jens and Krause, Heather and Krypotos, Angelos-Miltiadis and Lavbiƒç, Dejan and Ling Lee, Rui and Leffel, Timothy and Yang Lim, Wei and Liverani, Silvia and Loh, Bianca and L√∏nsmann, Dorte and Wei Low, Jia and Lu, Alton and MacDonald, Kyle and Madan, Christopher R. and Hjorth Madsen, Lasse and Maimone, Christina and Mangold, Alexandra and Marshall, Adrienne and Ester Matskewich, Helena and Mavon, Kimia and McLain, Katherine L. and McNamara, Amelia A. and McNeill, Mhairi and Mertens, Ulf and Miller, David and Moore, Ben and Moore, Andrew and Nantz, Eric and Nasrullah, Ziauddin and Nejkovic, Valentina and Nell, Colleen S and Arthur Nelson, Andrew and Nilsonne, Gustav and Nolan, Rory and O'Brien, Christopher E. and O'Neill, Patrick and O'Shea, Kieran and Olita, Toto and Otterbacher, Jahna and Palsetia, Diana and Pereira, Bianca and Pozdniakov, Ivan and Protzko, John and Reyt, Jean-Nicolas and Riddle, Travis and (Akmal) Ridhwan Omar Ali, Amal and Ropovik, Ivan and Rosenberg, Joshua M. and Rothen, Stephane and Schulte-Mecklenbeck, Michael and Sharma, Nirek and Shotwell, Gordon and Skarzynski, Martin and Stedden, William and Stodden, Victoria and Stoffel, Martin A. and Stoltzman, Scott and Subbaiah, Subashini and Tatman, Rachael and Thibodeau, Paul H. and Tomkins, Sabina and Valdivia, Ana and Druijff-van de Woestijne, Gerrieke B. and Viana, Laura and Villes√®che, Florence and Duncan Wadsworth, W. and Wanders, Florian and Watts, Krista and Wells, Jason D and Whelpley, Christopher E. and Won, Andy and Wu, Lawrence and Yip, Arthur and Youngflesh, Casey and Yu, Ju-Chi and Zandian, Arash and Zhang, Leilei and Zibman, Chava and Luis Uhlmann, Eric},
	month = jul,
	year = {2021},
	keywords = {Analysis-contingent results, Crowdsourcing data analysis, Research reliability, Researcher degrees of freedom, Scientific robustness, Scientific transparency},
	pages = {228--249},
}

@article{maier-hein_challenge_2017,
	title = {The challenge of mapping the human connectome based on diffusion tractography},
	volume = {8},
	issn = {2041-1723},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5677006/},
	doi = {10.1038/s41467-017-01285-x},
	abstract = {Tractography based on non-invasive diffusion imaging is central to the study of human brain connectivity. To date, the approach has not been systematically validated in ground truth studies. Based on a simulated human brain data set with ground truth tracts, we organized an open international tractography challenge, which resulted in 96 distinct submissions from 20 research groups. Here, we report the encouraging finding that most state-of-the-art algorithms produce tractograms containing 90\% of the ground truth bundles (to at least some extent). However, the same tractograms contain many more invalid than valid bundles, and half of these invalid bundles occur systematically across research groups. Taken together, our results demonstrate and confirm fundamental ambiguities inherent in tract reconstruction based on orientation information alone, which need to be considered when interpreting tractography and connectivity results. Our approach provides a novel framework for estimating reliability of tractography and encourages innovation to address its current limitations., Though tractography is widely used, it has not been systematically validated. Here, authors report results from 20 groups showing that many tractography algorithms produce both valid and invalid bundles.},
	urldate = {2024-02-13},
	journal = {Nature Communications},
	author = {Maier-Hein, Klaus H. and Neher, Peter F. and Houde, Jean-Christophe and C√¥t√©, Marc-Alexandre and Garyfallidis, Eleftherios and Zhong, Jidan and Chamberland, Maxime and Yeh, Fang-Cheng and Lin, Ying-Chia and Ji, Qing and Reddick, Wilburn E. and Glass, John O. and Chen, David Qixiang and Feng, Yuanjing and Gao, Chengfeng and Wu, Ye and Ma, Jieyan and He, Renjie and Li, Qiang and Westin, Carl-Fredrik and Deslauriers-Gauthier, Samuel and Gonz√°lez, J. Omar Ocegueda and Paquette, Michael and St-Jean, Samuel and Girard, Gabriel and Rheault, Fran√ßois and Sidhu, Jasmeen and Tax, Chantal M. W. and Guo, Fenghua and Mesri, Hamed Y. and D√°vid, Szabolcs and Froeling, Martijn and Heemskerk, Anneriet M. and Leemans, Alexander and Bor√©, Arnaud and Pinsard, Basile and Bedetti, Christophe and Desrosiers, Matthieu and Brambati, Simona and Doyon, Julien and Sarica, Alessia and Vasta, Roberta and Cerasa, Antonio and Quattrone, Aldo and Yeatman, Jason and Khan, Ali R. and Hodges, Wes and Alexander, Simon and Romascano, David and Barakovic, Muhamed and Aur√≠a, Anna and Esteban, Oscar and Lemkaddem, Alia and Thiran, Jean-Philippe and Cetingul, H. Ertan and Odry, Benjamin L. and Mailhe, Boris and Nadar, Mariappan S. and Pizzagalli, Fabrizio and Prasad, Gautam and Villalon-Reina, Julio E. and Galvis, Justin and Thompson, Paul M. and Requejo, Francisco De Santiago and Laguna, Pedro Luque and Lacerda, Luis Miguel and Barrett, Rachel and Dell‚ÄôAcqua, Flavio and Catani, Marco and Petit, Laurent and Caruyer, Emmanuel and Daducci, Alessandro and Dyrby, Tim B. and Holland-Letz, Tim and Hilgetag, Claus C. and Stieltjes, Bram and Descoteaux, Maxime},
	month = nov,
	year = {2017},
	pmid = {29116093},
	pmcid = {PMC5677006},
	pages = {1349},
}

@article{hoogeveen_many-analysts_2023,
	title = {A many-analysts approach to the relation between religiosity and well-being},
	volume = {13},
	issn = {2153-599X},
	url = {https://doi.org/10.1080/2153599X.2022.2070255},
	doi = {10.1080/2153599X.2022.2070255},
	abstract = {The relation between religiosity and well-being is one of the most researched topics in the psychology of religion, yet the directionality and robustness of the effect remains debated. Here, we adopted a many-analysts approach to assess the robustness of this relation based on a new cross-cultural dataset (N=10,535 participants from 24 countries). We recruited 120 analysis teams to investigate (1) whether religious people self-report higher well-being, and (2) whether the relation between religiosity and self-reported well-being depends on perceived cultural norms of religion (i.e., whether it is considered normal and desirable to be religious in a given country). In a two-stage procedure, the teams first created an analysis plan and then executed their planned analysis on the data. For the first research question, all but 3 teams reported positive effect sizes with credible/confidence intervals excluding zero (median reported Œ≤=0.120). For the second research question, this was the case for 65\% of the teams (median reported Œ≤=0.039). While most teams applied (multilevel) linear regression models, there was considerable variability in the choice of items used to construct the independent variables, the dependent variable, and the included covariates.},
	number = {3},
	urldate = {2024-02-13},
	journal = {Religion, Brain \& Behavior},
	author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Aczel, Balazs and Aditya, Yonathan and Alayan, Alexandra J. and Allen, Peter J. and Altay, Sacha and Alzahawi, Shilaan and Amir, Yulmaida and Anthony, Francis-Vincent and Kwame Appiah, Obed and Atkinson, Quentin D. and Baimel, Adam and Balkaya-Ince, Merve and Balsamo, Michela and Banker, Sachin and Barto≈°, Franti≈°ek and Becerra, Mario and Beffara, Bertrand and Beitner, Julia and Bendixen, Theiss and Berkessel, Jana B. and Berni≈´nas, Renatas and Billet, Matthew I. and Billingsley, Joseph and Bortolini, Tiago and Breitsohl, Heiko and Bret, Am√©lie and Brown, Faith L. and Brown, Jennifer and Brumbaugh, Claudia C. and Buczny, Jacek and Bulbulia, Joseph and Caballero, Sa√∫l and Carlucci, Leonardo and Carmichael, Cheryl L. and Cattaneo, Marco E. G. V. and Charles, Sarah J. and Claessens, Scott and Panagopoulos, Maxinne C. and Costa, Angelo Brandelli and Crone, Damien L. and Czoschke, Stefan and Czymara, Christian and D'Urso, E. Damiano and Dahlstr√∂m, √ñrjan and Rosa, Anna Dalla and Danielsson, Henrik and De Ron, Jill and de Vries, Ymkje Anna and Dean, Kristy K. and Dik, Bryan J. and Disabato, David J. and Doherty, Jaclyn K. and Draws, Tim and Drouhot, Lucas and Dujmovic, Marin and Dunham, Yarrow and Ebert, Tobias and Edelsbrunner, Peter A. and Eerland, Anita and Elbaek, Christian T. and Farahmand, Shole and Farahmand, Hooman and Farias, Miguel and Feliccia, Abrey A. and Fischer, Kyle and Fischer, Ronald and Fisher-Thompson, Donna and Francis, Zo√´ and Frick, Susanne and Frisch, Lisa K. and Geraldes, Diogo and Gerdin, Emily and Geven, Linda and Ghasemi, Omid and Gielens, Erwin and Gligoriƒá, Vuka≈°in and Hagel, Kristin and Hajdu, Nandor and Hamilton, Hannah R. and Hamzah, Imaduddin and Hanel, Paul H. P. and Hawk, Christopher E. and K. Himawan, Karel and Holding, Benjamin C. and Homman, Lina E. and Ingendahl, Moritz and Inkil√§, Hilla and Inman, Mary L. and Islam, Chris-Gabriel and Isler, Ozan and Izydorczyk, David and Jaeger, Bastian and Johnson, Kathryn A. and Jong, Jonathan and Karl, Johannes A. and Kaszubowski, Erikson and Katz, Benjamin A. and Keefer, Lucas A. and Kelchtermans, Stijn and Kelly, John M. and Klein, Richard A. and Kleinberg, Bennett and Knowles, Megan L. and Ko≈Çczy≈Ñska, Marta and Koller, Dave and Krasko, Julia and Kritzler, Sarah and Krypotos, Angelos-Miltiadis and Kyritsis, Thanos and L. Landes, Todd and Laukenmann, Ruben and Forsyth, Guy A. Lavender and Lazar, Aryeh and Lehman, Barbara J. and Levy, Neil and Lo, Ronda F. and Lodder, Paul and Lorenz, Jennifer and ≈Åowicki, Pawe≈Ç and Ly, Albert L. and Maassen, Esther and Magyar-Russell, Gina M. and Maier, Maximilian and Marsh, Dylan R. and Martinez, Nuria and Martinie, Marcellin and Martoyo, Ihan and Mason, Susan E. and Mauritsen, Anne Lundahl and McAleer, Phil and McCauley, Thomas and McCullough, Michael and McKay, Ryan and McMahon, Camilla M. and McNamara, Amelia A. and Means, Kira K. and Mercier, Brett and Mitkidis, Panagiotis and Monin, Beno√Æt and Moon, Jordan W. and Moreau, David and Morgan, Jonathan and Murphy, James and Muscatt, George and N√§gel, Christof and Nagy, Tam√°s and Nalborczyk, Ladislas and Nilsonne, Gustav and Noack, Pamina and Norenzayan, Ara and Nuijten, Mich√®le B. and Olsson-Collentine, Anton and Oviedo, Lluis and Pavlov, Yuri G. and Pawelski, James O. and Pearson, Hannah I. and Pedder, Hugo and Peetz, Hannah K. and Pinus, Michael and Pirutinsky, Steven and Polito, Vince and Porubanova, Michaela and Poulin, Michael J. and Prenoveau, Jason M. and Prince, Mark A. and Protzko, John and Pryor, Campbell and Purzycki, Benjamin G. and Qiu, Lin and P√ºtter, Julian Quevedo and Rabelo, Andr√© and Radell, Milen L. and Ramsay, Jonathan E. and Reid, Graham and J. Roberts, Andrew and Luna, Lindsey M. Root and Ross, Robert M. and Roszak, Piotr and Roy, Nirmal and Saarelainen, Suvi-Maria K. and Sasaki, Joni Y. and Schaumans, Catherine and Schivinski, Bruno and Schmitt, Marcel C. and Schnitker, Sarah A. and Schnuerch, Martin and Schreiner, Marcel R. and Sch√ºttengruber, Victoria and Sebben, Simone and Segerstrom, Suzanne C. and Seryczy≈Ñska, Berenika and Shjoedt, Uffe and Simsek, M√ºge and Sleegers, Willem W. A. and Smith, Eliot R. and Sowden, Walter J. and Sp√§th, Marion and Sp√∂rlein, Christoph and Stedden, William and Stoevenbelt, Andrea H. and Stuber, Simon and Sulik, Justin and Suwartono, Christiany and Syropoulos, Stylianos and Szaszi, Barnabas and Szecsi, Peter and Tappin, Ben M. and Tay, Louis and Thibault, Robert T. and Thompson, Burt and Thurn, Christian M. and Torralba, Josefa and Tuthill, Shelby D. and Ullein, Ann-Marie and Van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Van Cappellen, Patty and van den Akker, Olmo R. and Van der Cruyssen, Ine and Van der Noll, Jolanda and van Dongen, Noah N. N. and Van Lissa, Caspar J. and van Mulukom, Valerie and van Ravenzwaaij, Don and van Zyl, Casper J. J. and Ann Vaughn, Leigh and Veƒákalov, Bojana and Verschuere, Bruno and Vianello, Michelangelo and Vilanova, Felipe and Vishkin, Allon and Vogel, Vera and Vogelsmeier, Leonie V. D. E. and Watanabe, Shoko and White, Cindel J. M. and Wiebels, Kristina and Wiechert, Sera and Willett, Zachary Z. and Witkowiak, Maciej and Witvliet, Charlotte V. O. and Wiwad, Dylan and Wuyts, Robin and Xygalatas, Dimitris and Yang, Xin and Yeo, Darren J. and Yilmaz, Onurcan and Zarzeczna, Natalia and Zhao, Yitong and Zijlmans, Josjan and van Elk, Michiel and Wagenmakers, Eric-Jan},
	month = jul,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/2153599X.2022.2070255},
	keywords = {Health, many analysts, open science, religion},
	pages = {237--283},
}

@article{salganik_measuring_2020,
	title = {Measuring the predictability of life outcomes with a scientific mass collaboration},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1915006117},
	doi = {10.1073/pnas.1915006117},
	abstract = {How predictable are life trajectories? We investigated this question with a scientific mass collaboration using the common task method; 160 teams built predictive models for six life outcomes using data from the Fragile Families and Child Wellbeing Study, a high-quality birth cohort study. Despite using a rich dataset and applying machine-learning methods optimized for prediction, the best predictions were not very accurate and were only slightly better than those from a simple benchmark model. Within each outcome, prediction error was strongly associated with the family being predicted and weakly associated with the technique used to generate the prediction. Overall, these results suggest practical limits to the predictability of life outcomes in some settings and illustrate the value of mass collaborations in the social sciences.},
	number = {15},
	urldate = {2024-02-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Salganik, Matthew J. and Lundberg, Ian and Kindel, Alexander T. and Ahearn, Caitlin E. and Al-Ghoneim, Khaled and Almaatouq, Abdullah and Altschul, Drew M. and Brand, Jennie E. and Carnegie, Nicole Bohme and Compton, Ryan James and Datta, Debanjan and Davidson, Thomas and Filippova, Anna and Gilroy, Connor and Goode, Brian J. and Jahani, Eaman and Kashyap, Ridhi and Kirchner, Antje and McKay, Stephen and Morgan, Allison C. and Pentland, Alex and Polimis, Kivan and Raes, Louis and Rigobon, Daniel E. and Roberts, Claudia V. and Stanescu, Diana M. and Suhara, Yoshihiko and Usmani, Adaner and Wang, Erik H. and Adem, Muna and Alhajri, Abdulla and AlShebli, Bedoor and Amin, Redwane and Amos, Ryan B. and Argyle, Lisa P. and Baer-Bositis, Livia and B√ºchi, Moritz and Chung, Bo-Ryehn and Eggert, William and Faletto, Gregory and Fan, Zhilin and Freese, Jeremy and Gadgil, Tejomay and Gagn√©, Josh and Gao, Yue and Halpern-Manners, Andrew and Hashim, Sonia P. and Hausen, Sonia and He, Guanhua and Higuera, Kimberly and Hogan, Bernie and Horwitz, Ilana M. and Hummel, Lisa M. and Jain, Naman and Jin, Kun and Jurgens, David and Kaminski, Patrick and Karapetyan, Areg and Kim, E. H. and Leizman, Ben and Liu, Naijia and M√∂ser, Malte and Mack, Andrew E. and Mahajan, Mayank and Mandell, Noah and Marahrens, Helge and Mercado-Garcia, Diana and Mocz, Viola and Mueller-Gastell, Katariina and Musse, Ahmed and Niu, Qiankun and Nowak, William and Omidvar, Hamidreza and Or, Andrew and Ouyang, Karen and Pinto, Katy M. and Porter, Ethan and Porter, Kristin E. and Qian, Crystal and Rauf, Tamkinat and Sargsyan, Anahit and Schaffner, Thomas and Schnabel, Landon and Schonfeld, Bryan and Sender, Ben and Tang, Jonathan D. and Tsurkov, Emma and van Loon, Austin and Varol, Onur and Wang, Xiafei and Wang, Zhi and Wang, Julia and Wang, Flora and Weissman, Samantha and Whitaker, Kirstie and Wolters, Maria K. and Woon, Wei Lee and Wu, James and Wu, Catherine and Yang, Kengran and Yin, Jingwen and Zhao, Bingyu and Zhu, Chenyun and Brooks-Gunn, Jeanne and Engelhardt, Barbara E. and Hardt, Moritz and Knox, Dean and Levy, Karen and Narayanan, Arvind and Stewart, Brandon M. and Watts, Duncan J. and McLanahan, Sara},
	month = apr,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {8398--8403},
}

@article{breznau_observing_2022,
	title = {Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty},
	volume = {119},
	url = {https://www.pnas.org/doi/10.1073/pnas.2203150119},
	doi = {10.1073/pnas.2203150119},
	abstract = {This study explores how researchers‚Äô analytical choices affect the reliability of scientific findings. Most discussions of reliability problems in science focus on systematic biases. We broaden the lens to emphasize the idiosyncrasy of conscious and unconscious decisions that researchers make during data analysis. We coordinated 161 researchers in 73 research teams and observed their research decisions as they used the same data to independently test the same prominent social science hypothesis: that greater immigration reduces support for social policies among the public. In this typical case of social science research, research teams reported both widely diverging numerical findings and substantive conclusions despite identical start conditions. Researchers‚Äô expertise, prior beliefs, and expectations barely predict the wide variation in research outcomes. More than 95\% of the total variance in numerical results remains unexplained even after qualitative coding of all identifiable decisions in each team‚Äôs workflow. This reveals a universe of uncertainty that remains hidden when considering a single study in isolation. The idiosyncratic nature of how researchers‚Äô results and conclusions varied is a previously underappreciated explanation for why many scientific hypotheses remain contested. These results call for greater epistemic humility and clarity in reporting scientific findings.},
	number = {44},
	urldate = {2024-02-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Nguyen, Hung H. V. and Adem, Muna and Adriaans, Jule and Alvarez-Benjumea, Amalia and Andersen, Henrik K. and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix S. and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes N. and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin B. and Carlos-Castillo, Juan and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am√©lie and Gr√∂mping, Max and Gro√ü, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H√∂vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ign√°cz, Zs√≥fia S. and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko≈Çczy≈Ñska, Marta and Kuk, John and Kuni√üen, Katharina and Kurti Sinatra, Dafina and Langenkamp, Alexander and Lersch, Philipp M. and L√∂bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan E. and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar J. and McManus, Patricia and McWagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan and Moya, Crist√≥bal and Neunhoeffer, Marcel and N√ºst, Daniel and Nyg√•rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna O. and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel R. and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Regine and Schmidt, Katja M. and Schmidt-Catran, Alexander and Schmiedeberg, Claudia and Schneider, J√ºrgen and Schoonvelde, Martijn and Schulte-Cloos, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J√ºrgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian and Vagni, Giacomo and Van Assche, Jasper and van der Linden, Meta and van der Noll, Jolanda and Van Hootegem, Arno and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and ≈ª√≥≈Çtak, Tomasz},
	month = nov,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2203150119},
}

@article{coretta_multidimensional_2023,
	title = {Multidimensional {Signals} and {Analytic} {Flexibility}: {Estimating} {Degrees} of {Freedom} in {Human}-{Speech} {Analyses}},
	volume = {6},
	issn = {2515-2459},
	shorttitle = {Multidimensional {Signals} and {Analytic} {Flexibility}},
	url = {https://doi.org/10.1177/25152459231162567},
	doi = {10.1177/25152459231162567},
	abstract = {Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis that can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling but also from decisions regarding the quantification of the measured behavior. In this study, we gave the same speech-production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further found little to no evidence that the observed variability can be explained by analysts‚Äô prior beliefs, expertise, or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system, and calibrate their (un)certainty in their conclusions.},
	language = {en},
	number = {3},
	urldate = {2024-02-13},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Coretta, Stefano and Casillas, Joseph V. and Roessig, Simon and Franke, Michael and Ahn, Byron and Al-Hoorie, Ali H. and Al-Tamimi, Jalal and Alotaibi, Najd E. and AlShakhori, Mohammed K. and Altmiller, Ruth M. and Arantes, Pablo and Athanasopoulou, Angeliki and Baese-Berk, Melissa M. and Bailey, George and Sangma, Cheman Baira A and Beier, Eleonora J. and Benavides, Gabriela M. and Benker, Nicole and BensonMeyer, Emelia P. and Benway, Nina R. and Berry, Grant M. and Bing, Liwen and Bjorndahl, Christina and Bolyanatz, Mari≈°ka and Braver, Aaron and Brown, Violet A. and Brown, Alicia M. and Brugos, Alejna and Buchanan, Erin M. and Butlin, Tanna and Bux√≥-Lugo, Andr√©s and Caillol, Coline and Cangemi, Francesco and Carignan, Christopher and Carraturo, Sita and Caudrelier, Tiphaine and Chodroff, Eleanor and Cohn, Michelle and Cronenberg, Johanna and Crouzet, Olivier and Dagar, Erica L. and Dawson, Charlotte and Diantoro, Carissa A. and Dokovova, Marie and Drake, Shiloh and Du, Fengting and Dubuis, Margaux and Du√™me, Florent and Durward, Matthew and Egurtzegi, Ander and Elsherif, Mahmoud M. and Esser, Janina and Ferragne, Emmanuel and Ferreira, Fernanda and Fink, Lauren K. and Finley, Sara and Foster, Kurtis and Foulkes, Paul and Franzke, Rosa and Frazer-McKee, Gabriel and Fromont, Robert and Garc√≠a, Christina and Geller, Jason and Grasso, Camille L. and Greca, Pia and Grice, Martine and Grose-Hodge, Magdalena S. and Gully, Amelia J. and Halfacre, Caitlin and Hauser, Ivy and Hay, Jen and Haywood, Robert and Hellmuth, Sam and Hilger, Allison I. and Holliday, Nicole and Hoogland, Damar and Huang, Yaqian and Hughes, Vincent and Icardo Isasa, Ane and Ilchovska, Zlatomira G. and Jeon, Hae-Sung and Jones, Jacq and Junges, M√°gat N. and Kaefer, Stephanie and Kaland, Constantijn and Kelley, Matthew C. and Kelly, Niamh E. and Kettig, Thomas and Khattab, Ghada and Koolen, Ruud and Krahmer, Emiel and Krajewska, Dorota and Krug, Andreas and Kumar, Abhilasha A. and Lander, Anna and Lentz, Tomas O. and Li, Wanyin and Li, Yanyu and Lialiou, Maria and Lima, Ronaldo M. and Lo, Justin J. H. and Lopez Otero, Julio Cesar and Mackay, Bradley and MacLeod, Bethany and Mallard, Mel and McConnellogue, Carol-Ann Mary and Moroz, George and Murali, Mridhula and Nalborczyk, Ladislas and Nenadiƒá, Filip and Nieder, Jessica and Nikoliƒá, Du≈°an and Nogueira, Francisco G. S. and Offerman, Heather M. and Passoni, Elisa and P√©lissier, Maud and Perry, Scott J. and Pfiffner, Alexandra M. and Proctor, Michael and Rhodes, Ryan and Rodr√≠guez, Nicole and Roepke, Elizabeth and R√∂er, Jan P. and Sbacco, Lucia and Scarborough, Rebecca and Schaeffler, Felix and Schleef, Erik and Schmitz, Dominic and Shiryaev, Alexander and S√≥skuthy, M√°rton and Spaniol, Malin and Stanley, Joseph A. and Strickler, Alyssa and Tavano, Alessandro and Tomaschek, Fabian and Tucker, Benjamin V. and Turnbull, Rory and Ugwuanyi, Kingsley O. and Urrestarazu-Porta, I√±igo and van de Vijver, Ruben and Van Engen, Kristin J. and van Miltenburg, Emiel and Wang, Bruce Xiao and Warner, Natasha and Wehrle, Simon and Westerbeek, Hans and Wiener, Seth and Winters, Stephen and Wong, Sidney G.-J. and Wood, Anna and Wottawa, Jane and Xu, Chenzi and Z√°rate-S√°ndez, Germ√°n and Zellou, Georgia and Zhang, Cong and Zhu, Jian and Roettger, Timo B.},
	month = jul,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459231162567},
}

@article{botvinik-nezer_variability_2020,
	title = {Variability in the analysis of a single neuroimaging dataset by many teams},
	volume = {582},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2314-9},
	doi = {10.1038/s41586-020-2314-9},
	abstract = {Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging¬†by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses1. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset2‚Äì5. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed.},
	language = {en},
	number = {7810},
	urldate = {2024-02-13},
	journal = {Nature},
	author = {Botvinik-Nezer, Rotem and Holzmeister, Felix and Camerer, Colin F. and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A. and Adcock, R. Alison and Avesani, Paolo and Baczkowski, Blazej M. and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nad√®ge and Beaton, Derek and Beitner, Julia and Benoit, Roland G. and Berkers, Ruud M. W. J. and Bhanji, Jamil P. and Biswal, Bharat B. and Bobadilla-Suarez, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine L. and Bowring, Alexander and Braem, Senne and Brooks, Hayley R. and Brudner, Emily G. and Calderon, Cristian B. and Camilleri, Julia A. and Castrellon, Jaime J. and Cecchetti, Luca and Cieslik, Edna C. and Cole, Zachary J. and Collignon, Olivier and Cox, Robert W. and Cunningham, William A. and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles P. and Luca, Alberto De and Delgado, Mauricio R. and Demetriou, Lysia and Dennison, Jeffrey B. and Di, Xin and Dickie, Erin W. and Dobryakova, Ekaterina and Donnat, Claire L. and Dukart, Juergen and Duncan, Niall W. and Durnez, Joke and Eed, Amr and Eickhoff, Simon B. and Erhart, Andrew and Fontanesi, Laura and Fricke, G. Matthew and Fu, Shiguang and Galv√°n, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle J. and Golowin, Sergej A. E. and Gonz√°lez-Garc√≠a, Carlos and Gorgolewski, Krzysztof J. and Grady, Cheryl L. and Green, Mikella A. and Guassi Moreira, Jo√£o F. and Guest, Olivia and Hakimi, Shabnam and Hamilton, J. Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson B. and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott A. and Hughes, Matthew E. and Iacovella, Vittorio and Iordan, Alexandru D. and Isager, Peder M. and Isik, Ayse I. and Jahn, Andrew and Johnson, Matthew R. and Johnstone, Tom and Joseph, Michael J. E. and Juliano, Anthony C. and Kable, Joseph W. and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy R. and Kucukboyaci, Nuri Erkut and Kuhl, Brice A. and Kupek, Sebastian and Laird, Angela R. and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y. C. and Lim, Phui Cheng and Lintz, Evan N. and Liphardt, Schuyler W. and Losecaat Vermeer, Annabel B. and Love, Bradley C. and Mack, Michael L. and Malpica, Norberto and Marins, Theo and Maumet, Camille and McDonald, Kelsey and McGuire, Joseph T. and Melero, Helena and M√©ndez Leal, Adriana S. and Meyer, Benjamin and Meyer, Kristin N. and Mihai, Glad and Mitsis, Georgios D. and Moll, Jorge and Nielson, Dylan M. and Nilsonne, Gustav and Notter, Michael P. and Olivetti, Emanuele and Onicas, Adrian I. and Papale, Paolo and Patil, Kaustubh R. and Peelle, Jonathan E. and P√©rez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and Reuter-Lorenz, Patricia A. and Reynolds, Richard C. and Ricciardi, Emiliano and Rieck, Jenny R. and Rodriguez-Thompson, Anais M. and Romyn, Anthony and Salo, Taylor and Samanez-Larkin, Gregory R. and Sanz-Morales, Emilio and Schlichting, Margaret L. and Schultz, Douglas H. and Shen, Qiang and Sheridan, Margaret A. and Silvers, Jennifer A. and Skagerlund, Kenny and Smith, Alec and Smith, David V. and Sokol-Hessner, Peter and Steinkamp, Simon R. and Tashjian, Sarah M. and Thirion, Bertrand and Thorp, John N. and Tingh√∂g, Gustav and Tisdall, Loreen and Tompson, Steven H. and Toro-Serey, Claudio and Torre Tresols, Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and van ‚Äòt Veer, Anna E. and Verguts, Tom and Vettel, Jean M. and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B. and Weeda, Wouter D. and Weis, Susanne and White, David J. and Wisniewski, David and Xifra-Porxas, Alba and Yearling, Emily A. and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S. L. and Zhang, Lei and Zhang, Xu and Zosky, Joshua E. and Nichols, Thomas E. and Poldrack, Russell A. and Schonberg, Tom},
	month = jun,
	year = {2020},
	note = {Number: 7810
Publisher: Nature Publishing Group},
	keywords = {Decision, Decision making, Human behaviour, Scientific community},
	pages = {84--88},
}

@article{boehm_estimating_2018,
	title = {Estimating across-trial variability parameters of the {Diffusion} {Decision} {Model}: {Expert} advice and recommendations},
	volume = {87},
	issn = {0022-2496},
	shorttitle = {Estimating across-trial variability parameters of the {Diffusion} {Decision} {Model}},
	url = {https://www.sciencedirect.com/science/article/pii/S002224961830021X},
	doi = {10.1016/j.jmp.2018.09.004},
	abstract = {For many years the Diffusion Decision Model (DDM) has successfully accounted for behavioral data from a wide range of domains. Important contributors to the DDM‚Äôs success are the across-trial variability parameters, which allow the model to account for the various shapes of response time distributions encountered in practice. However, several researchers have pointed out that estimating the variability parameters can be a challenging task. Moreover, the numerous fitting methods for the DDM each come with their own associated problems and solutions. This often leaves users in a difficult position. In this collaborative project we invited researchers from the DDM community to apply their various fitting methods to simulated data and provide advice and expert guidance on estimating the DDM‚Äôs across-trial variability parameters using these methods. Our study establishes a comprehensive reference resource and describes methods that can help to overcome the challenges associated with estimating the DDM‚Äôs across-trial variability parameters.},
	urldate = {2024-02-13},
	journal = {Journal of Mathematical Psychology},
	author = {Boehm, Udo and Annis, Jeffrey and Frank, Michael J. and Hawkins, Guy E. and Heathcote, Andrew and Kellen, David and Krypotos, Angelos-Miltiadis and Lerche, Veronika and Logan, Gordon D. and Palmeri, Thomas J. and van Ravenzwaaij, Don and Servant, Mathieu and Singmann, Henrik and Starns, Jeffrey J. and Voss, Andreas and Wiecki, Thomas V. and Matzke, Dora and Wagenmakers, Eric-Jan},
	month = dec,
	year = {2018},
	keywords = {Across-trial variability parameters, Diffusion Decision Model, Parameter estimation},
	pages = {46--75},
}

@article{bastiaansen_time_2020,
	title = {Time to get personal? {The} impact of researchers choices on the selection of treatment targets using the experience sampling methodology},
	volume = {137},
	issn = {0022-3999},
	shorttitle = {Time to get personal?},
	url = {https://www.sciencedirect.com/science/article/pii/S002239992030773X},
	doi = {10.1016/j.jpsychores.2020.110211},
	abstract = {Objective
One of the promises of the experience sampling methodology (ESM) is that a statistical analysis of an individual's emotions, cognitions and behaviors in everyday-life could be used to identify relevant treatment targets. A requisite for clinical implementation is that outcomes of such person-specific time-series analyses are not wholly contingent on the researcher performing them.
Methods
To evaluate this, we crowdsourced the analysis of one individual patient's ESM data to 12 prominent research teams, asking them what symptom(s) they would advise the treating clinician to target in subsequent treatment.
Results
Variation was evident at different stages of the analysis, from preprocessing steps (e.g., variable selection, clustering, handling of missing data) to the type of statistics and rationale for selecting targets. Most teams did include a type of vector autoregressive model, examining relations between symptoms over time. Although most teams were confident their selected targets would provide useful information to the clinician, not one recommendation was similar: both the number (0‚Äì16) and nature of selected targets varied widely.
Conclusion
This study makes transparent that the selection of treatment targets based on personalized models using ESM data is currently highly conditional on subjective analytical choices and highlights key conceptual and methodological issues that need to be addressed in moving towards clinical implementation.},
	urldate = {2024-02-13},
	journal = {Journal of Psychosomatic Research},
	author = {Bastiaansen, Jojanneke A. and Kunkels, Yoram K. and Blaauw, Frank J. and Boker, Steven M. and Ceulemans, Eva and Chen, Meng and Chow, Sy-Miin and de Jonge, Peter and Emerencia, Ando C. and Epskamp, Sacha and Fisher, Aaron J. and Hamaker, Ellen L. and Kuppens, Peter and Lutz, Wolfgang and Meyer, M. Joseph and Moulder, Robert and Oravecz, Zita and Riese, Harri√´tte and Rubel, Julian and Ryan, Ois√≠n and Servaas, Michelle N. and Sjobeck, Gustav and Snippe, Evelien and Trull, Timothy J. and Tschacher, Wolfgang and van der Veen, Date C. and Wichers, Marieke and Wood, Phillip K. and Woods, William C. and Wright, Aidan G. C. and Albers, Casper J. and Bringmann, Laura F.},
	month = oct,
	year = {2020},
	keywords = {Crowdsourcing science, Electronic diary, Mental disorders, Personalized medicine, Psychological networks, Time-series analysis},
	pages = {110211},
}

@article{fisar_reproducibility_2024,
	title = {Reproducibility in {Management} {Science}},
	url = {https://osf.io/mydzv},
	doi = {10.31219/osf.io/mydzv},
	abstract = {With the help of more than 700 reviewers we assess the reproducibility of nearly 500 articles published in the journal Management Science before and after the introduction of a new Data and Code Disclosure policy in 2019. When considering only articles for which data accessibility and hard- and software requirements were not an obstacle for reviewers, the results of more than 95\% of articles under the new disclosure policy could be fully or largely computationally reproduced. However, for 29\% of articles at least part of the dataset was not accessible to the reviewer. Considering all articles in our sample reduces the share of reproduced articles to 68\%. These figures represent a significant increase compared to the period before the introduction of the disclosure policy, where only 12\% of articles voluntarily provided replication materials, out of which 55\% could be (largely) reproduced. Substantial heterogeneity in reproducibility rates across different fields is mainly driven by differences in dataset accessibility. Other reasons for unsuccessful reproduction attempts include missing code, unresolvable code errors, weak or missing documentation, but also soft- and hardware requirements and code complexity. Our findings highlight the importance of journal code and data disclosure policies, and suggest potential avenues for enhancing their effectiveness.},
	language = {en-us},
	urldate = {2024-02-13},
	author = {Fi≈°ar, Milo≈° and Greiner, Ben and Huber, Christoph and Katok, Elena and Ozkes, Ali and Collaboration, Management Science Reproducibility},
	month = feb,
	year = {2024},
	note = {Publisher: OSF},
}

@article{marcoci_predicting_2024,
	title = {Predicting the replicability of social and behavioural science claims from the {COVID}-19 {Preprint} {Replication} {Project} with structured expert and novice groups},
	url = {https://osf.io/xdsjf},
	doi = {10.31222/osf.io/xdsjf},
	abstract = {Replication is an important ‚Äúcredibility control‚Äù mechanism for clarifying the reliability of published findings. However, replication is costly, and it is infeasible to replicate everything. Accurate, fast, lower cost alternatives such as eliciting predictions from experts or novices could accelerate credibility assessment and improve allocation of replication resources for important and uncertain findings. We elicited judgments from experts and novices on 100 claims from preprints about an emerging area of research (COVID-19 pandemic) using a new interactive structured elicitation protocol and we conducted 35 new replications. Participants‚Äô average estimates were similar to the observed replication rate of 60\%. After interacting with their peers, novices updated both their estimates and confidence in their judgements significantly more than experts and their accuracy improved more between elicitation rounds. Experts‚Äô average accuracy was 0.54 (95\% CI: [0.454, 0.628]) after interaction and they correctly classified 55\% of claims; novices‚Äô average accuracy was 0.55 (95\% CI: [0.455, 0.628]), correctly classifying 61\% of claims. The difference in accuracy between experts and novices was not significant and their judgments on the full set of claims were strongly correlated (r=.48). These results are consistent with prior investigations eliciting predictions about the replicability of published findings in established areas of research and suggest that expertise may not be required for credibility assessment of some research findings.},
	language = {en-us},
	urldate = {2024-02-13},
	author = {Marcoci, Alexandru and Wilkinson, David Peter and Abatayo, Anna Lou and Baskin, Ernest and Berkman, Henk and Buchanan, Erin Michelle and Capit√°n, Sara and Capit√°n, Tabar√© and Chan, Ginny and Cheng, Kent Jason Go and Coupe, Tom and Dryhurst, Sarah and Duan, Jane and Edlund, John and Errington, Timothy M. and Fedor, Anna and Fidler, Fiona and Field, James and Fox [SCORE, Nicholas and Fraser, Hannah and Freeman, Alexandra L. J. and Hanea, Anca and Holzmeister, Felix and Hong, Sanghyun and Huggins, Raquel and Huntington-Klein, Nick and Johannesson, Magnus and Jones, Angela and Kapoor, Hansika and Kerr, John R. and Struhl, Melissa Kline and Kolczynska, Marta and Liu, Yang and Loomas, Zachary and Luis [SCORE, Bri and M√©ndez, Esteban and Miske, Olivia and Nast, Carolin and Nosek, Brian A. and Parsons, Elan Simon and Pfeiffer, Thomas and Reed, W. Robert and Roozenbeek, Jon and Schlyfestone, Alexa R. and Schneider, Claudia R. and Soh, Andrew and Tagat, Anirudh and Tutor, Melba and Tyner, Andrew and Urbanska, Karolina and Linden, Dr Sander van der and Vercammen, Ans and Wintle, Bonnie},
	month = feb,
	year = {2024},
	note = {Publisher: OSF},
}

@article{balli_interaction_2013,
	title = {Interaction effects in econometrics},
	volume = {45},
	issn = {1435-8921},
	url = {https://doi.org/10.1007/s00181-012-0604-2},
	doi = {10.1007/s00181-012-0604-2},
	abstract = {We provide practical advice for applied economists regarding robust specification and interpretation of linear regression models with interaction terms. We replicate a number of prominently published results using interaction effects and examine if they are robust to reasonable specification permutations.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Empirical Economics},
	author = {Balli, Hatice Ozer and S√∏rensen, Bent E.},
	month = aug,
	year = {2013},
	keywords = {C12, C13, Interaction terms, Non-linear regression},
	pages = {583--603},
}

@article{amaral_brazilian_2019,
	title = {The {Brazilian} {Reproducibility} {Initiative}},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.41602},
	doi = {10.7554/eLife.41602},
	abstract = {Most efforts to estimate the reproducibility of published findings have focused on specific areas of research, even though science is usually assessed and funded on a regional or national basis. Here we describe a project to assess the reproducibility of findings in biomedical science published by researchers based in Brazil. The Brazilian Reproducibility Initiative is a systematic, multicenter effort to repeat between 60 and 100 experiments: the project will focus on a set of common methods, repeating each experiment in three different laboratories from a countrywide network. The results, due in 2021, will allow us to estimate the level of reproducibility of biomedical science in Brazil, and to investigate what aspects of the published literature might help to predict whether a finding is reproducible.},
	urldate = {2024-02-13},
	journal = {eLife},
	author = {Amaral, Olavo B and Neves, Kleber and Wasilewska-Sampaio, Ana P and Carneiro, Clarissa FD},
	editor = {Rodgers, Peter and Errington, Timothy M and Klein, Richard},
	month = feb,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {Brazil, biomedical research, metascience, open science, replication, reproducibility},
	pages = {e41602},
}

@article{brauer_data_2007,
	title = {Data, {Models}, {Coefficients}: {The} {Case} of {United} {States} {Military} {Expenditure}},
	volume = {24},
	issn = {0738-8942},
	shorttitle = {Data, {Models}, {Coefficients}},
	url = {https://doi.org/10.1080/07388940601102845},
	doi = {10.1080/07388940601102845},
	abstract = {This article is an exercise in economic methodology. It replicates two published models of the effect of military expenditure on the United States economy but, in order to study variations in the relevant estimated parameters, applies two different military expenditure data sets to the models (budget vs. National Income and Product Accounts [NIPA] data). In an extension, the article examines coefficient stability when the economically preferred NIPA data are applied across varying time-periods. Two major findings are that economic models should avoid the use of budget data and that even when the preferred NIPA data are used, estimated parameters are highly unstable across time.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Conflict Management and Peace Science},
	author = {Brauer, Jurgen},
	month = feb,
	year = {2007},
	note = {Publisher: SAGE Publications Ltd},
	pages = {55--64},
}

@article{amini_comparison_2012,
	title = {Comparison of {Model} {Averaging} {Techniques}: {Assessing} {Growth} {Determinants}},
	volume = {27},
	copyright = {Copyright ¬© 2012 John Wiley \& Sons, Ltd.},
	issn = {1099-1255},
	shorttitle = {Comparison of {Model} {Averaging} {Techniques}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2288},
	doi = {10.1002/jae.2288},
	abstract = {This paper investigates the replicability of three important studies on growth theory uncertainty that employed Bayesian model averaging tools. We compare these results with estimates obtained using alternative, recently developed model averaging techniques. Overall, we successfully replicate all three studies, find that the sign and magnitude of these new estimates are reasonably close to those produced via traditional Bayesian methods and deploy a novel strategy to implement one of the new averaging estimators. Copyright ¬© 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2024-02-13},
	journal = {Journal of Applied Econometrics},
	author = {Amini, Shahram M. and Parmeter, Christopher F.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2288},
	pages = {870--876},
}

@misc{chang_leveraging_2024,
	type = {Monograph},
	title = {Leveraging real-world data to assess treatment sequences in health economic evaluations: a study protocol for emulating target trials using the {English} {Cancer} {Registry} and {US} {Electronic} {Health} {Records}-{Derived} {Database}},
	copyright = {cc\_by\_nc\_nd\_4},
	shorttitle = {Leveraging real-world data to assess treatment sequences in health economic evaluations},
	url = {https://eprints.whiterose.ac.uk/208318/},
	abstract = {Background

Considering the sequence of treatments is vital for optimising healthcare resource allocation, especially in cancer care, where sequence changes can affect patients‚Äô overall survival and associated costs. A key challenge in evaluating treatment sequences in health technology assessments (HTA) is the scarce evidence on effectiveness, leading to uncertainties in decision making. While randomised controlled trials (RCTs) and meta-analyses are viewed as the gold standards for evidence, applying
them to determine the effectiveness of treatment sequences in economic models often necessitates making arbitrary assumptions due to insufficient information on patients' treatment histories and
subsequent therapies. In contrast, real-world data (RWD) presents a promising alternative source of evidence, often encompassing details across treatment lines. However, due to its non-randomised
nature, estimates of the treatment effectiveness based on RWD analyses can be susceptible to biases if not properly adjusted for confounding factors. To date, several international initiatives have been investigating methods to derive reliable treatment effects from RWD ‚Äî by emulating Target Trials that replicate existing RCTs (i.e. benchmarks) and comparing the emulated results against the benchmarks. These studies primarily seek to determine the viability of obtaining trial-equivalent results through deploying specific 
analytical methodologies and study designs within the Target Trial emulation framework, using a given database. Adopting the Target Trial emulation framework facilitates the analyses to be operated
under causal inference principles. Upon validation in a particular database, these techniques can be applied to address similar questions (e.g., same disease area, same outcome type), but in populations lacking clinical trial evidence, leveraging the same RWD source. Studies to date, however, have predominantly focused on the comparison of individual treatments rather than treatment sequences. Moreover, the majority of these investigations have been undertaken in non-English contexts. Consequently, the use of RWD in evaluating treatment sequences for HTA, especially in an English setting, remains largely unexplored.

Objectives

The goal of this project is to investigate the feasibility of leveraging RWD to produce reliable, trial-like effectiveness estimates for treatment sequences. We aim to assess the capability of two
oncology databases: the US-based Flatiron electronic health record and the National Cancer Registration and Analysis Service (NCRAS) database of England. To achieve this, we plan to harness the Target Trial Emulation (TTE) framework for replicating two existing  oncology RCTs that compared treatment sequences, with the intent of benchmarking our results against the original studies. Further, we aim to detail the practicalities involved with implementing TTE in diverse databases and outline the challenges encountered.

Methods

1. We aim to emulate existing RCTs that compare the effect of different treatment sequences by constructing the study design and analysis plan following the TTE framework. Specifically, the
following case studies are planned:
(1) Prostate cancer case study 1 (PC1) - US direct proof-of-concept study (method direct validation): replicating the GUTG-001 trial using Flatiron data
(2) Prostate cancer case study 2 (PC2) - US-England bridging study (method extension): emulating Target Trials that compare treatment sequences that have been common in England using Flatiron data
(3) Prostate cancer case study 3 (PC3) - English indirect proof-of-concept study (method indirect validation): emulating the same Target Trial in PC2 using English NCRAS data
(4) Renal cell carcinoma case study (RCC) - method direct validation in a single-arm setting: emulating the sunitinib followed by everolimus arm in the RECORD-3 trial using English NCRAS data
2. We will compare results of the emulated Target Trials with those from the benchmark trials.
3. We plan to compare different advanced causal inference methods (e.g. marginal structural models using IPW and other g-methods) in estimating the effect of treatment sequences in RWD.

Expected results

This study will provide evidence on whether it is feasible to obtain reliable estimates of the (comparative) effectiveness of treatment sequences using Flatiron data and English NCRAS data. If applicable, we intend to develop a framework that provides a systematic way of obtaining the (comparative) effectiveness of treatment sequences using RWD. It is possible that the data quality is insufficient to emulate the planned Target Trials. In this case, we will report reasons for the implausibility of data analysis. If  applicable, we will make suggestions to whether the national health
data collection may be enhanced to make the analyses possible. The results of this study will be submitted to peer-reviewed journals and international conferences.},
	language = {en},
	urldate = {2024-02-13},
	journal = {HEDS Discussion Paper},
	author = {Chang, J.-Y. A. and Chilcott, J. B. and Latimer, N. R.},
	month = jan,
	year = {2024},
	note = {Issue: 24.01
Number: 24.01
Pages: 1-61
Publisher: Sheffield Centre for Health and Related Research, University of Sheffield},
}

@article{hanousek_rise_2008,
	title = {A rise by any other name? {Sensitivity} of growth regressions to data source},
	volume = {30},
	issn = {0164-0704},
	shorttitle = {A rise by any other name?},
	url = {https://www.sciencedirect.com/science/article/pii/S016407040700122X},
	doi = {10.1016/j.jmacro.2007.08.015},
	abstract = {Measured rates of growth in real per capita income differ drastically depending on the data source. This phenomenon occurs largely because data sets differ in whether and how they adjust for changes in relative prices across countries. Replication of several recent studies of growth determinants shows that results are sensitive in important ways to the choice of data. Previous warnings against using data adjusted to increase cross-country comparability to study within-country patterns over time (growth rates) have been largely ignored at the cost of possibly contaminating the conclusions.},
	number = {3},
	urldate = {2024-02-13},
	journal = {Journal of Macroeconomics},
	author = {Hanousek, Jan and Hajkova, Dana and Filer, Randall K.},
	month = sep,
	year = {2008},
	keywords = {Growth, Measurement},
	pages = {1188--1206},
}

@article{alipourfard_systematizing_2024,
	title = {Systematizing {Confidence} in {Open} {Research} and {Evidence} ({SCORE})},
	url = {https://osf.io/46mnb},
	doi = {10.31235/osf.io/46mnb},
	abstract = {Assessing the credibility of research claims is a central, continuous, and laborious part of the scientific process. Credibility assessment strategies range from expert judgment to aggregating existing evidence to systematic replication efforts. Such assessments can require substantial time and effort. Research progress could be accelerated if there were rapid, scalable, accurate credibility indicators to guide attention and resource allocation for further assessment. The SCORE program is creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools, teams are creating: a database of claims from papers in the social and behavioral sciences; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates. Beyond the primary research objective, the data and artifacts generated from this program will be openly shared and provide an unprecedented opportunity to examine research credibility and evidence.},
	language = {en-us},
	urldate = {2024-02-13},
	author = {Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel M. and Benkler, Noam and Bishop, Michael and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Almenberg, Anna Dreber and Errington, Timothy M. and Fidler, Fiona and Field, Samuel and Fox [SCORE, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Giles, C. Lee and Gordon, Michael B. and Gordon-Sarney, Reed and Griffin, Christopher and Gulden, Timothy and Hahn, Krystal and Hartman, Robert and Holzmeister, Felix and Hu, Xia Ben and Johannesson, Magnus and Kezar, Lee and Struhl, Melissa Kline and Kuter, Ugur and Kwasnica, Anthony M. and Lee, Dong-Ho and Lerman, Kristina and Liu, Yang and Loomas, Zachary and Luis [SCORE, Bri and Magnusson, Ian and Miske, Olivia and Mody, Fallon and Morstatter, Fred and Nosek, Brian A. and Parsons, Elan Simon and Pennock, David and Pfeiffer, Thomas and Pujara, Jay and Rajtmajer, Sarah and Ren, Xiang and Salinas, Abel and Selvam, Ravi Kiran and Shipman, Frank and Silverstein, Priya and Sprenger, Amber and Squicciarini, Anna Ms and Stratman, Steve and Sun, Kexuan and Tikoo, Saatvik and Twardy, Charles R. and Tyner, Andrew and Viganola, Domenico and Wang, Juntao and Wilkinson, David Peter and Wintle, Bonnie and Wu, Jian},
	month = feb,
	year = {2024},
	note = {Publisher: OSF},
}

@article{luijken_replicability_2024,
	title = {Replicability of simulation studies for the investigation of statistical methods: the {RepliSims} project},
	volume = {11},
	shorttitle = {Replicability of simulation studies for the investigation of statistical methods},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.231003},
	doi = {10.1098/rsos.231003},
	abstract = {Results of simulation studies evaluating the performance of statistical methods can have a major impact on the way empirical research is implemented. However, so far there is limited evidence of the replicability of simulation studies. Eight highly cited statistical simulation studies were selected, and their replicability was assessed by teams of replicators with formal training in quantitative methodology. The teams used information in the original publications to write simulation code with the aim of replicating the results. The primary outcome was to determine the feasibility of replicability based on reported information in the original publications and supplementary materials. Replicasility varied greatly: some original studies provided detailed information leading to almost perfect replication of results, whereas other studies did not provide enough information to implement any of the reported simulations. Factors facilitating replication included availability of code, detailed reporting or visualization of data-generating procedures and methods, and replicator expertise. Replicability of statistical simulation studies was mainly impeded by lack of information and sustainability of information sources. We encourage researchers publishing simulation studies to transparently report all relevant implementation details either in the research paper itself or in easily accessible supplementary material and to make their simulation code publicly available using permanent links.},
	number = {1},
	urldate = {2024-02-13},
	journal = {Royal Society Open Science},
	author = {Luijken, K. and Lohmann, A. and Alter, U. and Claramunt Gonzalez, J. and Clouth, F. J. and Fossum, J. L. and Hesen, L. and Huizing, A. H. J. and Ketelaar, J. and Montoya, A. K. and Nab, L. and Nijman, R. C. C. and Penning de Vries, B. B. L. and Tibbe, T. D. and Wang, Y. A. and Groenwold, R. H. H.},
	month = jan,
	year = {2024},
	note = {Publisher: Royal Society},
	keywords = {open materials, replication, simulation studies, statistical methods},
	pages = {231003},
}

@article{milcu_genotypic_2018,
	title = {Genotypic variability enhances the reproducibility of an ecological study},
	volume = {2},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-017-0434-x},
	doi = {10.1038/s41559-017-0434-x},
	language = {en},
	number = {2},
	urldate = {2024-02-13},
	journal = {Nature Ecology \& Evolution},
	author = {Milcu, Alexandru and Puga-Freitas, Ruben and Ellison, Aaron M. and Blouin, Manuel and Scheu, Stefan and Freschet, Gr√©goire T. and Rose, Laura and Barot, Sebastien and Cesarz, Simone and Eisenhauer, Nico and Girin, Thomas and Assandri, Davide and Bonkowski, Michael and Buchmann, Nina and Butenschoen, Olaf and Devidal, Sebastien and Gleixner, Gerd and Gessler, Arthur and Gigon, Agn√®s and Greiner, Anna and Grignani, Carlo and Hansart, Amandine and Kayler, Zachary and Lange, Markus and Lata, Jean-Christophe and Le Galliard, Jean-Fran√ßois and Lukac, Martin and Mannerheim, Neringa and M√ºller, Marina E. H. and Pando, Anne and Rotter, Paula and Scherer-Lorenzen, Michael and Seyhun, Rahme and Urban-Mead, Katherine and Weigelt, Alexandra and Zavattaro, Laura and Roy, Jacques},
	month = jan,
	year = {2018},
	pages = {279--287},
}

@article{soto_how_2019,
	title = {How {Replicable} {Are} {Links} {Between} {Personality} {Traits} and {Consequential} {Life} {Outcomes}? {The} {Life} {Outcomes} of {Personality} {Replication} {Project}},
	volume = {30},
	issn = {0956-7976},
	shorttitle = {How {Replicable} {Are} {Links} {Between} {Personality} {Traits} and {Consequential} {Life} {Outcomes}?},
	url = {https://doi.org/10.1177/0956797619831612},
	doi = {10.1177/0956797619831612},
	abstract = {The Big Five personality traits have been linked to dozens of life outcomes. However, metascientific research has raised questions about the replicability of behavioral science. The Life Outcomes of Personality Replication (LOOPR) Project was therefore conducted to estimate the replicability of the personality-outcome literature. Specifically, I conducted preregistered, high-powered (median N = 1,504) replications of 78 previously published trait‚Äìoutcome associations. Overall, 87\% of the replication attempts were statistically significant in the expected direction. The replication effects were typically 77\% as strong as the corresponding original effects, which represents a significant decline in effect size. The replicability of individual effects was predicted by the effect size and design of the original study, as well as the sample size and statistical power of the replication. These results indicate that the personality-outcome literature provides a reasonably accurate map of trait‚Äìoutcome associations but also that it stands to benefit from efforts to improve replicability.},
	language = {en},
	number = {5},
	urldate = {2024-02-13},
	journal = {Psychological Science},
	author = {Soto, Christopher J.},
	month = may,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {711--727},
}

@article{jaric_using_2024,
	title = {Using mice from different breeding sites fails to improve replicability of results from single-laboratory studies},
	volume = {53},
	copyright = {2023 The Author(s)},
	issn = {1548-4475},
	url = {https://www.nature.com/articles/s41684-023-01307-w},
	doi = {10.1038/s41684-023-01307-w},
	abstract = {Theoretical and empirical evidence indicates that low external validity due to rigorous standardization of study populations is a cause of poor replicability in animal research. Here we report a multi-laboratory study aimed at investigating whether heterogenization of study populations by using animals from different breeding sites increases the replicability of results from single-laboratory studies. We used male C57BL/6J mice from six different breeding sites to test a standardized against a heterogenized (HET) study design in six independent replicate test laboratories. For the standardized design, each laboratory ordered mice from a single breeding site (each laboratory from a different one), while for the HET design, each laboratory ordered proportionate numbers of mice from the five remaining breeding sites. To test our hypothesis, we assessed 14 outcome variables, including body weight, behavioral measures obtained from a single session on an elevated plus maze, and clinical blood parameters. Both breeding site and test laboratory affected variation in outcome variables, but the effect of test laboratory was more pronounced for most outcome variables. Moreover, heterogenization of study populations by breeding site (HET) did not reduce variation in outcome variables between test laboratories, which was most likely due to the fact that breeding site had only little effect on variation in outcome variables, thereby limiting the scope for HET to reduce between-lab variation. We conclude that heterogenization of study populations by breeding site has limited capacity for improving the replicability of results from single-laboratory animal studies.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Lab Animal},
	author = {Jaric, Ivana and Voelkl, Bernhard and Amrein, Irmgard and Wolfer, David P. and Novak, Janja and Detotto, Carlotta and Weber-Stadlbauer, Ulrike and Meyer, Urs and Manuella, Francesca and Mansuy, Isabelle M. and W√ºrbel, Hanno},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Animal behaviour, Robustness},
	pages = {18--22},
}

@article{arroyo-araujo_systematic_2022,
	title = {Systematic assessment of the replicability and generalizability of preclinical findings: {Impact} of protocol harmonization across laboratory sites},
	volume = {20},
	issn = {1545-7885},
	shorttitle = {Systematic assessment of the replicability and generalizability of preclinical findings},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001886},
	doi = {10.1371/journal.pbio.3001886},
	abstract = {The influence of protocol standardization between laboratories on their replicability of preclinical results has not been addressed in a systematic way. While standardization is considered good research practice as a means to control for undesired external noise (i.e., highly variable results), some reports suggest that standardized protocols may lead to idiosyncratic results, thus undermining replicability. Through the EQIPD consortium, a multi-lab collaboration between academic and industry partners, we aimed to elucidate parameters that impact the replicability of preclinical animal studies. To this end, 3 experimental protocols were implemented across 7 laboratories. The replicability of results was determined using the distance travelled in an open field after administration of pharmacological compounds known to modulate locomotor activity (MK-801, diazepam, and clozapine) in C57BL/6 mice as a worked example. The goal was to determine whether harmonization of study protocols across laboratories improves the replicability of the results and whether replicability can be further improved by systematic variation (heterogenization) of 2 environmental factors (time of testing and light intensity during testing) within laboratories. Protocols were tested in 3 consecutive stages and differed in the extent of harmonization across laboratories and standardization within laboratories: stage 1, minimally aligned across sites (local protocol); stage 2, fully aligned across sites (harmonized protocol) with and without systematic variation (standardized and heterogenized cohort); and stage 3, fully aligned across sites (standardized protocol) with a different compound. All protocols resulted in consistent treatment effects across laboratories, which were also replicated within laboratories across the different stages. Harmonization of protocols across laboratories reduced between-lab variability substantially compared to each lab using their local protocol. In contrast, the environmental factors chosen to introduce systematic variation within laboratories did not affect the behavioral outcome. Therefore, heterogenization did not reduce between-lab variability further compared to the harmonization of the standardized protocol. Altogether, these findings demonstrate that subtle variations between lab-specific study protocols may introduce variation across independent replicate studies even after protocol harmonization and that systematic heterogenization of environmental factors may not be sufficient to account for such between-lab variation. Differences in replicability of results within and between laboratories highlight the ubiquity of study-specific variation due to between-lab variability, the importance of transparent and fine-grained reporting of methodologies and research protocols, and the importance of independent study replication.},
	language = {en},
	number = {11},
	urldate = {2024-02-13},
	journal = {PLOS Biology},
	author = {Arroyo-Araujo, Mar√≠a and Voelkl, Bernhard and Laloux, Cl√©ment and Novak, Janja and Koopmans, Bastijn and Waldron, Ann-Marie and Seiffert, Isabel and Stirling, Helen and Aulehner, Katharina and Janhunen, Sanna K. and Ramboz, Sylvie and Potschka, Heidrun and Holappa, Johanna and Fine, Tania and Loos, Maarten and Boulanger, Bruno and W√ºrbel, Hanno and Kas, Martien J.},
	month = nov,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Biological laboratories, Biological locomotion, Drug interactions, Drug therapy, Phenotypes, Research laboratories, Statistical data, Tails},
	pages = {e3001886},
}

@article{coles_multi-lab_2022,
	title = {A multi-lab test of the facial feedback hypothesis by the {Many} {Smiles} {Collaboration}},
	volume = {6},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01458-9},
	doi = {10.1038/s41562-022-01458-9},
	abstract = {Following theories of emotional embodiment, the facial feedback hypothesis suggests that individuals‚Äô subjective experiences of emotion are influenced by their facial expressions. However, evidence for this hypothesis has been mixed. We thus formed a global adversarial collaboration and carried out a preregistered, multicentre study designed to specify and test the conditions that should most reliably produce facial feedback effects. Data from n‚Äâ=‚Äâ3,878 participants spanning 19 countries indicated that a facial mimicry and voluntary facial action task could both amplify and initiate feelings of happiness. However, evidence of facial feedback effects was less conclusive when facial feedback was manipulated unobtrusively via a pen-in-mouth task.},
	language = {en},
	number = {12},
	urldate = {2024-02-13},
	journal = {Nature Human Behaviour},
	author = {Coles, Nicholas A. and March, David S. and Marmolejo-Ramos, Fernando and Larsen, Jeff T. and Arinze, Nwadiogo C. and Ndukaihe, Izuchukwu L. G. and Willis, Megan L. and Foroni, Francesco and Reggev, Niv and Mokady, Aviv and Forscher, Patrick S. and Hunter, John F. and Kaminski, Gwena√´l and Y√ºvr√ºk, Elif and Kapucu, Aycan and Nagy, Tam√°s and Hajdu, Nandor and Tejada, Julian and Freitag, Raquel M. K. and Zambrano, Danilo and Som, Bidisha and Aczel, Balazs and Barzykowski, Krystian and Adamus, Sylwia and Filip, Katarzyna and Yamada, Yuki and Ikeda, Ayumi and Eaves, Daniel L. and Levitan, Carmel A. and Leiweke, Sydney and Parzuchowski, Michal and Butcher, Natalie and Pfuhl, Gerit and Basnight-Brown, Dana M. and Hinojosa, Jos√© A. and Montoro, Pedro R. and Javela D, Lady G. and Vezirian, Kevin and IJzerman, Hans and Trujillo, Natalia and Pressman, Sarah D. and Gygax, Pascal M. and √ñzdoƒüru, Asil A. and Ruiz-Fernandez, Susana and Ellsworth, Phoebe C. and Gaertner, Lowell and Strack, Fritz and Marozzi, Marco and Liuzza, Marco Tullio},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Psychology},
	pages = {1731--1742},
}

@article{arroyo-araujo_reproducibility_2019,
	title = {Reproducibility via coordinated standardization: a multi-center study in a {Shank2} genetic rat model for {Autism} {Spectrum} {Disorders}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Reproducibility via coordinated standardization},
	url = {https://www.nature.com/articles/s41598-019-47981-0},
	doi = {10.1038/s41598-019-47981-0},
	abstract = {Inconsistent findings between laboratories are hampering scientific progress and are of increasing public concern. Differences in laboratory environment is a known factor contributing to poor reproducibility of findings between research sites, and well-controlled multisite efforts are an important next step to identify the relevant factors needed to reduce variation in study outcome between laboratories. Through harmonization of apparatus, test protocol, and aligned and non-aligned environmental variables, the present study shows that behavioral pharmacological responses in Shank2 knockout (KO) rats, a model of synaptic dysfunction relevant to autism spectrum disorders, were highly replicable across three research centers. All three sites reliably observed a hyperactive and repetitive behavioral phenotype in KO rats compared to their wild-type littermates as well as a dose-dependent phenotype attenuation following acute injections of a selective mGluR1 antagonist. These results show that reproducibility in preclinical studies can be obtained and emphasizes the need for high quality and rigorous methodologies in scientific research. Considering the observed external validity, the present study also¬†suggests mGluR1 as potential target for the treatment of autism spectrum disorders.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Scientific Reports},
	author = {Arroyo-Araujo, Mar√≠a and Graf, Radka and Maco, Martine and van Dam, Elsbeth and Schenker, Esther and Drinkenburg, Wilhelmus and Koopmans, Bastijn and de Boer, Sietse F. and Cullum-Doyle, Michaela and Noldus, Lucas P. J. J. and Loos, Maarten and van Dommelen, Wil and Spooren, Will and Biemans, Barbara and Buhl, Derek L. and Kas, Martien J.},
	month = aug,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Autism spectrum disorders, Pharmacology},
	pages = {11602},
}

@article{protzko_high_2023,
	title = {High replicability of newly discovered social-behavioural findings is achievable},
	copyright = {2023 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01749-9},
	doi = {10.1038/s41562-023-01749-9},
	abstract = {Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50\%, replication attempts here produced the expected effects with significance testing (P‚Äâ{\textless}‚Äâ0.05) in 86\% of attempts, slightly exceeding the maximum expected replicability based on observed effect sizes and sample sizes. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97\% that in the original study. This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries.},
	language = {en},
	urldate = {2024-02-13},
	journal = {Nature Human Behaviour},
	author = {Protzko, John and Krosnick, Jon and Nelson, Leif and Nosek, Brian A. and Axt, Jordan and Berent, Matt and Buttrick, Nicholas and DeBell, Matthew and Ebersole, Charles R. and Lundmark, Sebastian and MacInnis, Bo and O‚ÄôDonnell, Michael and Perfecto, Hannah and Pustejovsky, James E. and Roeder, Scott S. and Walleczek, Jan and Schooler, Jonathan W.},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Psychology},
	pages = {1--9},
}

@article{ebersole_many_2020,
	title = {Many {Labs} 5: {Testing} {Pre}-{Data}-{Collection} {Peer} {Review} as an {Intervention} to {Increase} {Replicability}},
	volume = {3},
	issn = {2515-2459},
	shorttitle = {Many {Labs} 5},
	url = {https://doi.org/10.1177/2515245920958687},
	doi = {10.1177/2515245920958687},
	abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {\textless} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3‚Äì9; median total sample = 1,279.5, range = 276‚Äì3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (Œîr = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00‚Äì.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19‚Äì.50).},
	language = {en},
	number = {3},
	urldate = {2024-02-13},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazareviƒá, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban√≠k, Gabriel and Baskin, Ernest and Belopavloviƒá, Radomir and Bernstein, Michael H. and Bia≈Çek, Micha≈Ç and Bloxsom, Nicholas G. and Bodro≈æa, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br√ºhlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and ƒåoliƒá, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc√£o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M√°ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha≈Çasa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H√ºffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and Jim√©nez-Leal, William and Johannesson, Magnus and Joy-Gaba, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko≈Çodziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and de Lima, Tiago Jess√© Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, ≈Åukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa≈Ç and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orliƒá, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedoviƒá, Ivana and Pƒôkala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petroviƒá, Boban and Pfeiffer, Thomas and Pie≈Ñkosz, Damian and Preti, Emanuele and Puriƒá, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys≈Çaw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and Schulz-Hardt, Stefan and Sch√ºtz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R√∫ben and Sioma, Barbara and Skorb, Lauren and de Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojiloviƒá, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz√∂ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and ≈Ωe≈æelj, Iris and Zrubka, Mark and Nosek, Brian A.},
	month = sep,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	pages = {309--331},
}

@article{klein_many_2022,
	title = {Many {Labs} 4: {Failure} to {Replicate} {Mortality} {Salience} {Effect} {With} and {Without} {Original} {Author} {Involvement}},
	volume = {8},
	issn = {2474-7394},
	shorttitle = {Many {Labs} 4},
	url = {https://doi.org/10.1525/collabra.35271},
	doi = {10.1525/collabra.35271},
	abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 17 Labs and N = 1,550 participants, after exclusions) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not fully understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
	number = {1},
	urldate = {2024-02-13},
	journal = {Collabra: Psychology},
	author = {Klein, Richard A and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Hilgard, Joseph and Ahn, Paul Hangsan and Brady, Abbie J. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret T. and Cromar, Ryan and Gardiner, Gwendolyn and Gosnell, Courtney L. and Grahe, Jon and Hall, Calvin and Howard, Irene and Joy-Gaba, Jennifer A. and Kolb, Miranda and Legg, Angela M. and Levitan, Carmel A. and Mancini, Anthony D. and Manfredi, Dylan and Miller, Jason and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine L. M. and Storage, Daniel and Swanson, Trevor and Van Swol, Lyn M. and Vaughn, Leigh Ann and Vidamuerte, Devere and Wiggins, Brady and Ratliff, Kate A.},
	month = apr,
	year = {2022},
	pages = {35271},
}

@article{ebersole_many_2016,
	series = {Special {Issue}: {Confirmatory}},
	title = {Many {Labs} 3: {Evaluating} participant pool quality across the academic semester via replication},
	volume = {67},
	issn = {0022-1031},
	shorttitle = {Many {Labs} 3},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
	doi = {10.1016/j.jesp.2015.10.012},
	abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences‚Äîconscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
	urldate = {2024-02-13},
	journal = {Journal of Experimental Social Psychology},
	author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
	month = nov,
	year = {2016},
	keywords = {Cognitive psychology, Individual differences, Participant pool, Replication, Sampling effects, Situational effects, Social psychology},
	pages = {68--82},
}

@article{klein_many_2018,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} {Across} {Samples} and {Settings}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Many {Labs} 2},
	url = {https://doi.org/10.1177/2515245918810225},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen‚Äôs ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	language = {en},
	number = {4},
	urldate = {2024-02-13},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn√≠k, ≈†tƒõp√°n and Batra, Rishtee and Berkics, Mih√°ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R√©dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doƒüulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-√Ångel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G√≥mez, √Ångel and Gonz√°lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, √Öse H. and Jim√©nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloƒülu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne≈æeviƒá, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani√´l and Lazareviƒá, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Meƒëedoviƒá, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F√©lix and Lee Nichols, Austin and Ocampo, Aaron and O‚ÄôDonnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G√°bor and Osowiecka, Malgorzata and Packard, Grant and P√©rez-S√°nchez, Rolando and Petroviƒá, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch√∂nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop√∫, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ‚Äôt Veer, Anna Elisabeth and V√°squez- Echeverr√≠a, Alejandro and Ann Vaughn, Leigh and V√°zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	month = dec,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {443--490},
}

@article{klein_investigating_2014,
	title = {Investigating {Variation} in {Replicability}},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178},
	doi = {10.1027/1864-9335/a000178},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect ‚Äì imagined contact reducing prejudice ‚Äì showed weak support for replicability. And two effects ‚Äì flag priming influencing conservatism and currency priming influencing system justification ‚Äì did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	number = {3},
	urldate = {2024-02-13},
	journal = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn√≠k, ≈†tƒõp√°n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and van ‚Äòt Veer, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
	month = may,
	year = {2014},
	note = {Publisher: Hogrefe Publishing},
	keywords = {cross-cultural, generalizability, replication, reproducibility, variation},
	pages = {142--152},
}

@article{wang_emulation_2023,
	title = {Emulation of {Randomized} {Clinical} {Trials} {With} {Nonrandomized} {Database} {Analyses}: {Results} of 32 {Clinical} {Trials}},
	volume = {329},
	issn = {0098-7484},
	shorttitle = {Emulation of {Randomized} {Clinical} {Trials} {With} {Nonrandomized} {Database} {Analyses}},
	url = {https://doi.org/10.1001/jama.2023.4221},
	doi = {10.1001/jama.2023.4221},
	abstract = {Nonrandomized studies using insurance claims databases can be analyzed to produce real-world evidence on the effectiveness of medical products. Given the lack of baseline randomization and measurement issues, concerns exist about whether such studies produce unbiased treatment effect estimates.To emulate the design of 30 completed and 2 ongoing randomized clinical trials (RCTs) of medications with database studies using observational analogues of the RCT design parameters (population, intervention, comparator, outcome, time [PICOT]) and to quantify agreement in RCT-database study pairs.New-user cohort studies with propensity score matching using 3 US claims databases (Optum Clinformatics, MarketScan, and Medicare). Inclusion-exclusion criteria for each database study were prespecified to emulate the corresponding RCT. RCTs were explicitly selected based on feasibility, including power, key confounders, and end points more likely to be emulated with real-world data. All 32 protocols were registered on ClinicalTrials.gov before conducting analyses. Emulations were conducted from 2017 through 2022.Therapies for multiple clinical conditions were included.Database study emulations focused on the primary outcome of the corresponding RCT. Findings of database studies were compared with RCTs using predefined metrics, including Pearson correlation coefficients and binary metrics based on statistical significance agreement, estimate agreement, and standardized difference.In these highly selected RCTs, the overall observed agreement between the RCT and the database emulation results was a Pearson correlation of 0.82 (95\% CI, 0.64-0.91), with 75\% meeting statistical significance, 66\% estimate agreement, and 75\% standardized difference agreement. In a post hoc analysis limited to 16 RCTs with closer emulation of trial design and measurements, concordance was higher (Pearson r, 0.93; 95\% CI, 0.79-0.97; 94\% meeting statistical significance, 88\% estimate agreement, 88\% standardized difference agreement). Weaker concordance occurred among 16 RCTs for which close emulation of certain design elements that define the research question (PICOT) with data from insurance claims was not possible (Pearson r, 0.53; 95\% CI, 0.00-0.83; 56\% meeting statistical significance, 50\% estimate agreement, 69\% standardized difference agreement).Real-world evidence studies can reach similar conclusions as RCTs when design and measurements can be closely emulated, but this may be difficult to achieve. Concordance in results varied depending on the agreement metric. Emulation differences, chance, and residual confounding can contribute to divergence in results and are difficult to disentangle.},
	number = {16},
	urldate = {2024-02-13},
	journal = {JAMA},
	author = {Wang, Shirley V. and Schneeweiss, Sebastian and {RCT-DUPLICATE Initiative}},
	month = apr,
	year = {2023},
	pages = {1376--1385},
}

@article{wang_reproducibility_2022,
	title = {Reproducibility of real-world evidence studies using clinical practice data to inform regulatory and coverage decisions},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32310-3},
	doi = {10.1038/s41467-022-32310-3},
	abstract = {Abstract
            
              Studies that generate real-world evidence on the effects of medical products through analysis of digital data collected in clinical practice provide key insights for regulators, payers, and other healthcare decision-makers. Ensuring reproducibility of such findings is fundamental to effective evidence-based decision-making. We reproduce results for 150 studies published in peer-reviewed journals using the same healthcare databases as original investigators and evaluate the completeness of reporting for 250. Original and reproduction effect sizes were positively correlated (Pearson‚Äôs correlation‚Äâ=‚Äâ0.85), a strong relationship with some room for improvement. The median and interquartile range for the relative magnitude of effect (e.g., hazard ratio
              original
              /hazard ratio
              reproduction
              ) is 1.0 [0.9, 1.1], range [0.3, 2.1]. While the majority of results are closely reproduced, a subset are not. The latter can be explained by incomplete reporting and updated data. Greater methodological transparency aligned with new guidance may further improve reproducibility and validity assessment, thus facilitating evidence-based decision-making. Study registration number: EUPAS19636.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Nature Communications},
	author = {Wang, Shirley V. and Sreedhara, Sushama Kattinakere and Schneeweiss, Sebastian and {REPEAT Initiative} and Franklin, Jessica M. and Gagne, Joshua J. and Huybrechts, Krista F. and Patorno, Elisabetta and Jin, Yinzhu and Lee, Moa and Mahesri, Mufaddal and Pawar, Ajinkya and Barberio, Julie and Bessette, Lily G. and Chin, Kristyn and Gautam, Nileesa and Ortiz, Adrian Santiago and Sears, Ellen and Stefanini, Kristina and Zakarian, Mimi and Dejene, Sara and Rogers, James R. and Brill, Gregory and Landon, Joan and Lii, Joyce and Tsacogianis, Theodore and Vine, Seanna and Garry, Elizabeth M. and Gibbs, Liza R. and Gierada, Monica and Isaman, Danielle L. and Payne, Emma and Alwardt, Sarah and Arlett, Peter and Bartels, Dorothee B. and Bate, Andrew and Berlin, Jesse and Bourke, Alison and Bradbury, Brian and Brown, Jeffrey and Burnett, Karen and Brennan, Troyen and Chan, K. Arnold and Choi, Nam-Kyong and De Vries, Frank and Eichler, Hans-Georg and Filion, Kristian B. and Freeman, Lisa and Hallas, Jesper and Happe, Laura and Hennessy, Sean and J√≥nsson, P√°ll and Ioannidis, John and Jimenez, Javier and Kahler, Kristijan H. and Laine, Christine and Loder, Elizabeth and Makady, Amr and Martin, David and Nguyen, Michael and Nosek, Brian and Platt, Richard and Platt, Robert W. and Seeger, John and Shrank, William and Smeeth, Liam and S√∏rensen, Henrik Toft and Tugwell, Peter and Uyama, Yoshiaki and Willke, Richard and Winkelmayer, Wolfgang and Zarin, Deborah},
	month = aug,
	year = {2022},
	pages = {5126},
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = {351},
	url = {https://www.science.org/doi/10.1126/science.aaf0918},
	doi = {10.1126/science.aaf0918},
	abstract = {The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
	number = {6280},
	urldate = {2024-02-13},
	journal = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J√ºrgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	month = mar,
	year = {2016},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1433--1436},
}

@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in {Nature} and {Science} between 2010 and 2015},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Being able to replicate scientific findings is crucial for scientific progress1‚Äì15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516‚Äì36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
	language = {en},
	number = {9},
	urldate = {2024-02-13},
	journal = {Nature Human Behaviour},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J√ºrgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Economics, Psychology},
	pages = {637--644},
}

@article{cova_estimating_2021,
	title = {Estimating the {Reproducibility} of {Experimental} {Philosophy}},
	volume = {12},
	issn = {1878-5166},
	url = {https://doi.org/10.1007/s13164-018-0400-9},
	doi = {10.1007/s13164-018-0400-9},
	abstract = {Responding to recent concerns about the reliability of the published literature in psychology and other disciplines, we formed the X-Phi Replicability Project (XRP) to estimate the reproducibility of experimental philosophy (osf.io/dvkpr). Drawing on a representative sample of 40 x-phi studies published between 2003 and 2015, we enlisted 20 research teams across 8 countries to conduct a high-quality replication of each study in order to compare the results to the original published findings. We found that x-phi studies ‚Äì as represented in our sample ‚Äì successfully replicated about 70\% of the time. We discuss possible reasons for this relatively high replication rate in the field of experimental philosophy and offer suggestions for best research practices going forward.},
	language = {en},
	number = {1},
	urldate = {2024-02-13},
	journal = {Review of Philosophy and Psychology},
	author = {Cova, Florian and Strickland, Brent and Abatista, Angela and Allard, Aur√©lien and Andow, James and Attie, Mario and Beebe, James and Berni≈´nas, Renatas and Boudesseul, Jordane and Colombo, Matteo and Cushman, Fiery and Diaz, Rodrigo and N‚ÄôDjaye Nikolai van Dongen, Noah and Dranseika, Vilius and Earp, Brian D. and Torres, Antonio Gait√°n and Hannikainen, Ivar and Hern√°ndez-Conde, Jos√© V. and Hu, Wenjia and Jaquet, Fran√ßois and Khalifa, Kareem and Kim, Hanna and Kneer, Markus and Knobe, Joshua and Kurthy, Miklos and Lantian, Anthony and Liao, Shen-yi and Machery, Edouard and Moerenhout, Tania and Mott, Christian and Phelan, Mark and Phillips, Jonathan and Rambharose, Navin and Reuter, Kevin and Romero, Felipe and Sousa, Paulo and Sprenger, Jan and Thalabard, Emile and Tobia, Kevin and Viciana, Hugo and Wilkenfeld, Daniel and Zhou, Xiang},
	month = mar,
	year = {2021},
	pages = {9--44},
}

@article{errington_investigating_2021,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.71601},
	doi = {10.7554/eLife.71601},
	abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary ‚Äì the replication was either a success or a failure ‚Äì and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
	urldate = {2024-02-13},
	journal = {eLife},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	editor = {Pasqualini, Renata and Franco, Eduardo},
	month = dec,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {reproducibility},
	pages = {e71601},
}

@article{kirkby_quantitative_2023,
	title = {Quantitative {Macroeconomics}: {Lessons} {Learned} from {Fourteen} {Replications}},
	volume = {61},
	issn = {1572-9974},
	shorttitle = {Quantitative {Macroeconomics}},
	url = {https://doi.org/10.1007/s10614-022-10234-w},
	doi = {10.1007/s10614-022-10234-w},
	abstract = {I replicate all tables and figures from fourteen papers in Quantitative Macroeconomics, with an emphasis on incomplete market heterogeneous agent models. I report three main findings: (i) all (non-welfare related) major findings of the papers replicate, (ii) welfare findings based on linear approximation methods‚Äî1st-order perturbation, linear and log-linearization around steady-state, and linear-quadratic methods‚Äîshould be treated as quantitatively suspect, (iii) decisions around methods for discretizing exogenous shocks have a large and unappreciated influence on results and should be prominently discussed in papers. While some smaller aspects of the papers do not replicate exactly, rather than nitpick in the body of this paper I instead describe some lessons learnt that may be useful for practitioners working with Quantitative Macroeconomic models. The replications use global methods allowing for non-linearities and I argue that these are important and need to be more widely used. I provide a checklist that researchers can use when trying to check that their work will be more easily reproducible. Matlab codes implementing the replications using the VFI Toolkit are provided, and full results of all replications are given in the online appendix. I conclude with three core points for best practice: (i) codes be made directly available (e.g., on github, not only ‚Äôon request‚Äô, and not just inside a zip file), (ii) report not just baseline parameters but also hyperparameters, equilibrium values, non-baseline parameters and initial conditions, and (iii) replication means rewriting codes from scratch, not just re-running available codes.},
	language = {en},
	number = {2},
	urldate = {2024-02-11},
	journal = {Computational Economics},
	author = {Kirkby, Robert},
	month = feb,
	year = {2023},
	pages = {875--896},
}

@article{huntington-klein_influence_2021,
	title = {The influence of hidden researcher decisions in applied microeconomics},
	volume = {59},
	issn = {1465-7295},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ecin.12992},
	doi = {10.1111/ecin.12992},
	abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3‚Äì4 times the mean reported standard error.},
	language = {en},
	number = {3},
	urldate = {2024-02-11},
	journal = {Economic Inquiry},
	author = {Huntington-Klein, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R. and Burli, Pralhad and Chen, Naibin and Grieco, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecin.12992},
	pages = {944--960},
}

@article{irvine_law_2018,
	title = {Law and {Psychology} {Grows} {Up}, {Goes} {Online}, and {Replicates}},
	volume = {15},
	issn = {1740-1461},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jels.12180},
	doi = {10.1111/jels.12180},
	abstract = {Over the last 30 years, legal scholars have increasingly deployed experimental studies, particularly hypothetical scenarios, to test intuitions about legal reasoning and behavior. That movement has accelerated in the last decade, facilitated in large part by cheap and convenient Internet participant recruiting platforms like Amazon Mechanical Turk. The widespread use of online subjects, a practice that dramatically lowers the barriers to entry for experimental research, has been controversial. At the same time, the field of experimental psychology is experiencing a public crisis of confidence widely discussed in terms of the ‚Äúreplication crisis.‚Äù At present, law and psychology research is arguably in a new era, in which it is both an accepted feature of the legal landscape and also a target of fresh skepticism. The moment is ripe for taking stock. In this article, we bring an empirical approach to these problems. Using three canonical law and psychology findings, we document the challenges and the feasibility of reproducing results across platforms. We evaluate the extent to which we are able to reproduce the original findings with contemporary subject pools (Amazon Mechanical Turk, other national online platforms, and in-person labs). We partially replicate all three results, and show marked similarities in subject responses across platforms. In the context of the experiments here, we conclude that meaningful replication requires active intervention in order to keep the materials relevant and sensible. The second aim is to compare Amazon Mechanical Turk subjects to the original samples and to the replication samples. We find, consistent with the weight of recent evidence, that the Amazon Mechanical Turk samples are reasonably appropriate for these kinds of scenario studies. Subjects are highly similar to subjects on other online platforms and in-person samples, though they differ in their high level of attentiveness. Finally, we review the growing replication literature across disciplines, as well as our firsthand experience, to propose a set of standard practices for the publication of results in law and psychology.},
	language = {en},
	number = {2},
	urldate = {2024-02-11},
	journal = {Journal of Empirical Legal Studies},
	author = {Irvine, Krin and Hoffman, David A. and Wilkinson-Ryan, Tess},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jels.12180},
	pages = {320--355},
}

@article{wilensky_making_2007,
	title = {Making {Models} {Match}: {Replicating} an {Agent}-{Based} {Model}},
	volume = {10},
	abstract = {Scientists have increasingly employed computer models in their work. Recent years have seen a proliferation of agent-based models in the natural and social sciences. But with the exception of a few "classic" models, most of these models have never been replicated by anyone but the original developer. As replication is a critical component of the scientific method and a core practice of scientists, we argue herein for an increased practice of replication in the agentbased modeling community, and for widespread discussion of the issues surrounding replication. We begin by clarifying the concept of replication as it applies to ABM. Furthermore we argue that replication may have even greater benefits when applied to computational models than when applied to physical experiments. Replication of computational models affects model verification and validation and fosters shared understanding about modeling decisions. To facilitate replication, we must create standards for both how to replicate models and how to evaluate the replication. In this paper, we present a case study of our own attempt to replicate a classic agent-based model. We begin by describing an agent-based model from political science that was developed by Axelrod and Hammond. We then detail our effort to replicate that model and the challenges that arose in recreating the model and in determining if the replication was successful. We conclude this paper by discussing issues for (1) researchers attempting to replicate models and (2) researchers developing models in order to facilitate the replication of their results.},
	language = {en},
	number = {4},
	journal = {Journal of Artificial Societies and Social Simulation},
	author = {Wilensky, Uri and Rand, William},
	month = oct,
	year = {2007},
}

@article{yang_estimating_2020-1,
	title = {Estimating the deep replicability of scientific findings using human and artificial intelligence},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1909046117},
	doi = {10.1073/pnas.1909046117},
	abstract = {Significance
            After years of urgent concern about the failure of scientific papers to replicate, an accurate, scalable method for identifying findings at risk has yet to arrive. We present a method that combines machine intelligence and human acumen for estimating a study‚Äôs likelihood of replication. Our model‚Äîtrained and tested on hundreds of manually replicated studies and out-of-sample datasets ‚Äîis comparable to the best current methods, yet reduces the strain on researchers‚Äô resources. In practice, our model can complement prediction market and survey replication methods, prioritize studies for expensive manual replication tests, and furnish independent feedback to researchers prior to submitting a study for review.
          , 
            Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study‚Äôs replicability. Here, we trained an artificial intelligence model to estimate a paper‚Äôs replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model‚Äôs generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model‚Äôs predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like ‚Äúremarkable‚Äù or ‚Äúunexpected.‚Äù We did find that the model‚Äôs accuracy is higher when trained on a paper‚Äôs text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications‚Äîa task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
	language = {en},
	number = {20},
	urldate = {2024-08-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
	month = may,
	year = {2020},
	pages = {10762--10768},
}

@misc{bachmann_generalization_2022,
	title = {Generalization {Through} {The} {Lens} {Of} {Leave}-{One}-{Out} {Error}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2203.03443},
	doi = {10.48550/ARXIV.2203.03443},
	abstract = {Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings (Nagarajan \&amp; Kolter, 2019). In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning. Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Bachmann, Gregor and Hofmann, Thomas and Lucchi, Aur√©lien},
	year = {2022},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{simonsohn_p-curve_2014,
	title = {P-curve: {A} key to the file-drawer.},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	shorttitle = {P-curve},
	url = {https://doi.apa.org/doi/10.1037/a0033242},
	doi = {10.1037/a0033242},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Journal of Experimental Psychology: General},
	author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
	month = apr,
	year = {2014},
	pages = {534--547},
}

@article{bartos_z-curve_2022,
	title = {Z-curve 2.0: {Estimating} {Replication} {Rates} and {Discovery} {Rates}},
	volume = {6},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2003-2714},
	shorttitle = {Z-curve 2.0},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/2720},
	doi = {10.15626/MP.2021.2720},
	abstract = {Selection for statistical significance is a well-known factor that distorts the published literature and challenges the cumulative progress in science. Recent replication failures have fueled concerns that many published results are false-positives. Brunner and Schimmack (2020) developed z-curve, a method for estimating the expected replication rate (ERR) ‚Äì the predicted success rate of exact replication studies based on the mean power after selection for significance. This article introduces an extension of this method, z-curve 2.0. The main extension is an estimate of the expected discovery rate (EDR) ‚Äì the estimate of a proportion that the reported statistically significant results constitute from all conducted statistical tests. This information can be used to detect and quantify the amount of selection bias by comparing the EDR to the observed discovery rate (ODR; observed proportion of statistically significant results). In addition, we examined the performance of bootstrapped confidence intervals in simulation studies. Based on these results, we created robust confidence intervals with good coverage across a wide range of scenarios to provide information about the uncertainty in EDR and ERR estimates. We implemented the method in the zcurve R package (Barto≈° \& Schimmack, 2020).},
	urldate = {2024-08-08},
	journal = {Meta-Psychology},
	author = {Barto≈°, Franti≈°ek and Schimmack, Ulrich},
	month = sep,
	year = {2022},
}

@article{cumming_replication_2008,
	title = {Replication and \textit{p} {Intervals}: \textit{p} {Values} {Predict} the {Future} {Only} {Vaguely}, but {Confidence} {Intervals} {Do} {Much} {Better}},
	volume = {3},
	copyright = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Replication and \textit{p} {Intervals}},
	url = {http://journals.sagepub.com/doi/10.1111/j.1745-6924.2008.00079.x},
	doi = {10.1111/j.1745-6924.2008.00079.x},
	abstract = {Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is ‚ÄúSurprisingly little.‚Äù In one simulation of 25 repetitions of a typical experiment, p varied from {\textless}.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p {\textless}.00008, and fully a 10\% chance that p {\textgreater}.44. Remarkably, the interval‚Äîtermed a p interval‚Äîis this wide however large the sample size. p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and model-fitting techniques and by adopting meta-analytic thinking.},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Perspectives on Psychological Science},
	author = {Cumming, Geoff},
	month = jul,
	year = {2008},
	pages = {286--300},
}

@article{held_assessment_2019,
	title = {The assessment of intrinsic credibility and a new argument for \textit{p} {\textless} 0.005},
	volume = {6},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181534},
	doi = {10.1098/rsos.181534},
	abstract = {The concept of intrinsic credibility has been recently introduced to check the credibility of ‚Äòout of the blue‚Äô findings without any prior support. A significant result is deemed intrinsically credible if it is in conflict with a sceptical prior derived from the very same data that would make the effect just non-significant. In this paper, I propose to use Bayesian prior-predictive tail probabilities to assess intrinsic credibility. For the standard 5\% significance level, this leads to a new
              p
              -value threshold that is remarkably close to the recently proposed
              p
              {\textless} 0.005 standard. I also introduce the credibility ratio, the ratio of the upper to the lower limit (or
              vice versa
              ) of a confidence interval for a significant effect size. I show that the credibility ratio has to be smaller than 5.8 such that a significant finding is also intrinsically credible. Finally, a
              p
              -value for intrinsic credibility is proposed that is a simple function of the ordinary
              p
              -value and has a direct frequentist interpretation in terms of the probability of replicating an effect. An application to data from the Open Science Collaboration study on the reproducibility of psychological science suggests that intrinsic credibility of the original experiment is better suited to predict the success of a replication experiment than standard significance.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Royal Society Open Science},
	author = {Held, Leonhard},
	month = mar,
	year = {2019},
	pages = {181534},
}

@article{clemens_meaning_2017,
	title = {{THE} {MEANING} {OF} {FAILED} {REPLICATIONS}: {A} {REVIEW} {AND} {PROPOSAL}},
	volume = {31},
	issn = {0950-0804, 1467-6419},
	shorttitle = {{THE} {MEANING} {OF} {FAILED} {REPLICATIONS}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/joes.12139},
	doi = {10.1111/joes.12139},
	abstract = {Abstract
            
              The welcome rise of replication tests in economics has not been accompanied by a consensus standard for determining what constitutes a
              replication
              . A discrepant replication, in current usage of the term, can signal anything from an unremarkable disagreement over methods to scientific incompetence or misconduct. This paper proposes a standard for classifying one study as a replication of some other study. It is a standard that places the burden of proof on a study to demonstrate that it should have obtained identical results to the original, a conservative standard that is already used implicitly by many researchers. It contrasts this standard with decades of unsuccessful attempts to harmonize terminology, and argues that many prominent results described as replication tests should not be described as such. Adopting a conservative standard like this one can improve incentives for researchers, encouraging more and better replication tests.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Journal of Economic Surveys},
	author = {Clemens, Michael A.},
	month = feb,
	year = {2017},
	pages = {326--342},
}

@article{mathur_new_2020,
	title = {New {Statistical} {Metrics} for {Multisite} {Replication} {Projects}},
	volume = {183},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0964-1998, 1467-985X},
	url = {https://academic.oup.com/jrsssa/article/183/3/1145/7056432},
	doi = {10.1111/rssa.12572},
	abstract = {Summary
            Increasingly, researchers are attempting to replicate published original studies by using large, multisite replication projects, at least 134 of which have been completed or are on going. These designs are promising to assess whether the original study is statistically consistent with the replications and to reassess the strength of evidence for the scientific effect of interest. However, existing analyses generally focus on single replications; when applied to multisite designs, they provide an incomplete view of aggregate evidence and can lead to misleading conclusions about replication success. We propose new statistical metrics representing firstly the probability that the original study's point estimate would be at least as extreme as it actually was, if in fact the original study were statistically consistent with the replications, and secondly the estimated proportion of population effects agreeing in direction with the original study. Generalized versions of the second metric enable consideration of only meaningfully strong population effects that agree in direction, or alternatively that disagree in direction, with the original study. These metrics apply when there are at least 10 replications (unless the heterogeneity estimate œÑ{\textasciicircum}=0, in which case the metrics apply regardless of the number of replications). The first metric assumes normal population effects but appears robust to violations in simulations; the second is distribution free. We provide R packages (Replicate and MetaUtility).},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	month = jun,
	year = {2020},
	pages = {1145--1166},
}

@misc{zhao_statistical_2021,
	title = {Statistical {Assessment} of {Replicability} via {Bayesian} {Model} {Criticism}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2105.03993},
	doi = {10.48550/ARXIV.2105.03993},
	abstract = {Assessment of replicability is critical to ensure the quality and rigor of scientific research. In this paper, we discuss inference and modeling principles for replicability assessment. Targeting distinct application scenarios, we propose two types of Bayesian model criticism approaches to identify potentially irreproducible results in scientific experiments. They are motivated by established Bayesian prior and posterior predictive model-checking procedures and generalize many existing replicability assessment methods. Finally, we discuss the statistical properties of the proposed replicability assessment approaches and illustrate their usages by simulations and examples of real data analysis, including the data from the Reproducibility Project: Psychology and a systematic review of impacts of pre-existing cardiovascular disease on COVID-19 outcomes.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Zhao, Yi and Wen, Xiaoquan},
	year = {2021},
	note = {Version Number: 1},
	keywords = {Applications (stat.AP), FOS: Computer and information sciences, Methodology (stat.ME)},
}

@article{sumner_ripetascore_2022,
	title = {{RipetaScore}: {Measuring} the {Quality}, {Transparency}, and {Trustworthiness} of a {Scientific} {Work}},
	volume = {6},
	issn = {2504-0537},
	shorttitle = {{RipetaScore}},
	url = {https://www.frontiersin.org/articles/10.3389/frma.2021.751734/full},
	doi = {10.3389/frma.2021.751734},
	abstract = {A wide array of existing metrics quantifies a scientific paper's prominence or the author's prestige. Many who use these metrics make assumptions that higher citation counts or more public attention must indicate more reliable, better quality science. While current metrics offer valuable insight into scientific publications, they are an inadequate proxy for measuring the quality, transparency, and trustworthiness of published research. Three essential elements to establishing trust in a work include: trust in the paper, trust in the author, and trust in the data. To address these elements in a systematic and automated way, we propose the ripetaScore as a direct measurement of a paper's research practices, professionalism, and reproducibility. Using a sample of our current corpus of academic papers, we demonstrate the ripetaScore's efficacy in determining the quality, transparency, and trustworthiness of an academic work. In this paper, we aim to provide a metric to evaluate scientific reporting quality in terms of transparency and trustworthiness of the research, professionalism, and reproducibility.},
	urldate = {2024-08-08},
	journal = {Frontiers in Research Metrics and Analytics},
	author = {Sumner, Josh Q. and Vitale, Cynthia Hudson and McIntosh, Leslie D.},
	month = jan,
	year = {2022},
	pages = {751734},
}

@article{xiao_quantifying_2024,
	title = {Quantifying replicability of multiple studies in a meta-analysis},
	volume = {18},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-18/issue-1/Quantifying-replicability-of-multiple-studies-in-a-meta-analysis/10.1214/23-AOAS1806.full},
	doi = {10.1214/23-AOAS1806},
	number = {1},
	urldate = {2024-08-08},
	journal = {The Annals of Applied Statistics},
	author = {Xiao, Mengli and Chu, Haitao and Hodges, James S. and Lin, Lifeng},
	month = mar,
	year = {2024},
}

@article{fraser_predicting_2023-1,
	title = {Predicting reliability through structured expert elicitation with the {repliCATS} ({Collaborative} {Assessments} for {Trustworthy} {Science}) process},
	volume = {18},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0274429},
	doi = {10.1371/journal.pone.0274429},
	abstract = {As replications of individual studies are resource intensive, techniques for predicting the replicability are required. We introduce the repliCATS (Collaborative Assessments for Trustworthy Science) process, a new method for eliciting expert predictions about the replicability of research. This process is a structured expert elicitation approach based on a modified Delphi technique applied to the evaluation of research claims in social and behavioural sciences. The utility of processes to predict replicability is their capacity to test scientific claims without the costs of full replication. Experimental data supports the validity of this process, with a validation study producing a classification accuracy of 84\% and an Area Under the Curve of 0.94, meeting or exceeding the accuracy of other techniques used to predict replicability. The repliCATS process provides other benefits. It is highly scalable, able to be deployed for both rapid assessment of small numbers of claims, and assessment of high volumes of claims over an extended period through an online elicitation platform, having been used to assess 3000 research claims over an 18 month period. It is available to be implemented in a range of ways and we describe one such implementation. An important advantage of the repliCATS process is that it collects qualitative data that has the potential to provide insight in understanding the limits of generalizability of scientific claims. The primary limitation of the repliCATS process is its reliance on human-derived predictions with consequent costs in terms of participant fatigue although careful design can minimise these costs. The repliCATS process has potential applications in alternative peer review and in the allocation of effort for replication studies.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Bush, Martin and Wintle, Bonnie C. and Mody, Fallon and Smith, Eden T. and Hanea, Anca M. and Gould, Elliot and Hemming, Victoria and Hamilton, Daniel G. and Rumpff, Libby and Wilkinson, David P. and Pearson, Ross and Singleton Thorn, Felix and Ashton, Raquel and Willcox, Aaron and Gray, Charles T. and Head, Andrew and Ross, Melissa and Groenewegen, Rebecca and Marcoci, Alexandru and Vercammen, Ans and Parker, Timothy H. and Hoekstra, Rink and Nakagawa, Shinichi and Mandel, David R. and Van Ravenzwaaij, Don and McBride, Marissa and Sinnott, Richard O. and Vesk, Peter and Burgman, Mark and Fidler, Fiona},
	editor = {Catal√°-L√≥pez, Ferr√°n},
	month = jan,
	year = {2023},
	pages = {e0274429},
}

@article{mcguire_model-based_2021,
	title = {Model-based assessment of replicability for genome-wide association meta-analysis},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21226-z},
	doi = {10.1038/s41467-021-21226-z},
	abstract = {Abstract
            Genome-wide association meta-analysis (GWAMA) is an effective approach to enlarge sample sizes and empower the discovery of novel associations between genotype and phenotype. Independent replication has been used as a gold-standard for validating genetic associations. However, as current GWAMA often seeks to aggregate all available datasets, it becomes impossible to find a large enough independent dataset to replicate new discoveries. Here we introduce a method, MAMBA (Meta-Analysis Model-based Assessment of replicability), for assessing the ‚Äúposterior-probability-of-replicability‚Äù for identified associations by leveraging the strength and consistency of association signals between contributing studies. We demonstrate using simulations that MAMBA is more powerful and robust than existing methods, and produces more accurate genetic effects estimates. We apply MAMBA to a large-scale meta-analysis of addiction phenotypes with 1.2 million individuals. In addition to accurately identifying replicable common variant associations, MAMBA also pinpoints novel replicable rare variant associations from imputation-based GWAMA and hence greatly expands the set of analyzable variants.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Nature Communications},
	author = {McGuire, Daniel and Jiang, Yu and Liu, Mengzhen and Weissenkampen, J. Dylan and Eckert, Scott and Yang, Lina and Chen, Fang and {GWAS and Sequencing Consortium of Alcohol and Nicotine Use (GSCAN)} and Liu, Mengzhen and Jiang, Yu and Wedow, Robbee and Li, Yue and Brazel, David M. and Chen, Fang and Datta, Gargi and Davila-Velderrain, Jose and McGuire, Daniel and Tian, Chao and Zhan, Xiaowei and Choquet, H. √âl√©ne and Docherty, Anna R. and Faul, Jessica D. and Foerster, Johanna R. and Fritsche, Lars G. and Gabrielsen, Maiken Elvestad and Gordon, Scott D. and Haessler, Jeffrey and Hottenga, Jouke-Jan and Huang, Hongyan and Jang, Seon-Kyeong and Jansen, Philip R. and Ling, Yueh and Ma ÃàGi, Reedik and Matoba, Nana and McMahon, George and Mulas, Antonella and Orru, Valeria and Palviainen, Teemu and Pandit, Anita and Reginsson, Gunnar W. and Skogholt, Anne Heidi and Smith, Jennifer A. and Taylor, Amy E. and Turman, Constance and Willemsen, Gonneke and Young, Hannah and Young, Kendra A. and Zajac, Gregory J. M. and Zhao, Wei and Zhou, Wei and Bjornsdottir, Gyda and Boardman, Jason D. and Boehnke, Michael and Boomsma, Dorret I. and Chen, Chu and Cucca, Francesco and Davies, Gareth E. and Eaton, Charles B. and Ehringer, Marissa A. and Esko, To ÃÉNu and Fiorillo, Edoardo and Gillespie, Nathan A. and Gudbjartsson, Daniel F. and Haller, Toomas and Harris, Kathleen Mullan and Heath, Andrew C. and Hewitt, John K. and Hickie, Ian B. and Hokanson, John E. and Hopfer, Christian J. and Hunter, David J. and Iacono, William G. and Johnson, Eric O. and Kamatani, Yoichiro and Kardia, Sharon L. R. and Keller, Matthew C. and Kellis, Manolis and Kooperberg, Charles and Kraft, Peter and Krauter, Kenneth S. and Laakso, Markku and Lind, Penelope A. and Loukola, Anu and Lutz, Sharon M. and Madden, Pamela A. F. and Martin, Nicholas G. and McGue, Matt and McQueen, Matthew B. and Medland, Sarah E. and Metspalu, Andres and Mohlke, Karen L. and Nielsen, Jonas B. and Okada, Yukinori and Peters, Ulrike and Polderman, Tinca J. C. and Posthuma, Danielle and Reiner, Alexander P. and Rice, John P. and Rimm, Eric and Rose, Richard J. and Runarsdottir, Valgerdur and Stallings, Michael C. and StanÀáca ÃÅKova, Alena and Stefansson, Hreinn and Thai, Khanh K. and Tindle, Hilary A. and Tyrfingsson, Thorarinn and Wall, Tamara L. and Weir, David R. and Weisner, Constance and Whitfield, John B. and Winsvold, Bendik Slagsvold and Yin, Jie and Zuccolo, Luisa and Bierut, Laura J. and Hveem, Kristian and Lee, James J. and Munafo, Marcus R. and Saccone, Nancy L. and Willer, Cristen J. and Cornelis, Marilyn C. and David, Sean P. and Hinds, David and Jorgenson, Eric and Kaprio, Jaakko and Stitzel, Jerry A. and Stefansson, Kari and Thorgeirsson, Thorgeir E. and Abecasis, Goncalo and Liu, Dajiang J. and Vrieze, Scott and Berg, Arthur and Vrieze, Scott and Jiang, Bibo and Li, Qunhua and Liu, Dajiang J.},
	month = mar,
	year = {2021},
	pages = {1964},
}

@article{forbes_quantifying_2021,
	title = {Quantifying the {Reliability} and {Replicability} of {Psychopathology} {Network} {Characteristics}},
	volume = {56},
	issn = {0027-3171, 1532-7906},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2019.1616526},
	doi = {10.1080/00273171.2019.1616526},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Multivariate Behavioral Research},
	author = {Forbes, Miriam K. and Wright, Aidan G. C. and Markon, Kristian E. and Krueger, Robert F.},
	month = mar,
	year = {2021},
	pages = {224--242},
}

@article{matthews_methods_2001,
	title = {Methods for {Assessing} the {Credibility} of {Clinical} {Trial} {Outcomes}},
	volume = {35},
	copyright = {http://www.springer.com/tdm},
	issn = {0092-8615, 2164-9200},
	url = {http://link.springer.com/10.1177/009286150103500442},
	doi = {10.1177/009286150103500442},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Drug Information Journal},
	author = {Matthews, Robert A. J.},
	month = oct,
	year = {2001},
	pages = {1469--1478},
}

@article{higgins_measuring_2003,
	title = {Measuring inconsistency in meta-analyses},
	volume = {327},
	issn = {0959-8138, 1468-5833},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.327.7414.557},
	doi = {10.1136/bmj.327.7414.557},
	language = {en},
	number = {7414},
	urldate = {2024-08-08},
	journal = {BMJ},
	author = {Higgins, J. P T},
	month = sep,
	year = {2003},
	pages = {557--560},
}

@article{muradchanian_how_2021,
	title = {How best to quantify replication success? {A} simulation study on the comparison of replication success metrics},
	volume = {8},
	issn = {2054-5703},
	shorttitle = {How best to quantify replication success?},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.201697},
	doi = {10.1098/rsos.201697},
	abstract = {To overcome the frequently debated crisis of confidence, replicating studies is becoming increasingly more common. Multiple frequentist and Bayesian measures have been proposed to evaluate whether a replication is successful, but little is known about which method best captures replication success. This study is one of the first attempts to compare a number of quantitative measures of replication success with respect to their ability to draw the correct inference when the underlying truth is known, while taking publication bias into account. Our results show that Bayesian metrics seem to slightly outperform frequentist metrics across the board. Generally, meta-analytic approaches seem to slightly outperform metrics that evaluate single studies, except in the scenario of extreme publication bias, where this pattern reverses.},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Royal Society Open Science},
	author = {Muradchanian, Jasmine and Hoekstra, Rink and Kiers, Henk and Van Ravenzwaaij, Don},
	month = may,
	year = {2021},
	pages = {201697},
}

@article{baig_bayesian_2022,
	title = {Bayesian {Inference}: {Evaluating} {Replication} {Attempts} {With} {Bayes} {Factors}},
	volume = {24},
	issn = {1469-994X},
	shorttitle = {Bayesian {Inference}},
	url = {https://academic.oup.com/ntr/article/24/4/626/6407606},
	doi = {10.1093/ntr/ntab219},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Nicotine \& Tobacco Research},
	author = {Baig, Sabeeh A},
	month = mar,
	year = {2022},
	pages = {626--629},
}

@article{fletcher_how_2021,
	title = {How (not) to measure replication},
	volume = {11},
	issn = {1879-4912, 1879-4920},
	url = {https://link.springer.com/10.1007/s13194-021-00377-2},
	doi = {10.1007/s13194-021-00377-2},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {European Journal for Philosophy of Science},
	author = {Fletcher, Samuel C.},
	month = jun,
	year = {2021},
	pages = {57},
}

@article{guan_evaluating_2004,
	title = {Evaluating the {Replicability} of {Sample} {Results}: {A} {Tutorial} of {Double} {Cross}-{Validation} {Methods}},
	volume = {8},
	issn = {1091-367X, 1532-7841},
	shorttitle = {Evaluating the {Replicability} of {Sample} {Results}},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327841mpee0804_4},
	doi = {10.1207/s15327841mpee0804_4},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Measurement in Physical Education and Exercise Science},
	author = {Guan, Jianmin and Xiang, Ping and Keating, Xiaofen Deng},
	month = dec,
	year = {2004},
	pages = {227--241},
}

@article{fabrigar_conceptualizing_2016,
	title = {Conceptualizing and evaluating the replication of research results},
	volume = {66},
	issn = {00221031},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103115000967},
	doi = {10.1016/j.jesp.2015.07.009},
	language = {en},
	urldate = {2024-08-08},
	journal = {Journal of Experimental Social Psychology},
	author = {Fabrigar, Leandre R. and Wegener, Duane T.},
	month = sep,
	year = {2016},
	pages = {68--80},
}

@article{lin_assessing_2023,
	title = {Assessing the robustness of results from clinical trials and meta-analyses with the fragility index},
	volume = {228},
	issn = {00029378},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0002937822007207},
	doi = {10.1016/j.ajog.2022.08.053},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {American Journal of Obstetrics and Gynecology},
	author = {Lin, Lifeng and Xing, Aiwen and Chu, Haitao and Murad, M. Hassan and Xu, Chang and Baer, Benjamin R. and Wells, Martin T. and Sanchez-Ramos, Luis},
	month = mar,
	year = {2023},
	pages = {276--282},
}

@article{dixon_assessing_2020,
	title = {Assessing evidence for replication: {A} likelihood-based approach},
	volume = {52},
	issn = {1554-3528},
	shorttitle = {Assessing evidence for replication},
	url = {https://link.springer.com/10.3758/s13428-020-01403-6},
	doi = {10.3758/s13428-020-01403-6},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Behavior Research Methods},
	author = {Dixon, Peter and Glover, Scott},
	month = dec,
	year = {2020},
	pages = {2452--2459},
}

@article{van_aert_bayesian_2017,
	title = {Bayesian evaluation of effect size after replicating an original study},
	volume = {12},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0175302},
	doi = {10.1371/journal.pone.0175302},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {PLOS ONE},
	author = {Van Aert, Robbie C. M. and Van Assen, Marcel A. L. M.},
	editor = {Marinazzo, Daniele},
	month = apr,
	year = {2017},
	pages = {e0175302},
}

@article{schauer_evaluation_2021,
	title = {An evaluation of statistical methods for aggregate patterns of replication failure},
	volume = {15},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-15/issue-1/An-evaluation-of-statistical-methods-for-aggregate-patterns-of-replication/10.1214/20-AOAS1387.full},
	doi = {10.1214/20-AOAS1387},
	number = {1},
	urldate = {2024-08-08},
	journal = {The Annals of Applied Statistics},
	author = {Schauer, Jacob M. and Fitzgerald, Kaitlyn G. and Peko-Spicer, Sarah and Whalen, Mena C. R. and Zejnullahi, Rrita and Hedges, Larry V.},
	month = mar,
	year = {2021},
}

@article{mateu_towards_2024,
	title = {Towards more credible conceptual replications under heteroscedasticity and unbalanced designs},
	volume = {58},
	issn = {0033-5177, 1573-7845},
	url = {https://link.springer.com/10.1007/s11135-023-01657-0},
	doi = {10.1007/s11135-023-01657-0},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Quality \& Quantity},
	author = {Mateu, Pedro and Applegate, Brooks and Coryn, Chris L.},
	month = feb,
	year = {2024},
	pages = {723--751},
}

@article{lebel_unified_2018,
	title = {A {Unified} {Framework} to {Quantify} the {Credibility} of {Scientific} {Findings}},
	volume = {1},
	issn = {2515-2459, 2515-2467},
	url = {http://journals.sagepub.com/doi/10.1177/2515245918787489},
	doi = {10.1177/2515245918787489},
	abstract = {Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {LeBel, Etienne P. and McCarthy, Randy J. and Earp, Brian D. and Elson, Malte and Vanpaemel, Wolf},
	month = sep,
	year = {2018},
	pages = {389--402},
}

@misc{nordling_literature_2022,
	title = {A literature review of methods for assessment of reproducibility in science},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://www.researchsquare.com/article/rs-2267847/v4},
	doi = {10.21203/rs.3.rs-2267847/v4},
	abstract = {Abstract
          
            Introduction
            : In response to the US Congress petition, the National Academies of Sciences, Engineering, and Medicine investigated the status of reproducibility and replicability in science. A piece of work is reproducible if the same results can be obtained while following the methods under the same conditions and using the same data. Unavailable data, missing code, and unclear or incomplete method descriptions are common reasons for failure to reproduce results.
            Objectives
            : The motivation behind this review is to investigate the current methods for reproducibility assessment and analyze their strengths and weaknesses so that we can determine where there is room for improvement.
            Methods
            : We followed the PRISMA 2020 standard and conducted a literature review to find the current methods to assess the reproducibility of scientific articles. We made use of three databases for our search: Web of Science, Scopus, and Engineering Village. Our criteria to find relevant articles was to look for methods, algorithms, or techniques to evaluate, assess, or predict reproducibility in science. We discarded methods that were specific to a single study, or that could not be adapted to scientific articles in general.
            Results
            : We found ten articles describing methods to evaluate reproducibility, and classified them as either a prediction market, a survey, a machine learning algorithm, or a numerical method. A prediction market requires participants to bet on the reproducibility of a study. The surveys are simple and straightforward, but their performance has not been assessed rigorously. Two types of machine learning methods have been applied: handpicked features and natural language processing.
            Conclusion
            : While the machine learning methods are promising because they can be scaled to reduce time and cost for researchers, none of the models reviewed achieved an accuracy above 75\%. Given the prominence of transformer models for state-of-the-art natural language processing (NLP) tasks, we believe a transformer model can achieve better accuracy.},
	urldate = {2024-08-08},
	author = {Nordling, Torbj√∂rn and Peralta, Tomas Melo},
	month = dec,
	year = {2022},
}

@article{wilson_importance_2023,
	title = {On the importance of modeling the invisible world of underlying effect sizes},
	volume = {18},
	issn = {2569-653X},
	url = {https://spb.psychopen.eu/index.php/spb/article/view/9981},
	doi = {10.32872/spb.9981},
	abstract = {The headline findings from the Open Science Collaboration (2015)‚Äïnamely, that 36\% of original experiments replicated at p {\textless} .05, with the overall replication effect sizes being half as large as the original effects‚Äïcannot be meaningfully interpreted without a formal model. A simple model-based approach might ask: what would the state of original science be and what would replication results show if original experiments tested true effects half the time (prior odds = 1), true effects had a medium effect size (Cohen‚Äôs Œ¥ = 0.50), and power to detect true effects was 50\%? Assuming no questionable research practices, 91\% of p {\textless} .05 findings in the original literature would be true positives. However, only 58\% of original p {\textless} .05 findings would be expected to replicate using the Open Science Collaboration approach, and the replication effects overall would be only {\textasciitilde}60\% as large as the original effects. A minor variant of this model yields an expected replication rate of only 45\%, with overall replication effect sizes dropping by half. If the state of original science is as grim as a non-model-based (i.e., intuitive) interpretation of the Open Science Collaboration data suggests, should it be this easy to largely account for those findings using a model in which 91\% of statistically significant findings in the original science literature are true positives? Claims that the findings reported by the Open Science Collaboration indicate a replication crisis should not be based solely on intuition but should instead be accompanied by a specific model that supports that interpretation.
          , 
            Highlights
            
              
                
                  
                    Repeated failures to replicate
                    selected
                    high-profile findings from psychological science revealed that the small percentage of experiments that attract attention must be independently and directly replicated before being taken too seriously.
                  
                
                
                  
                    A similar message seemed to apply to the broader field of psychological science when the Open Science Collaboration (OSC) failed to replicate 64\% of
                    representative
                    findings.
                  
                
                
                  However, intuition notwithstanding, a simple formal model of the OSC results suggests that the substantial majority of the replicated findings are true positives.
                
                
                  We submit that any interpretation‚Äîfavorable or unfavorable‚Äîbased on the OSC results should rest on a model, not intuition.},
	urldate = {2024-08-08},
	journal = {Social Psychological Bulletin},
	author = {Wilson, Brent M. and Wixted, John T.},
	month = nov,
	year = {2023},
	pages = {e9981},
}

@article{mcshane_variation_2022,
	title = {Variation and {Covariation} in {Large}-{Scale} {Replication} {Projects}: {An} {Evaluation} of {Replicability}},
	volume = {117},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variation and {Covariation} in {Large}-{Scale} {Replication} {Projects}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2022.2054816},
	doi = {10.1080/01621459.2022.2054816},
	language = {en},
	number = {540},
	urldate = {2024-08-08},
	journal = {Journal of the American Statistical Association},
	author = {McShane, Blakeley B. and B√∂ckenholt, Ulf and Hansen, Karsten T.},
	month = oct,
	year = {2022},
	pages = {1605--1621},
}

@article{liou_bridging_2003,
	title = {Bridging {Functional} {MR} {Images} and {Scientific} {Inference}: {Reproducibility} {Maps}},
	volume = {15},
	issn = {0898-929X, 1530-8898},
	shorttitle = {Bridging {Functional} {MR} {Images} and {Scientific} {Inference}},
	url = {https://direct.mit.edu/jocn/article/15/7/935/3814/Bridging-Functional-MR-Images-and-Scientific},
	doi = {10.1162/089892903770007326},
	abstract = {Abstract
            Historically, reproducibility has been the sine qua non of experimental findings that are considered to be scientifically useful. Typically, findings from functional magnetic resonance imaging (fMRI) studies are assessed with statistical parametric maps (SPMs) using a p value threshold. However, a smaller p value does not imply that the observed result will be reproducible. In this study, we suggest interpreting SPMs in conjunction with reproducibility evidence. Reproducibility is defined as the extent to which the active status of a voxel remains the same across replicates conducted under the same conditions. We propose a methodology for assessing reproducibility in functional MR images without conducting separate experiments. Our procedures include the empirical Bayes method for estimating effects due to experimental stimuli, the threshold optimization procedure for assigning voxels to the active status, and the construction of reproducibility maps. In an empirical example, we implemented the proposed methodology to construct reproducibility maps based on data from the study by Ishai et al. (2000). The original experiments involved 12 human subjects and investigated brain regions most responsive to visual presentation of 3 categories of objects: faces, houses, and chairs. The brain regions identified included occipital, temporal, and fusiform gyri. Using our reproducibility analysis, we found that subjects in one of the experiments exercised at least 2 mechanisms in responding to visual objects when performing alternately matching and passive tasks. One gave activation maps closer to those reported in Ishai et al., and the other had related regions in the precuneus and posterior cingulate. The patterns of activated regions are reproducible for at least 4 out of 6 subjects involved in the experiment. Empirical application of the proposed methodology suggests that human brains exhibit different strategies to accomplish experimental tasks when responding to stimuli. It is important to correlate activations to subjects' behavior such as reaction time and response accuracy. Also, the latency between the stimulus presentation and the peak of the hemodynamic response function varies considerably among individual subjects according to types of stimuli and experimental tasks. These variations per se also deserve scientific inquiries. We conclude by discussing research directions relevant to reproducibility evidence in fMRI.},
	language = {en},
	number = {7},
	urldate = {2024-08-08},
	journal = {Journal of Cognitive Neuroscience},
	author = {Liou, Michelle and Su, Hong-Ren and Lee, Juin-Der and Cheng, Philip E. and Huang, Chien-Chih and Tsai, Chih-Hsin},
	month = oct,
	year = {2003},
	pages = {935--945},
}

@article{youyou_discipline-wide_2023,
	title = {A discipline-wide investigation of the replicability of {Psychology} papers over the past two decades},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2208863120},
	doi = {10.1073/pnas.2208863120},
	abstract = {Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (
              N
              = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper‚Äôs likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors‚Äô cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author‚Äôs university prestige and a paper‚Äôs citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability.},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Youyou, Wu and Yang, Yang and Uzzi, Brian},
	month = feb,
	year = {2023},
	pages = {e2208863120},
}

@article{braver_continuously_2014,
	title = {Continuously {Cumulating} {Meta}-{Analysis} and {Replicability}},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691614529796},
	doi = {10.1177/1745691614529796},
	abstract = {The current crisis in scientific psychology about whether our findings are irreproducible was presaged years ago by Tversky and Kahneman (1971), who noted that even sophisticated researchers believe in the fallacious Law of Small Numbers‚Äîerroneous intuitions about how imprecisely sample data reflect population phenomena. Combined with the low power of most current work, this often leads to the use of misleading criteria about whether an effect has replicated. Rosenthal (1990) suggested more appropriate criteria, here labeled the continuously cumulating meta-analytic (CCMA) approach. For example, a CCMA analysis on a replication attempt that does not reach significance might nonetheless provide more, not less, evidence that the effect is real. Alternatively, measures of heterogeneity might show that two studies that differ in whether they are significant might have only trivially different effect sizes. We present a nontechnical introduction to the CCMA framework (referencing relevant software), and then explain how it can be used to address aspects of replicability or more generally to assess quantitative evidence from numerous studies. We then present some examples and simulation results using the CCMA approach that show how the combination of evidence can yield improved results over the consideration of single studies.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Perspectives on Psychological Science},
	author = {Braver, Sanford L. and Thoemmes, Felix J. and Rosenthal, Robert},
	month = may,
	year = {2014},
	pages = {333--342},
}

@article{brunner_estimating_2020,
	title = {Estimating {Population} {Mean} {Power} {Under} {Conditions} of {Heterogeneity} and {Selection} for {Significance}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/874},
	doi = {10.15626/MP.2018.874},
	abstract = {In scientific fields that use significance tests, statistical power is important for successful replications of significant results because it is the long-run success rate in a series of exact replication studies. For any population of significant results, there is a population of power values of the statistical tests on which conclusions are based. We give exact theoretical results showing how selection for significance affects the distribution of statistical power in a heterogeneous population of significance tests. In a set of large-scale simulation studies, we compare four methods for estimating population mean power of a set of studies selected for significance (a maximum likelihood model, extensions of p-curve and p-uniform, \& z-curve). The p-uniform and p-curve methods performed well with a fixed effects size and varying sample sizes. However, when there was substantial variability in effect sizes as well as sample sizes, both methods systematically overestimate mean power. With heterogeneity in effect sizes, the maximum likelihood model produced the most accurate estimates when the distribution of effect sizes matched the assumptions of the model, but z-curve produced more accurate estimates when the assumptions of the maximum likelihood model were not met. We recommend the use of z-curve to estimate the typical power of significant results, which has implications for the replicability of significant results in psychology journals.},
	urldate = {2024-08-08},
	journal = {Meta-Psychology},
	author = {Brunner, Jerry and Schimmack, Ulrich},
	month = may,
	year = {2020},
}

@article{cumming_confidence_2006,
	title = {Confidence intervals and replication: {Where} will the next mean fall?},
	volume = {11},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Confidence intervals and replication},
	url = {https://doi.apa.org/doi/10.1037/1082-989X.11.3.217},
	doi = {10.1037/1082-989X.11.3.217},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Cumming, Geoff and Maillardet, Robert},
	year = {2006},
	pages = {217--227},
}

@article{maitra_re-defined_2010,
	title = {A re-defined and generalized percent-overlap-of-activation measure for studies of {fMRI} reproducibility and its use in identifying outlier activation maps},
	volume = {50},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811909012567},
	doi = {10.1016/j.neuroimage.2009.11.070},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {NeuroImage},
	author = {Maitra, Ranjan},
	month = mar,
	year = {2010},
	pages = {124--135},
}

@article{heller_replicability_2014,
	title = {Replicability analysis for genome-wide association studies},
	volume = {8},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-8/issue-1/Replicability-analysis-for-genome-wide-association-studies/10.1214/13-AOAS697.full},
	doi = {10.1214/13-AOAS697},
	number = {1},
	urldate = {2024-08-08},
	journal = {The Annals of Applied Statistics},
	author = {Heller, Ruth and Yekutieli, Daniel},
	month = mar,
	year = {2014},
}

@article{gelman_beyond_2014,
	title = {Beyond {Power} {Calculations}: {Assessing} {Type} {S} ({Sign}) and {Type} {M} ({Magnitude}) {Errors}},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Beyond {Power} {Calculations}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691614551642},
	doi = {10.1177/1745691614551642},
	abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction ( Type S [ sign] error) and (b) the factor by which the magnitude of an effect might be overestimated ( Type M [ magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Perspectives on Psychological Science},
	author = {Gelman, Andrew and Carlin, John},
	month = nov,
	year = {2014},
	pages = {641--651},
}

@article{heirene_call_2021,
	title = {A call for replications of addiction research: which studies should we replicate and what constitutes a ‚Äòsuccessful‚Äô replication?},
	volume = {29},
	issn = {1606-6359, 1476-7392},
	shorttitle = {A call for replications of addiction research},
	url = {https://www.tandfonline.com/doi/full/10.1080/16066359.2020.1751130},
	doi = {10.1080/16066359.2020.1751130},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Addiction Research \& Theory},
	author = {Heirene, Robert M.},
	month = mar,
	year = {2021},
	pages = {89--97},
}

@article{jiang_what_2016,
	title = {What is the probability of replicating a statistically significant association in genome-wide association studies?},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw091},
	doi = {10.1093/bib/bbw091},
	language = {en},
	urldate = {2024-08-08},
	journal = {Briefings in Bioinformatics},
	author = {Jiang, Wei and Xue, Jing-Hao and Yu, Weichuan},
	month = sep,
	year = {2016},
	pages = {bbw091},
}

@article{de_vet_when_2006,
	title = {When to use agreement versus reliability measures},
	volume = {59},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435606000291},
	doi = {10.1016/j.jclinepi.2005.10.015},
	language = {en},
	number = {10},
	urldate = {2024-08-08},
	journal = {Journal of Clinical Epidemiology},
	author = {De Vet, Henrica C.W. and Terwee, Caroline B. and Knol, Dirk L. and Bouter, Lex M.},
	month = oct,
	year = {2006},
	pages = {1033--1039},
}

@article{anderson_theres_2016,
	title = {There‚Äôs more than one way to conduct a replication study: {Beyond} statistical significance.},
	volume = {21},
	issn = {1939-1463, 1082-989X},
	shorttitle = {There‚Äôs more than one way to conduct a replication study},
	url = {https://doi.apa.org/doi/10.1037/met0000051},
	doi = {10.1037/met0000051},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Anderson, Samantha F. and Maxwell, Scott E.},
	year = {2016},
	pages = {1--12},
}

@article{pauli_statistical_2019,
	title = {A {Statistical} {Model} to {Investigate} the {Reproducibility} {Rate} {Based} on {Replication} {Experiments}},
	volume = {87},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12273},
	doi = {10.1111/insr.12273},
	abstract = {Summary
            
              The reproducibility crisis, that is, the fact that many scientific results are difficult to replicate, pointing to their unreliability or falsehood, is a hot topic in the recent scientific literature, and statistical methodologies, testing procedures and
              p
              ‚Äêvalues, in particular, are at the centre of the debate. Assessment of the extent of the problem‚Äìthe reproducibility rate or the false discovery rate‚Äìand the role of contributing factors are still an open problem. Replication experiments, that is, systematic replications of existing results, may offer relevant information on these issues. We propose a statistical model to deal with such information, in particular to estimate the reproducibility rate and the effect of some study characteristics on its reliability. We analyse data from a recent replication experiment in psychology finding a reproducibility rate broadly coherent with other assessments from the same experiment. Our results also confirm the expected role of some contributing factor (unexpectedness of the result and room for bias) while they suggest that the similarity between original study and the replica is not so relevant, thus mitigating some criticism directed to replication experiments.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {International Statistical Review},
	author = {Pauli, Francesco},
	month = apr,
	year = {2019},
	pages = {68--79},
}

@article{duvendack_what_2017,
	title = {What {Is} {Meant} by ‚Äú{Replication}‚Äù and {Why} {Does} {It} {Encounter} {Resistance} in {Economics}?},
	volume = {107},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.p20171031},
	doi = {10.1257/aer.p20171031},
	abstract = {This paper discusses recent trends in the use of replications in economics. We include the results of recent replication studies that have attempted to identify replication rates within the discipline. These studies generally find that replication rates are relatively low. We then consider obstacles to undertaking replication studies and highlight replication initiatives in psychology and political science, behind which economics appears to lag.},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {American Economic Review},
	author = {Duvendack, Maren and Palmer-Jones, Richard and Reed, W. Robert},
	month = may,
	year = {2017},
	pages = {46--51},
}

@article{dreber_using_2015,
	title = {Using prediction markets to estimate the reproducibility of scientific research},
	volume = {112},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1516179112},
	doi = {10.1073/pnas.1516179112},
	abstract = {Significance
            There is increasing concern about the reproducibility of scientific research. For example, the costs associated with irreproducible preclinical research alone have recently been estimated at US\$28 billion a year in the United States. However, there are currently no mechanisms in place to quickly identify findings that are unlikely to replicate. We show that prediction markets are well suited to bridge this gap. Prediction markets set up to estimate the reproducibility of 44 studies published in prominent psychology journals and replicated in The Reproducibility Project: Psychology predict the outcomes of the replications well and outperform a survey of individual forecasts.
          , 
            Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants‚Äô individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a ‚Äústatistically significant‚Äù finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
	language = {en},
	number = {50},
	urldate = {2024-08-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
	month = dec,
	year = {2015},
	pages = {15343--15347},
}

@article{pawel_sceptical_2022,
	title = {The {Sceptical} {Bayes} {Factor} for the {Assessment} of {Replication} {Success}},
	volume = {84},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/84/3/879/7056121},
	doi = {10.1111/rssb.12491},
	abstract = {Abstract
            Replication studies are increasingly conducted but there is no established statistical criterion for replication success. We propose a novel approach combining reverse-Bayes analysis with Bayesian hypothesis testing: a sceptical prior is determined for the effect size such that the original finding is no longer convincing in terms of a Bayes factor. This prior is then contrasted to an advocacy prior (the reference posterior of the effect size based on the original study), and replication success is declared if the replication data favour the advocacy over the sceptical prior at a higher level than the original data favoured the sceptical prior over the null hypothesis. The sceptical Bayes factor is the highest level where replication success can be declared. A comparison to existing methods reveals that the sceptical Bayes factor combines several notions of replicability: it ensures that both studies show sufficient evidence against the null and penalises incompatibility of their effect estimates. Analysis of asymptotic properties and error rates, as well as case studies from the Social Sciences Replication Project show the advantages of the method for the assessment of replicability.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Pawel, Samuel and Held, Leonhard},
	month = jul,
	year = {2022},
	pages = {879--911},
}

@article{thompson_pivotal_1994,
	title = {The {Pivotal} {Role} of {Replication} in {Psychological} {Research}: {Empirically} {Evaluating} the {Replicability} of {Sample} {Results}},
	volume = {62},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0022-3506, 1467-6494},
	shorttitle = {The {Pivotal} {Role} of {Replication} in {Psychological} {Research}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-6494.1994.tb00289.x},
	doi = {10.1111/j.1467-6494.1994.tb00289.x},
	abstract = {ABSTRACT
              This Article discusses reasons for the contemporary emphasis on evaluating the replicability of results from psychological research. Three logics for empirically evaluating the replicability of sample results‚Äîcross‚Äêvalidation, the jackknife, and the bootstrap‚Äîare described. A small heuristic data set is employed to make the discussion more concrete and accessible.},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Journal of Personality},
	author = {Thompson, Bruce},
	month = jun,
	year = {1994},
	pages = {157--176},
}

@article{held_assessment_2022,
	title = {The assessment of replication success based on relative effect size},
	volume = {16},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-16/issue-2/The-assessment-of-replication-success-based-on-relative-effect-size/10.1214/21-AOAS1502.full},
	doi = {10.1214/21-AOAS1502},
	number = {2},
	urldate = {2024-08-08},
	journal = {The Annals of Applied Statistics},
	author = {Held, Leonhard and Micheloud, Charlotte and Pawel, Samuel},
	month = jun,
	year = {2022},
}

@article{hedges_design_2021,
	title = {The {Design} of {Replication} {Studies}},
	volume = {184},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0964-1998, 1467-985X},
	url = {https://academic.oup.com/jrsssa/article/184/3/868/7068411},
	doi = {10.1111/rssa.12688},
	abstract = {Abstract
            Empirical evaluations of replication have become increasingly common, but there has been no unified approach to doing so. Some evaluations conduct only a single replication study while others run several, usually across multiple laboratories. Designing such programs has largely contended with difficult issues about which experimental components are necessary for a set of studies to be considered replications. However, another important consideration is that replication studies be designed to support sufficiently sensitive analyses. For instance, if hypothesis tests are to be conducted about replication, studies should be designed to ensure these tests are well-powered; if not, it can be difficult to determine conclusively if replication attempts succeeded or failed. This paper describes methods for designing ensembles of replication studies to ensure that they are both adequately sensitive and cost-efficient. It describes two potential analyses of replication studies‚Äîhypothesis tests and variance component estimation‚Äîand approaches to obtaining optimal designs for them. Using these results, it assesses the statistical power, precision of point estimators and optimality of the design used by the Many Labs Project and finds that while it may have been sufficiently powered to detect some larger differences between studies, other designs would have been less costly and/or produced more precise estimates or higher-powered hypothesis tests.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	month = jul,
	year = {2021},
	pages = {868--886},
}

@article{hildebrandt_rigor_2020,
	title = {Rigor and reproducibility for data analysis and design in the behavioral sciences},
	volume = {126},
	issn = {00057967},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005796720300036},
	doi = {10.1016/j.brat.2020.103552},
	language = {en},
	urldate = {2024-08-08},
	journal = {Behaviour Research and Therapy},
	author = {Hildebrandt, Tom and Prenoveau, Jason M.},
	month = mar,
	year = {2020},
	pages = {103552},
}

@article{belbasis_reproducibility_2022,
	title = {Reproducibility of prediction models in health services research},
	volume = {15},
	issn = {1756-0500},
	url = {https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-06082-4},
	doi = {10.1186/s13104-022-06082-4},
	abstract = {Abstract
            The field of health services research studies the health care system by examining outcomes relevant to patients and clinicians but also health economists and policy makers. Such outcomes often include health care spending, and utilization of care services. Building accurate prediction models using reproducible research practices for health services research is important for evidence-based decision making. Several systematic reviews have summarized prediction models for outcomes relevant to health services research, but these systematic reviews do not present a thorough assessment of reproducibility and research quality of the prediction modelling studies. In the present commentary, we discuss how recent advances in prediction modelling in other medical fields can be applied to health services research. We also describe the current status of prediction modelling in health services research, and we summarize available methodological guidance for the development, update, external validation and systematic appraisal of prediction models.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {BMC Research Notes},
	author = {Belbasis, Lazaros and Panagiotou, Orestis A.},
	month = dec,
	year = {2022},
	pages = {204},
}

@article{stroebe_alleged_2014,
	title = {The {Alleged} {Crisis} and the {Illusion} of {Exact} {Replication}},
	volume = {9},
	issn = {1745-6916, 1745-6924},
	url = {http://journals.sagepub.com/doi/10.1177/1745691613514450},
	doi = {10.1177/1745691613514450},
	abstract = {There has been increasing criticism of the way psychologists conduct and analyze studies. These critiques as well as failures to replicate several high-profile studies have been used as justification to proclaim a ‚Äúreplication crisis‚Äù in psychology. Psychologists are encouraged to conduct more ‚Äúexact‚Äù replications of published studies to assess the reproducibility of psychological research. This article argues that the alleged ‚Äúcrisis of replicability‚Äù is primarily due to an epistemological misunderstanding that emphasizes the phenomenon instead of its underlying mechanisms. As a consequence, a replicated phenomenon may not serve as a rigorous test of a theoretical hypothesis because identical operationalizations of variables in studies conducted at different times and with different subject populations might test different theoretical constructs. Therefore, we propose that for meaningful replications, attempts at reinstating the original circumstances are not sufficient. Instead, replicators must ascertain that conditions are realized that reflect the theoretical variable(s) manipulated (and/or measured) in the original study.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Perspectives on Psychological Science},
	author = {Stroebe, Wolfgang and Strack, Fritz},
	month = jan,
	year = {2014},
	pages = {59--71},
}

@article{hung_statistical_2020,
	title = {Statistical methods for replicability assessment},
	volume = {14},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-14/issue-3/Statistical-methods-for-replicability-assessment/10.1214/20-AOAS1336.full},
	doi = {10.1214/20-AOAS1336},
	number = {3},
	urldate = {2024-08-08},
	journal = {The Annals of Applied Statistics},
	author = {Hung, Kenneth and Fithian, William},
	month = sep,
	year = {2020},
}

@article{hedges_statistical_2019,
	title = {Statistical analyses for studying replication: {Meta}-analytic perspectives.},
	volume = {24},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Statistical analyses for studying replication},
	url = {https://doi.apa.org/doi/10.1037/met0000189},
	doi = {10.1037/met0000189},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	month = oct,
	year = {2019},
	pages = {557--570},
}

@article{simonsohn_small_2015,
	title = {Small {Telescopes}: {Detectability} and the {Evaluation} of {Replication} {Results}},
	volume = {26},
	issn = {0956-7976, 1467-9280},
	shorttitle = {Small {Telescopes}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797614567341},
	doi = {10.1177/0956797614567341},
	abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ‚Äúunsuccessful‚Äù replication attempts (i.e., studies yielding p {\textgreater} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ‚Äúprotecting‚Äù true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Psychological Science},
	author = {Simonsohn, Uri},
	month = may,
	year = {2015},
	pages = {559--569},
}

@article{bonett_replication-extension_2012,
	title = {Replication-{Extension} {Studies}},
	volume = {21},
	issn = {0963-7214, 1467-8721},
	url = {http://journals.sagepub.com/doi/10.1177/0963721412459512},
	doi = {10.1177/0963721412459512},
	abstract = {Replication-extension studies combine results from prior studies with results from a new study specifically designed to replicate and extend the results of the prior studies. Replication-extension studies have many advantages over the traditional single-study designs used in psychology: Formal assessments of replication can be obtained, effect sizes can be estimated with greater precision and generalizability, misleading findings from prior studies can be exposed, and moderator effects can be assessed.},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Current Directions in Psychological Science},
	author = {Bonett, Douglas G.},
	month = dec,
	year = {2012},
	pages = {409--412},
}

@article{nosek_replicability_2022,
	title = {Replicability, {Robustness}, and {Reproducibility} in {Psychological} {Science}},
	volume = {73},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-020821-114157},
	doi = {10.1146/annurev-psych-020821-114157},
	abstract = {Replication‚Äîan important, uncommon, and misunderstood practice‚Äîis gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Annual Review of Psychology},
	author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur√©lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich√®le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch√∂nbrodt, Felix D. and Vazire, Simine},
	month = jan,
	year = {2022},
	pages = {719--748},
}

@article{wang_replicability_2022,
	title = {Replicability in cancer omics data analysis: measures and empirical explorations},
	volume = {23},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1467-5463, 1477-4054},
	shorttitle = {Replicability in cancer omics data analysis},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbac304/6649493},
	doi = {10.1093/bib/bbac304},
	abstract = {Abstract
            In biomedical research, the replicability of findings across studies is highly desired. In this study, we focus on cancer omics data, for which the examination of replicability has been mostly focused on important omics variables identified in different studies. In published literature, although there have been extensive attention and ad hoc discussions, there is insufficient quantitative research looking into replicability measures and their properties. The goal of this study is to fill this important knowledge gap. In particular, we consider three sensible replicability measures, for which we examine distributional properties and develop a way of making inference. Applying them to three The Cancer Genome Atlas (TCGA) datasets reveals in general low replicability and significant across-data variations. To further comprehend such findings, we resort to simulation, which confirms the validity of the findings with the TCGA data and further informs the dependence of replicability on signal level (or equivalently sample size). Overall, this study can advance our understanding of replicability for cancer omics and other studies that have identification as a key goal.},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Briefings in Bioinformatics},
	author = {Wang, Jiping and Liang, Hongmin and Zhang, Qingzhao and Ma, Shuangge},
	month = sep,
	year = {2022},
	pages = {bbac304},
}

@article{mcintosh_repeat_2017,
	title = {Repeat: a framework to assess empirical reproducibility in biomedical research},
	volume = {17},
	issn = {1471-2288},
	shorttitle = {Repeat},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0377-6},
	doi = {10.1186/s12874-017-0377-6},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {BMC Medical Research Methodology},
	author = {McIntosh, Leslie D. and Juehne, Anthony and Vitale, Cynthia R. H. and Liu, Xiaoyan and Alcoser, Rosalia and Lukas, J. Christian and Evanoff, Bradley},
	month = dec,
	year = {2017},
	pages = {143},
}

@article{farrar_replications_2020,
	title = {Replications in {Comparative} {Cognition}: {What} {Should} {We} {Expect} and {How} {Can} {We} {Improve}?},
	volume = {7},
	issn = {23725052, 23724323},
	shorttitle = {Replications in {Comparative} {Cognition}},
	url = {http://www.animalbehaviorandcognition.org/article.php?id=1197},
	doi = {10.26451/abc.07.01.02.2020},
	number = {1},
	urldate = {2024-08-08},
	journal = {Animal Behavior and Cognition},
	author = {Farrar, Benjamin and Boeckle, Markus and Clayton, Nicola},
	month = feb,
	year = {2020},
	pages = {1--22},
}

@article{luijken_replicability_2024,
	title = {Replicability of simulation studies for the investigation of statistical methods: the {RepliSims} project},
	volume = {11},
	issn = {2054-5703},
	shorttitle = {Replicability of simulation studies for the investigation of statistical methods},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.231003},
	doi = {10.1098/rsos.231003},
	abstract = {Results of simulation studies evaluating the performance of statistical methods can have a major impact on the way empirical research is implemented. However, so far there is limited evidence of the replicability of simulation studies. Eight highly cited statistical simulation studies were selected, and their replicability was assessed by teams of replicators with formal training in quantitative methodology. The teams used information in the original publications to write simulation code with the aim of replicating the results. The primary outcome was to determine the feasibility of replicability based on reported information in the original publications and supplementary materials. Replicasility varied greatly: some original studies provided detailed information leading to almost perfect replication of results, whereas other studies did not provide enough information to implement any of the reported simulations. Factors facilitating replication included availability of code, detailed reporting or visualization of data-generating procedures and methods, and replicator expertise. Replicability of statistical simulation studies was mainly impeded by lack of information and sustainability of information sources. We encourage researchers publishing simulation studies to transparently report all relevant implementation details either in the research paper itself or in easily accessible supplementary material and to make their simulation code publicly available using permanent links.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Royal Society Open Science},
	author = {Luijken, K. and Lohmann, A. and Alter, U. and Claramunt Gonzalez, J. and Clouth, F. J. and Fossum, J. L. and Hesen, L. and Huizing, A. H. J. and Ketelaar, J. and Montoya, A. K. and Nab, L. and Nijman, R. C. C. and Penning De Vries, B. B. L. and Tibbe, T. D. and Wang, Y. A. and Groenwold, R. H. H.},
	month = jan,
	year = {2024},
	pages = {231003},
}

@article{schauer_reconsidering_2021,
	title = {Reconsidering statistical methods for assessing replication.},
	volume = {26},
	issn = {1939-1463, 1082-989X},
	url = {https://doi.apa.org/doi/10.1037/met0000302},
	doi = {10.1037/met0000302},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Schauer, J. M. and Hedges, L. V.},
	month = feb,
	year = {2021},
	pages = {127--139},
}

@article{erdfelder_zur_2018,
	title = {Zur {Methodologie} von {Replikationsstudien}},
	volume = {69},
	issn = {0033-3042, 2190-6238},
	url = {https://econtent.hogrefe.com/doi/10.1026/0033-3042/a000387},
	doi = {10.1026/0033-3042/a000387},
	abstract = {Zusammenfassung. Replikationsstudien sind in den empirischen Wissenschaften mit unterschiedlichen Zielen verbunden, abh√§ngig davon, ob wir uns im Kontext der Theorieentwicklung oder im Kontext der Theorie√ºberpr√ºfung bewegen (Entdeckungs- vs. Begr√ºndungszusammenhang sensu Reichenbach, 1938 ). Konzeptuelle Replikationsstudien zielen auf Generalisierung ab und k√∂nnen im Entdeckungszusammenhang n√ºtzlich sein. Direkte Replikationsstudien zielen demgegen√ºber auf den Nachweis der Replizierbarkeit eines bestimmten Forschungsergebnisses unter unabh√§ngigen Bedingungen ab und sind im Begr√ºndungszusammenhang unverzichtbar. Ohne die Annahme der direkten Replizierbarkeit wird man sich kaum auf allgemein akzeptierte empirische Tatbest√§nde einigen k√∂nnen, die eine notwendige Voraussetzung f√ºr Theorie√ºberpr√ºfungen in den empirischen Wissenschaften sind. Vor diesem Hintergrund werden Standards f√ºr Replikationsstudien vorgeschlagen und begr√ºndet. Eine Besonderheit in der Psychologie besteht darin, dass das Replikandum in aller Regel eine statistische Hypothese ist, √ºber die lediglich probabilistisch entschieden werden kann. Dies wirft Folgeprobleme in Bezug auf die Formulierung der Replizierbarkeitshypothese, die Kontrolle statistischer Fehlerwahrscheinlichkeiten bei der Entscheidung √ºber die Replizierbarkeitshypothese, die Bestimmung der zu entdeckenden Effektgr√∂√üe bei Verzerrung vorliegender Ergebnisse durch Publication Bias, die Festlegung des Stichprobenumfangs und die korrekte Interpretation der Replikationsquote auf, f√ºr die L√∂sungsvorschl√§ge unterbreitet und diskutiert werden.},
	language = {de},
	number = {1},
	urldate = {2024-08-08},
	journal = {Psychologische Rundschau},
	author = {Erdfelder, Edgar and Ulrich, Rolf},
	month = jan,
	year = {2018},
	pages = {3--21},
}

@article{costigan_performing_2024,
	title = {Performing {Small}-{Telescopes} {Analysis} by {Resampling}: {Empirically} {Constructing} {Confidence} {Intervals} and {Estimating} {Statistical} {Power} for {Measures} of {Effect} {Size}},
	volume = {7},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Performing {Small}-{Telescopes} {Analysis} by {Resampling}},
	url = {http://journals.sagepub.com/doi/10.1177/25152459241227865},
	doi = {10.1177/25152459241227865},
	abstract = {When new data are collected to check the findings of an original study, it can be challenging to evaluate replication results. The small-telescopes method is designed to assess not only whether the effect observed in the replication study is statistically significant but also whether this effect is large enough to have been detected in the original study. Unless both criteria are met, the replication either fails to support the original findings or the results are mixed. When implemented in the conventional manner, this small-telescopes method can be impractical or impossible to conduct, and doing so often requires parametric assumptions that may not be satisfied. We present an empirical approach that can be used for a variety of study designs and data-analytic techniques. The empirical approach to the small-telescopes method is intended to extend its reach as a tool for addressing the replication crisis by evaluating findings in psychological science and beyond. In the present tutorial, we demonstrate this approach using a Shiny app and R code and included an analysis of most studies (95\%) replicated as part of the Open Science Collaboration‚Äôs Reproducibility Project in Psychology. In addition to its versatility, simulations demonstrate the accuracy and precision of the empirical approach to implementing small-telescopes analysis.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Costigan, Samantha and Ruscio, John and Crawford, Jarret T.},
	month = jan,
	year = {2024},
	pages = {25152459241227865},
}

@article{gonzalez-barahona_reproducibility_2012,
	title = {On the reproducibility of empirical software engineering studies based on data retrieved from development repositories},
	volume = {17},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/10.1007/s10664-011-9181-9},
	doi = {10.1007/s10664-011-9181-9},
	language = {en},
	number = {1-2},
	urldate = {2024-08-08},
	journal = {Empirical Software Engineering},
	author = {Gonz√°lez-Barahona, Jes√∫s M. and Robles, Gregorio},
	month = feb,
	year = {2012},
	pages = {75--89},
}

@article{wilson_importance_2023-1,
	title = {On the importance of modeling the invisible world of underlying effect sizes},
	volume = {18},
	issn = {2569-653X},
	url = {https://spb.psychopen.eu/index.php/spb/article/view/9981},
	doi = {10.32872/spb.9981},
	abstract = {The headline findings from the Open Science Collaboration (2015)‚Äïnamely, that 36\% of original experiments replicated at p {\textless} .05, with the overall replication effect sizes being half as large as the original effects‚Äïcannot be meaningfully interpreted without a formal model. A simple model-based approach might ask: what would the state of original science be and what would replication results show if original experiments tested true effects half the time (prior odds = 1), true effects had a medium effect size (Cohen‚Äôs Œ¥ = 0.50), and power to detect true effects was 50\%? Assuming no questionable research practices, 91\% of p {\textless} .05 findings in the original literature would be true positives. However, only 58\% of original p {\textless} .05 findings would be expected to replicate using the Open Science Collaboration approach, and the replication effects overall would be only {\textasciitilde}60\% as large as the original effects. A minor variant of this model yields an expected replication rate of only 45\%, with overall replication effect sizes dropping by half. If the state of original science is as grim as a non-model-based (i.e., intuitive) interpretation of the Open Science Collaboration data suggests, should it be this easy to largely account for those findings using a model in which 91\% of statistically significant findings in the original science literature are true positives? Claims that the findings reported by the Open Science Collaboration indicate a replication crisis should not be based solely on intuition but should instead be accompanied by a specific model that supports that interpretation.
          , 
            Highlights
            
              
                
                  
                    Repeated failures to replicate
                    selected
                    high-profile findings from psychological science revealed that the small percentage of experiments that attract attention must be independently and directly replicated before being taken too seriously.
                  
                
                
                  
                    A similar message seemed to apply to the broader field of psychological science when the Open Science Collaboration (OSC) failed to replicate 64\% of
                    representative
                    findings.
                  
                
                
                  However, intuition notwithstanding, a simple formal model of the OSC results suggests that the substantial majority of the replicated findings are true positives.
                
                
                  We submit that any interpretation‚Äîfavorable or unfavorable‚Äîbased on the OSC results should rest on a model, not intuition.},
	urldate = {2024-08-08},
	journal = {Social Psychological Bulletin},
	author = {Wilson, Brent M. and Wixted, John T.},
	month = nov,
	year = {2023},
	pages = {e9981},
}

@article{schauer_accuracy_2023,
	title = {On the {Accuracy} of {Replication} {Failure} {Rates}},
	volume = {58},
	issn = {0027-3171, 1532-7906},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2022.2066500},
	doi = {10.1080/00273171.2022.2066500},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Multivariate Behavioral Research},
	author = {Schauer, Jacob M.},
	month = may,
	year = {2023},
	pages = {598--615},
}

@article{hedges_more_2019,
	title = {More {Than} {One} {Replication} {Study} {Is} {Needed} for {Unambiguous} {Tests} of {Replication}},
	volume = {44},
	issn = {1076-9986, 1935-1054},
	url = {http://journals.sagepub.com/doi/10.3102/1076998619852953},
	doi = {10.3102/1076998619852953},
	abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98\%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	month = oct,
	year = {2019},
	pages = {543--570},
}

@article{song_making_2021,
	title = {Making {Sense} of {Model} {Generalizability}: {A} {Tutorial} on {Cross}-{Validation} in {R} and {Shiny}},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	shorttitle = {Making {Sense} of {Model} {Generalizability}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245920947067},
	doi = {10.1177/2515245920947067},
	abstract = {Model generalizability describes how well the findings from a sample are applicable to other samples in the population. In this Tutorial, we explain model generalizability through the statistical concept of model overfitting and its outcome (i.e., validity shrinkage in new samples), and we use a Shiny app to simulate and visualize how model generalizability is influenced by three factors: model complexity, sample size, and effect size. We then discuss cross-validation as an approach for evaluating model generalizability and provide guidelines for implementing this approach. To help researchers understand how to apply cross-validation to their own research, we walk through an example, accompanied by step-by-step illustrations in R. This Tutorial is expected to help readers develop the basic knowledge and skills to use cross-validation to evaluate model generalizability in their research and practice.},
	language = {en},
	number = {1},
	urldate = {2024-08-08},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Song, Q. Chelsea and Tang, Chen and Wee, Serena},
	month = jan,
	year = {2021},
	pages = {251524592094706},
}

@article{zwaan_making_2018,
	title = {Making replication mainstream},
	volume = {41},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X17001972/type/journal_article},
	doi = {10.1017/S0140525X17001972},
	abstract = {Abstract
            Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A finding is elevated from single observation to scientific evidence when the procedures that were used to obtain it can be reproduced and the finding itself can be replicated. Recent replication attempts show that some high profile results¬†‚Äì¬†most notably in psychology, but in many other disciplines as well¬†‚Äì¬†cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.},
	language = {en},
	urldate = {2024-08-08},
	journal = {Behavioral and Brain Sciences},
	author = {Zwaan, Rolf A. and Etz, Alexander and Lucas, Richard E. and Donnellan, M. Brent},
	year = {2018},
	pages = {e120},
}

@article{puolivali_influence_2020,
	title = {Influence of multiple hypothesis testing on reproducibility in neuroimaging research: {A} simulation study and {Python}-based software},
	volume = {337},
	issn = {01650270},
	shorttitle = {Influence of multiple hypothesis testing on reproducibility in neuroimaging research},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027020300765},
	doi = {10.1016/j.jneumeth.2020.108654},
	language = {en},
	urldate = {2024-08-08},
	journal = {Journal of Neuroscience Methods},
	author = {Puoliv√§li, Tuomas and Palva, Satu and Palva, J. Matias},
	month = may,
	year = {2020},
	pages = {108654},
}

@article{tsai_generalizability_2012,
	title = {Generalizability {Analyses} of {NBDE} {Part} {II}},
	volume = {35},
	issn = {0163-2787, 1552-3918},
	url = {http://journals.sagepub.com/doi/10.1177/0163278711425382},
	doi = {10.1177/0163278711425382},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Evaluation \& the Health Professions},
	author = {Tsai, Tsung-Hsun and Shin, Chingwei David and Neumann, Laura M. and Grau, Barry W.},
	month = jun,
	year = {2012},
	pages = {169--181},
}

@article{fidler_metaresearch_2017,
	title = {Metaresearch for {Evaluating} {Reproducibility} in {Ecology} and {Evolution}},
	issn = {0006-3568, 1525-3244},
	url = {https://academic.oup.com/bioscience/article-lookup/doi/10.1093/biosci/biw159},
	doi = {10.1093/biosci/biw159},
	language = {en},
	urldate = {2024-08-08},
	journal = {BioScience},
	author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
	month = jan,
	year = {2017},
	pages = {biw159},
}

@article{wong_design-based_2022,
	title = {Design-{Based} {Approaches} to {Causal} {Replication} {Studies}},
	volume = {23},
	issn = {1389-4986, 1573-6695},
	url = {https://link.springer.com/10.1007/s11121-021-01234-7},
	doi = {10.1007/s11121-021-01234-7},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Prevention Science},
	author = {Wong, Vivian C. and Anglin, Kylie and Steiner, Peter M.},
	month = jul,
	year = {2022},
	pages = {723--738},
}

@article{tackett_conceptualizing_2018,
	title = {Conceptualizing and evaluating replication across domains of behavioral research},
	volume = {41},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X18000882/type/journal_article},
	doi = {10.1017/S0140525X18000882},
	abstract = {Abstract
            We discuss the authors' conceptualization of replication, in particular the false dichotomy of direct versus conceptual replication intrinsic to it, and suggest a broader one that better generalizes to other domains of psychological research. We also discuss their approach to the evaluation of replication results and suggest moving beyond their dichotomous statistical paradigms and employing hierarchical/meta-analytic statistical models.},
	language = {en},
	urldate = {2024-08-08},
	journal = {Behavioral and Brain Sciences},
	author = {Tackett, Jennifer L. and McShane, Blakeley B.},
	year = {2018},
	pages = {e152},
}

@article{borsboom_false_2017,
	title = {False alarm? {A} comprehensive reanalysis of ‚Äú{Evidence} that psychopathology symptom networks have limited replicability‚Äù by {Forbes}, {Wright}, {Markon}, and {Krueger} (2017).},
	volume = {126},
	issn = {1939-1846, 0021-843X},
	shorttitle = {False alarm?},
	url = {https://doi.apa.org/doi/10.1037/abn0000306},
	doi = {10.1037/abn0000306},
	language = {en},
	number = {7},
	urldate = {2024-08-08},
	journal = {Journal of Abnormal Psychology},
	author = {Borsboom, Denny and Fried, Eiko I. and Epskamp, Sacha and Waldorp, Lourens J. and Van Borkulo, Claudia D. and Van Der Maas, Han L. J. and Cramer, Ang√©lique O. J.},
	month = oct,
	year = {2017},
	pages = {989--999},
}

@article{cobey_epidemiological_2023,
	title = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: {A} scoping review of articles published in 2018-2019},
	volume = {12},
	issn = {2050-084X},
	shorttitle = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines},
	url = {https://elifesciences.org/articles/78518},
	doi = {10.7554/eLife.78518},
	abstract = {Background:
              Reproducibility is a central tenant of research. We aimed to synthesize the literature on reproducibility and describe its epidemiological characteristics, including how reproducibility is defined and assessed. We also aimed to determine and compare estimates for reproducibility across different fields.
            
            
              Methods:
              We conducted a scoping review to identify English language replication studies published between 2018 and 2019 in economics, education, psychology, health sciences, and biomedicine. We searched Medline, Embase, PsycINFO, Cumulative Index of Nursing and Allied Health Literature ‚Äì CINAHL, Education Source via EBSCOHost, ERIC, EconPapers, International Bibliography of the Social Sciences (IBSS), and EconLit. Documents retrieved were screened in duplicate against our inclusion criteria. We extracted year of publication, number of authors, country of affiliation of the corresponding author, and whether the study was funded. For the individual replication studies, we recorded whether a registered protocol for the replication study was used, whether there was contact between the reproducing team and the original authors, what study design was used, and what the primary outcome was. Finally, we recorded how reproducibilty was defined by the authors, and whether the assessed study(ies) successfully reproduced based on this definition. Extraction was done by a single reviewer and quality controlled by a second reviewer.
            
            
              Results:
              Our search identified 11,224 unique documents, of which 47 were included in this review. Most studies were related to either psychology (48.6\%) or health sciences (23.7\%). Among these 47 documents, 36 described a single reproducibility study while the remaining 11 reported at least two reproducibility studies in the same paper. Less than the half of the studies referred to a registered protocol. There was variability in the definitions of reproduciblity success. In total, across the 47 documents 177 studies were reported. Based on the definition used by the author of each study, 95 of 177 (53.7\%) studies reproduced.
            
            
              Conclusions:
              This study gives an overview of research across five disciplines that explicitly set out to reproduce previous research. Such reproducibility studies are extremely scarce, the definition of a successfully reproduced study is ambiguous, and the reproducibility rate is overall modest.
            
            
              Funding:
              No external funding was received for this work},
	language = {en},
	urldate = {2024-08-08},
	journal = {eLife},
	author = {Cobey, Kelly D and Fehlmann, Christophe A and Christ Franco, Marina and Ayala, Ana Patricia and Sikora, Lindsey and Rice, Danielle B and Xu, Chenchen and Ioannidis, John Pa and Lalu, Manoj M and M√©nard, Alixe and Neitzel, Andrew and Nguyen, Bea and Tsertsvadze, Nino and Moher, David},
	month = jun,
	year = {2023},
	pages = {e78518},
}

@article{bonett_design_2021,
	title = {Design and {Analysis} of {Replication} {Studies}},
	volume = {24},
	issn = {1094-4281, 1552-7425},
	url = {http://journals.sagepub.com/doi/10.1177/1094428120911088},
	doi = {10.1177/1094428120911088},
	abstract = {Issues surrounding the importance and interpretation of replication research have generated considerable debate and controversy in recent years. Some of the controversy can be attributed to imprecise and inadequate specifications of the statistical criteria needed to assess replication and nonreplication. Two types of statistical replication evidence and four types of statistical nonreplication evidence are described. In addition, three types of inconclusive statistical replication evidence are described. An important benefit of a replication study is the ability to combine an effect-size estimate from the original study with an effect-size estimate from the follow-up study to obtain a more precise and generalizable effect-size estimate. The sample size in the follow-up study is an important design consideration, and some methods for determining the follow-up sample size requirements are discussed. R functions are provided that can be used to analyze results from a replication study. R functions to determine the appropriate sample size in the follow-up study also are provided.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Organizational Research Methods},
	author = {Bonett, Douglas G.},
	month = jul,
	year = {2021},
	pages = {513--529},
}

@article{steiner_correspondence_2023,
	title = {Correspondence measures for assessing replication success.},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	url = {https://doi.apa.org/doi/10.1037/met0000597},
	doi = {10.1037/met0000597},
	language = {en},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Steiner, Peter M. and Sheehan, Patrick and Wong, Vivian C.},
	month = jul,
	year = {2023},
}

@article{mathur_challenges_2019,
	title = {Challenges and suggestions for defining replication ‚Äúsuccess‚Äù when effects may be heterogeneous: {Comment} on {Hedges} and {Schauer} (2019).},
	volume = {24},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Challenges and suggestions for defining replication ‚Äúsuccess‚Äù when effects may be heterogeneous},
	url = {https://doi.apa.org/doi/10.1037/met0000223},
	doi = {10.1037/met0000223},
	language = {en},
	number = {5},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Mathur, Maya B. and VanderWeele, Tyler J.},
	month = oct,
	year = {2019},
	pages = {571--575},
}

@article{suetake_workflow_2022,
	title = {A workflow reproducibility scale for automatic validation of biological interpretation results},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2047-217X},
	url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad031/7150394},
	doi = {10.1093/gigascience/giad031},
	abstract = {Abstract
            
              Background
              Reproducibility of data analysis workflow is a key issue in the field of bioinformatics. Recent computing technologies, such as virtualization, have made it possible to reproduce workflow execution with ease. However, the reproducibility of results is not well discussed; that is, there is no standard way to verify whether the biological interpretation of reproduced results is the same. Therefore, it still remains a challenge to automatically evaluate the reproducibility of results.
            
            
              Results
              We propose a new metric, a reproducibility scale of workflow execution results, to evaluate the reproducibility of results. This metric is based on the idea of evaluating the reproducibility of results using biological feature values (e.g., number of reads, mapping rate, and variant frequency) representing their biological interpretation. We also implemented a prototype system that automatically evaluates the reproducibility of results using the proposed metric. To demonstrate our approach, we conducted an experiment using workflows used by researchers in real research projects and the use cases that are frequently encountered in the field of bioinformatics.
            
            
              Conclusions
              Our approach enables automatic evaluation of the reproducibility of results using a fine-grained scale. By introducing our approach, it is possible to evolve from a binary view of whether the results are superficially identical or not to a more graduated view. We believe that our approach will contribute to more informed discussion on reproducibility in bioinformatics.},
	language = {en},
	urldate = {2024-08-08},
	journal = {GigaScience},
	author = {Suetake, Hirotaka and Fukusato, Tsukasa and Igarashi, Takeo and Ohta, Tazro},
	month = dec,
	year = {2022},
	pages = {giad031},
}

@article{verhagen_bayesian_2014,
	title = {Bayesian tests to quantify the result of a replication attempt.},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	url = {https://doi.apa.org/doi/10.1037/a0036731},
	doi = {10.1037/a0036731},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Journal of Experimental Psychology: General},
	author = {Verhagen, Josine and Wagenmakers, Eric-Jan},
	month = aug,
	year = {2014},
	pages = {1457--1475},
}

@article{klugkist_bayesian_2023,
	title = {Bayesian evidence synthesis for informative hypotheses: {An} introduction.},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Bayesian evidence synthesis for informative hypotheses},
	url = {https://doi.apa.org/doi/10.1037/met0000602},
	doi = {10.1037/met0000602},
	language = {en},
	urldate = {2024-08-08},
	journal = {Psychological Methods},
	author = {Klugkist, Irene and Volker, Thom Benjamin},
	month = sep,
	year = {2023},
}

@article{schauer_assessing_2020,
	title = {Assessing heterogeneity and power in replications of psychological experiments.},
	volume = {146},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1455, 0033-2909},
	url = {https://doi.apa.org/doi/10.1037/bul0000232},
	doi = {10.1037/bul0000232},
	language = {en},
	number = {8},
	urldate = {2024-08-08},
	journal = {Psychological Bulletin},
	author = {Schauer, Jacob M. and Hedges, Larry V.},
	month = aug,
	year = {2020},
	pages = {701--719},
}

@article{lin_assessing_2022,
	title = {Assessing and visualizing fragility of clinical results with binary outcomes in {R} using the fragility package},
	volume = {17},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0268754},
	doi = {10.1371/journal.pone.0268754},
	abstract = {With the growing concerns about research reproducibility and replicability, the assessment of scientific results‚Äô fragility (or robustness) has been of increasing interest. The fragility index was proposed to quantify the robustness of statistical significance of clinical studies with binary outcomes. It is defined as the minimal event status modifications that can alter statistical significance. It helps clinicians evaluate the reliability of the conclusions. Many factors may affect the fragility index, including the treatment groups in which event status is modified, the statistical methods used for testing for the association between treatments and outcomes, and the pre-specified significance level. In addition to assessing the fragility of individual studies, the fragility index was recently extended to both conventional pairwise meta-analyses and network meta-analyses of multiple treatment comparisons. It is not straightforward for clinicians to calculate these measures and visualize the results. We have developed an R package called ‚Äúfragility‚Äù to offer user-friendly functions for such purposes. This article provides an overview of methods for assessing and visualizing the fragility of individual studies as well as pairwise and network meta-analyses, introduces the usage of the ‚Äúfragility‚Äù package, and illustrates the implementations with several worked examples.},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {PLOS ONE},
	author = {Lin, Lifeng and Chu, Haitao},
	editor = {Gagniuc, Paul Aurelian},
	month = jun,
	year = {2022},
	pages = {e0268754},
}

@article{held_new_2020,
	title = {A {New} {Standard} for the {Analysis} and {Design} of {Replication} {Studies}},
	volume = {183},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {0964-1998, 1467-985X},
	url = {https://academic.oup.com/jrsssa/article/183/2/431/7056392},
	doi = {10.1111/rssa.12493},
	abstract = {Summary
            A new standard is proposed for the evidential assessment of replication studies. The approach combines a specific reverse Bayes technique with prior-predictive tail probabilities to define replication success. The method gives rise to a quantitative measure for replication success, called the sceptical p-value. The sceptical p-value integrates traditional significance of both the original and the replication study with a comparison of the respective effect sizes. It incorporates the uncertainty of both the original and the replication effect estimates and reduces to the ordinary p-value of the replication study if the uncertainty of the original effect estimate is ignored. The framework proposed can also be used to determine the power or the required replication sample size to achieve replication success. Numerical calculations highlight the difficulty of achieving replication success if the evidence from the original study is only suggestive. An application to data from the Open Science Collaboration project on the replicability of psychological science illustrates the methodology proposed.},
	language = {en},
	number = {2},
	urldate = {2024-08-08},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Held, Leonhard},
	month = feb,
	year = {2020},
	pages = {431--448},
}

@article{belz_metrological_2022,
	title = {A {Metrological} {Perspective} on {Reproducibility} in {NLP}*},
	volume = {48},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/48/4/1125/112113/A-Metrological-Perspective-on-Reproducibility-in},
	doi = {10.1162/coli_a_00448},
	abstract = {Abstract
            Reproducibility has become an increasingly debated topic in NLP and ML over recent years, but so far, no commonly accepted definitions of even basic terms or concepts have emerged. The range of different definitions proposed within NLP/ML not only do not agree with each other, they are also not aligned with standard scientific definitions. This article examines the standard definitions of repeatability and reproducibility provided by the meta-science of metrology, and explores what they imply in terms of how to assess reproducibility, and what adopting them would mean for reproducibility assessment in NLP/ML. It turns out the standard definitions lead directly to a method for assessing reproducibility in quantified terms that renders results from reproduction studies comparable across multiple reproductions of the same original study, as well as reproductions of different original studies. The article considers where this method sits in relation to other aspects of NLP work one might wish to assess in the context of reproducibility.},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Computational Linguistics},
	author = {Belz, Anya},
	month = dec,
	year = {2022},
	pages = {1125--1135},
}

@article{steiner_causal_2019,
	title = {A {Causal} {Replication} {Framework} for {Designing} and {Assessing} {Replication} {Efforts}},
	volume = {227},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2190-8370, 2151-2604},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000385},
	doi = {10.1027/2151-2604/a000385},
	abstract = {Abstract. Replication has long been a cornerstone for establishing trustworthy scientific results, but there remains considerable disagreement about what constitutes a replication, how results from these studies should be interpreted, and whether direct replication of results is even possible. This article addresses these concerns by presenting the methodological foundations for a replication science. It provides an introduction to the causal replication framework, which defines ‚Äúreplication‚Äù as a research design that tests whether two (or more) studies produce the same causal effect within the limits of sampling error. The framework formalizes the conditions under which replication success can be expected, and allows for the causal interpretation of replication failures. Through two applied examples, the article demonstrates how the causal replication framework may be utilized to plan prospective replication designs, as well as to interpret results from existing replication efforts.},
	language = {en},
	number = {4},
	urldate = {2024-08-08},
	journal = {Zeitschrift f√ºr Psychologie},
	author = {Steiner, Peter M. and Wong, Vivian C. and Anglin, Kylie},
	month = oct,
	year = {2019},
	pages = {280--292},
}

@article{verhagen_bayesian_2014-1,
	title = {"{Bayesian} tests to quantify the result of a replication attempt": {Correction} to {Verhagen} and {Wagenmakers} (2014).},
	volume = {143},
	issn = {1939-2222, 0096-3445},
	shorttitle = {"{Bayesian} tests to quantify the result of a replication attempt"},
	url = {https://doi.apa.org/doi/10.1037/a0038326},
	doi = {10.1037/a0038326},
	language = {en},
	number = {6},
	urldate = {2024-08-08},
	journal = {Journal of Experimental Psychology: General},
	author = {Verhagen, Josine and Wagenmakers, Eric-Jan},
	month = dec,
	year = {2014},
	pages = {2073--2073},
}

@article{manolov_proposal_2022,
	title = {A proposal for the assessment of replication of effects in single‚Äêcase experimental designs},
	volume = {55},
	issn = {0021-8855, 1938-3703},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jaba.923},
	doi = {10.1002/jaba.923},
	abstract = {In science in general and in the context of single‚Äêcase experimental designs, replication of the effects of the intervention within and/or across participants or experiments is crucial for establishing causality and for assessing the generality of the intervention effect. Specific developments and proposals for assessing whether an effect has been replicated or not (or to what extent) are scarce, in the general context of behavioral sciences, and practically null in the single‚Äêcase experimental designs context. We propose an extension of the modified Brinley plot for assessing how many of the effects replicate. To make this assessment possible, a definition of replication is suggested, on the basis of expert judgment, rather than on statistical criteria. The definition of replication and its graphical representation are justified, presenting their strengths and limitations, and illustrated with real data. A user‚Äêfriendly software is made available for obtaining automatically the graphical representation.},
	language = {en},
	number = {3},
	urldate = {2024-08-08},
	journal = {Journal of Applied Behavior Analysis},
	author = {Manolov, Rumen and Tanious, Ren√© and Fern√°ndez‚ÄêCastilla, Bel√©n},
	month = jun,
	year = {2022},
	pages = {997--1024},
}

@inproceedings{belz_quantified_2022,
	address = {Dublin, Ireland},
	title = {Quantified {Reproducibility} {Assessment} of {NLP} {Results}},
	url = {https://aclanthology.org/2022.acl-long.2},
	doi = {10.18653/v1/2022.acl-long.2},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Belz, Anya and Popovic, Maja and Mille, Simon},
	year = {2022},
	pages = {16--28},
}

@article{kacmar_replikacne_2020,
	title = {Replikaƒçn√© ≈°t√∫die v psychol√≥gii: {Pojednanie} o dvoch d√¥le≈æit√Ωch ot√°zk√°ch. [{Replication} studies in psychology: {A} discourse on two important issues.]},
	volume = {64},
	issn = {1804-6436},
	shorttitle = {Replikaƒçn√© ≈°t√∫die v psychol√≥gii},
	abstract = {During the progressing renaissance of psychological research, various suggestions have been proposed concerning how to improve the current situation. Among them, a prominent place belongs to replication studies. However the authors aiming to critically evaluate conducted replications or to conduct a replication study themselves needs to explicitly reflect two crucial issues. The first regards conditions that need to be met in order to consider the second study as a replication of the first. This concerns the definition of replication and broader categorization of replication types from a methodological point of view, reflecting various conceptual and terminological issues. The second regards conditions that need to be met in order to say that the results of the replication study replicated/did not replicate the original study. This involves the way in which the study is statistically analysed with respect to its main goal. Therefore, the aim of the present article is to discuss and to summarize the present state of the literature regarding these two issues and to synthesize it into two functional classifications: A, various types of replications from a methodological point of view; B, various possibilities of data analysis from a statistical point of view. It is argued that instead of non-critically preferring one specific approach, the researcher should reflect the pros and cons of various approaches and the broader context of the goal of the replication study. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {1},
	journal = {ƒåeskoslovensk√° Psychologie: ƒåasopis Pro Psychologickou Teorii a Praxi},
	author = {Kaƒçm√°r, Pavol and Adamkoviƒç, Mat√∫≈°},
	year = {2020},
	note = {Place: Czech Republic
Publisher: Akademie vƒõd ƒåesk√© Republiky/The Academy of Sciences of the Czech Republic},
	keywords = {Crisis Intervention, Experimental Replication, Methodology, Psychology},
	pages = {66--83},
}

@book{committee_on_reproducibility_and_replicability_in_science_reproducibility_2019,
	address = {Washington, D.C.},
	title = {Reproducibility and {Replicability} in {Science}},
	isbn = {978-0-309-48616-3},
	url = {https://www.nap.edu/catalog/25303},
	urldate = {2024-08-13},
	publisher = {National Academies Press},
	author = {{National Academies of Sciences, Engineering, and Medicine}},
	collaborator = {{Committee on Reproducibility and Replicability in Science} and {Board on Behavioral, Cognitive, and Sensory Sciences} and {Committee on National Statistics} and {Division of Behavioral and Social Sciences and Education} and {Nuclear and Radiation Studies Board} and {Division on Earth and Life Studies} and {Board on Mathematical Sciences and Analytics} and {Committee on Applied and Theoretical Statistics} and {Division on Engineering and Physical Sciences} and {Board on Research Data and Information} and {Committee on Science, Engineering, Medicine, and Public Policy} and {Policy and Global Affairs}},
	month = sep,
	year = {2019},
	doi = {10.17226/25303},
	keywords = {Policy for Science and Technology, Policy for Science and Technology--Research and Data, Surveys and Statistics},
}

@article{aria_openalexr_2024,
	title = {{openalexR}: {An} {R}-{Tool} for {Collecting} {Bibliometric} {Data} from {OpenAlex}},
	volume = {15},
	issn = {2073-4859},
	shorttitle = {{openalexR}},
	url = {https://journal.r-project.org/articles/RJ-2023-089},
	doi = {10.32614/RJ-2023-089},
	language = {en},
	number = {4},
	urldate = {2024-08-07},
	journal = {The R Journal},
	author = {Aria, Massimo and Le, Trang and Cuccurullo, Corrado and Belfiore, Alessandra and Choe, June},
	month = apr,
	year = {2024},
	pages = {167--180},
}

@article{mbuagbaw_tutorial_2020,
	title = {A tutorial on methodological studies: the what, when, how and why},
	volume = {20},
	issn = {1471-2288},
	shorttitle = {A tutorial on methodological studies},
	url = {https://doi.org/10.1186/s12874-020-01107-7},
	doi = {10.1186/s12874-020-01107-7},
	abstract = {Methodological studies ‚Äì studies that evaluate the design, analysis or reporting of other research-related reports ‚Äì play an important role in health research. They help to highlight issues in the conduct of research with the aim of improving health research methodology, and ultimately reducing research waste.},
	number = {1},
	urldate = {2024-08-05},
	journal = {BMC Medical Research Methodology},
	author = {Mbuagbaw, Lawrence and Lawson, Daeria O. and Puljak, Livia and Allison, David B. and Thabane, Lehana},
	month = sep,
	year = {2020},
	keywords = {Meta-epidemiology, Methodological study, Research methods, Research-on-research},
	pages = {226},
}

@article{voelkl_irise_2024,
	title = {The {iRISE} {Reproducibility} {Glossary}},
	url = {https://osf.io/ewybt},
	doi = {https://doi.org/10.17605/OSF.IO/BR9SP},
	abstract = {Presented by OSF},
	language = {eng},
	urldate = {2024-08-05},
	author = {Voelkl, Bernhard and Heyard, Rachel and Fanelli, Daniele and Wever, Kimberley and Held, Leonhard and Maniadis, Zacharias and McCann, Sarah and Zellers, Stephanie and W√ºrbel, Hanno},
	month = jun,
	year = {2024},
	note = {Publisher: Open Science Framework},
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	url = {https://www.science.org/doi/10.1126/scitranslmed.aaf5027},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of ‚Äúresearch reproducibility‚Äù are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ‚Äútruth.‚Äù},
	number = {341},
	urldate = {2024-07-30},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {341ps12--341ps12},
}

@article{steiner_correspondence_2023-1,
	title = {Correspondence measures for assessing replication success},
	issn = {1939-1463},
	doi = {10.1037/met0000597},
	abstract = {Given recent evidence challenging the replicability of results in the social and behavioral sciences, critical questions have been raised about appropriate measures for determining replication success in comparing effect estimates across studies. At issue is the fact that conclusions about replication success often depend on the measure used for evaluating correspondence in results. Despite the importance of choosing an appropriate measure, there is still no widespread agreement about which measures should be used. This article addresses these questions by describing formally the most commonly used measures for assessing replication success, and by comparing their performance in different contexts according to their replication probabilities‚Äîthat is, the probability of obtaining replication success given study-specific settings. The measures may be characterized broadly as conclusion-based approaches, which assess the congruence of two independent studies‚Äô conclusions about the presence of an effect, and distance-based approaches, which test for a significant difference or equivalence of two effect estimates. We also introduce a new measure for assessing replication success called the correspondence test, which combines a difference and equivalence test in the same framework. To help researchers plan prospective replication efforts, we provide closed formulas for power calculations that can be used to determine the minimum detectable effect size (and thus, sample sizes) for each study so that a predetermined minimum replication probability can be achieved. Finally, we use a replication data set from the Open Science Collaboration (2015) to demonstrate the extent to which conclusions about replication success depend on the correspondence measure selected. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	journal = {Psychological Methods},
	author = {Steiner, Peter M. and Sheehan, Patrick and Wong, Vivian C.},
	year = {2023},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Causal Analysis, Data Sets, Experimental Replication, Probability},
	pages = {No Pagination Specified--No Pagination Specified},
}

@misc{barba_terminologies_2018,
	title = {Terminologies for {Reproducible} {Research}},
	url = {http://arxiv.org/abs/1802.03311},
	doi = {10.48550/arXiv.1802.03311},
	abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Barba, Lorena A.},
	month = feb,
	year = {2018},
	note = {arXiv:1802.03311 [cs]},
	keywords = {Computer Science - Digital Libraries},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	urldate = {2024-07-30},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {aac4716},
}

@article{bahor_development_2021,
	title = {Development and uptake of an online systematic review platform: the early years of the {CAMARADES} {Systematic} {Review} {Facility} ({SyRF})},
	volume = {5},
	issn = {2398-8703},
	shorttitle = {Development and uptake of an online systematic review platform},
	doi = {10.1136/bmjos-2020-100103},
	abstract = {Preclinical research is a vital step in the drug discovery pipeline and more generally in helping to better understand human disease aetiology and its management. Systematic reviews (SRs) can be powerful in summarising and appraising this evidence concerning a specific research question, to highlight areas of improvements, areas for further research and areas where evidence may be sufficient to take forward to other research domains, for instance clinical trial. Guidance and tools for preclinical research synthesis remain limited despite their clear utility. We aimed to create an online end-to-end platform primarily for conducting SRs of preclinical studies, that was flexible enough to support a wide variety of experimental designs, was adaptable to different research questions, would allow users to adopt emerging automated tools and support them during their review process using best practice. In this article, we introduce the Systematic Review Facility (https://syrf.org.uk), which was launched in 2016 and designed to support primarily preclinical SRs from small independent projects to large, crowdsourced projects. We discuss the architecture of the app and its features, including the opportunity to collaborate easily, to efficiently manage projects, to screen and annotate studies for important features (metadata), to extract outcome data into a secure database, and tailor these steps to each project. We introduce how we are working to leverage the use of automation tools and allow the integration of these services to accelerate and automate steps in the systematic review workflow.},
	language = {eng},
	number = {1},
	journal = {BMJ open science},
	author = {Bahor, Zsanett and Liao, Jing and Currie, Gillian and Ayder, Can and Macleod, Malcolm and McCann, Sarah K. and Bannach-Brown, Alexandra and Wever, Kimberley and Soliman, Nadia and Wang, Qianying and Doran-Constant, Lee and Young, Laurie and Sena, Emily S. and Sena, Chris},
	year = {2021},
	pmid = {35047698},
	pmcid = {PMC8647599},
	keywords = {automation, preclinical research, systematic review},
	pages = {e100103},
}

@article{bonett_design_2021-1,
	title = {Design and {Analysis} of {Replication} {Studies}},
	volume = {24},
	issn = {1094-4281},
	url = {https://doi.org/10.1177/1094428120911088},
	doi = {10.1177/1094428120911088},
	abstract = {Issues surrounding the importance and interpretation of replication research have generated considerable debate and controversy in recent years. Some of the controversy can be attributed to imprecise and inadequate specifications of the statistical criteria needed to assess replication and nonreplication. Two types of statistical replication evidence and four types of statistical nonreplication evidence are described. In addition, three types of inconclusive statistical replication evidence are described. An important benefit of a replication study is the ability to combine an effect-size estimate from the original study with an effect-size estimate from the follow-up study to obtain a more precise and generalizable effect-size estimate. The sample size in the follow-up study is an important design consideration, and some methods for determining the follow-up sample size requirements are discussed. R functions are provided that can be used to analyze results from a replication study. R functions to determine the appropriate sample size in the follow-up study also are provided.},
	language = {en},
	number = {3},
	urldate = {2024-07-30},
	journal = {Organizational Research Methods},
	author = {Bonett, Douglas G.},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {513--529},
}

@article{mcshane_large-scale_2019,
	title = {Large-{Scale} {Replication} {Projects} in {Contemporary} {Psychological} {Research}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1505655},
	doi = {10.1080/00031305.2018.1505655},
	abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next‚Äîan issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous‚Äîeven if the sign of the effect is consistent‚Äîthe phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
	number = {sup1},
	urldate = {2024-04-24},
	journal = {The American Statistician},
	author = {McShane, Blakeley B. and Tackett, Jennifer L. and B√∂ckenholt, Ulf and Gelman, Andrew},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1505655},
	keywords = {Between-study variation, Heterogeneity, Hierarchical, Meta-Analysis, Multilevel, Null hypothesis significance testing, Psychology, Replication, p-value},
	pages = {99--105},
}

@article{cobey_epidemiological_2023-1,
	title = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines: {A} scoping review of articles published in 2018-2019},
	volume = {12},
	issn = {2050-084X},
	shorttitle = {Epidemiological characteristics and prevalence rates of research reproducibility across disciplines},
	url = {https://doi.org/10.7554/eLife.78518},
	doi = {10.7554/eLife.78518},
	abstract = {Background:. Reproducibility is a central tenant of research. We aimed to synthesize the literature on reproducibility and describe its epidemiological characteristics, including how reproducibility is defined and assessed. We also aimed to determine and compare estimates for reproducibility across different fields. Methods:. We conducted a scoping review to identify English language replication studies published between 2018 and 2019 in economics, education, psychology, health sciences, and biomedicine. We searched Medline, Embase, PsycINFO, Cumulative Index of Nursing and Allied Health Literature ‚Äì CINAHL, Education Source via EBSCOHost, ERIC, EconPapers, International Bibliography of the Social Sciences (IBSS), and EconLit. Documents retrieved were screened in duplicate against our inclusion criteria. We extracted year of publication, number of authors, country of affiliation of the corresponding author, and whether the study was funded. For the individual replication studies, we recorded whether a registered protocol for the replication study was used, whether there was contact between the reproducing team and the original authors, what study design was used, and what the primary outcome was. Finally, we recorded how reproducibilty was defined by the authors, and whether the assessed study(ies) successfully reproduced based on this definition. Extraction was done by a single reviewer and quality controlled by a second reviewer. Results:. Our search identified 11,224 unique documents, of which 47 were included in this review. Most studies were related to either psychology (48.6\%) or health sciences (23.7\%). Among these 47 documents, 36 described a single reproducibility study while the remaining 11 reported at least two reproducibility studies in the same paper. Less than the half of the studies referred to a registered protocol. There was variability in the definitions of reproduciblity success. In total, across the 47 documents 177 studies were reported. Based on the definition used by the author of each study, 95 of 177 (53.7\%) studies reproduced. Conclusions:. This study gives an overview of research across five disciplines that explicitly set out to reproduce previous research. Such reproducibility studies are extremely scarce, the definition of a successfully reproduced study is ambiguous, and the reproducibility rate is overall modest. Funding:. No external funding was received for this work},
	urldate = {2024-04-24},
	journal = {eLife},
	author = {Cobey, Kelly D and Fehlmann, Christophe A and Christ Franco, Marina and Ayala, Ana Patricia and Sikora, Lindsey and Rice, Danielle B and Xu, Chenchen and Ioannidis, John PA and Lalu, Manoj M and M√©nard, Alixe and Neitzel, Andrew and Nguyen, Bea and Tsertsvadze, Nino and Moher, David},
	editor = {Allison, David B and Zaidi, Mone and Vorland, Colby J and Lupia, Arthur and Agley, Jon},
	month = jun,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {meta-research, open science, replication, reproducibility},
	pages = {e78518},
}

@article{pollock_recommendations_2023,
	title = {Recommendations for the extraction, analysis, and presentation of results in scoping reviews},
	volume = {21},
	issn = {2689-8381},
	url = {https://journals.lww.com/jbisrir/fulltext/2023/03000/recommendations_for_the_extraction,_analysis,_and.7.aspx},
	doi = {10.11124/JBIES-22-00123},
	abstract = {Scoping reviewers often face challenges in the extraction, analysis, and presentation of scoping review results. Using best-practice examples and drawing on the expertise of the JBI Scoping Review Methodology Group and an editor of a journal that publishes scoping reviews, this paper expands on existing JBI scoping review guidance. The aim of this article is to clarify the process of extracting data from different sources of evidence; discuss what data should be extracted (and what should not); outline how to analyze extracted data, including an explanation of basic qualitative content analysis; and offer suggestions for the presentation of results in scoping reviews.},
	language = {en-US},
	number = {3},
	urldate = {2024-04-23},
	journal = {JBI Evidence Synthesis},
	author = {Pollock, Danielle and Peters, Micah D. J. and Khalil, Hanan and McInerney, Patricia and Alexander, Lyndsay and Tricco, Andrea C. and Evans, Catrin and de Moraes, √ârica Brand√£o and Godfrey, Christina M. and Pieper, Dawid and Saran, Ashrita and Stern, Cindy and Munn, Zachary},
	month = mar,
	year = {2023},
	pages = {520},
}

@article{tricco_prisma_2018,
	title = {{PRISMA} {Extension} for {Scoping} {Reviews} ({PRISMA}-{ScR}): {Checklist} and {Explanation}},
	volume = {169},
	issn = {1539-3704},
	shorttitle = {{PRISMA} {Extension} for {Scoping} {Reviews} ({PRISMA}-{ScR})},
	doi = {10.7326/M18-0850},
	abstract = {Scoping reviews, a type of knowledge synthesis, follow a systematic approach to map evidence on a topic and identify main concepts, theories, sources, and knowledge gaps. Although more scoping reviews are being done, their methodological and reporting quality need improvement. This document presents the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) checklist and explanation. The checklist was developed by a 24-member expert panel and 2 research leads following published guidance from the EQUATOR (Enhancing the QUAlity and Transparency Of health Research) Network. The final checklist contains 20 essential reporting items and 2 optional items. The authors provide a rationale and an example of good reporting for each item. The intent of the PRISMA-ScR is to help readers (including researchers, publishers, commissioners, policymakers, health care providers, guideline developers, and patients or consumers) develop a greater understanding of relevant terminology, core concepts, and key items to report for scoping reviews.},
	language = {eng},
	number = {7},
	journal = {Annals of Internal Medicine},
	author = {Tricco, Andrea C. and Lillie, Erin and Zarin, Wasifa and O'Brien, Kelly K. and Colquhoun, Heather and Levac, Danielle and Moher, David and Peters, Micah D. J. and Horsley, Tanya and Weeks, Laura and Hempel, Susanne and Akl, Elie A. and Chang, Christine and McGowan, Jessie and Stewart, Lesley and Hartling, Lisa and Aldcroft, Adrian and Wilson, Michael G. and Garritty, Chantelle and Lewin, Simon and Godfrey, Christina M. and Macdonald, Marilyn T. and Langlois, Etienne V. and Soares-Weiser, Karla and Moriarty, Jo and Clifford, Tammy and Tun√ßalp, √ñzge and Straus, Sharon E.},
	month = oct,
	year = {2018},
	pmid = {30178033},
	keywords = {Checklist, Delphi Technique, Humans, Meta-Analysis as Topic, Review Literature as Topic, Systematic Reviews as Topic},
	pages = {467--473},
}
