Title,Year,Doi,InvestigatorName,Field of research,Discipline,Type of project,Research question or aim,Did the authors define the type of reproducibility that is investigated?,Aspect of reproducibility investigated,"Even if defined by the authors, infer the aspect of reproducibility investigated",Did the authors measure reproducibility,Did they measure reproducibility using measures not in the previous list?,Paste description of each of the measures used,Number of measures,Did the paper refer to other papers for more information on the metric(s) used?,Paste the doi of all paper(s),Did the authors discuss limitations or assumptions of the metric(s) used?,Paste text on limitation/assumptions,"Did the authors discuss equity, diversity, and/or inclusion at any point?",Paste text on EDI dimension discussion
Law and Psychology Grows Up Goes Online and Replicates,2018,10.1111/jels.12180,Joris Frese; Samuel Pawel,Social Sciences and Humanities,Psychology; Law,"Many Phenomena, One Study","""Using three canonical law and psychology findings, we document the challenges and the feasibility of reproducing results across platforms."" (Abstract); ""we would like to show, through replication of canonical legal experiments on a variety of survey platforms, the potential and the pitfalls of replication in law and psychology.""",FALSE,NA,Different data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size [Comment: relatively informal comparison of effect estimates]; [Comment: Compared attentiveness of respondents across replication samples] (3) Subjective assessment,FALSE,NA,3,TRUE,"10.1177/0956797614567341 [Comment: this was for the sample size determination, rather than the analysis]",FALSE,NA,TRUE,"""The gender split identiﬁed by PCI has balanced out in recent years.30 Respecting other demographic characteristics, results vary. One study, for example, concludes that workers are “wealthier, younger, more educated, less racially diverse, and more Demo- cratic than national samples” and less religiously afﬁliated.31 Others are more equivocal, concluding that apart from workers’ relative youth and liberalism, they closely approxi- mate a nationally representative sample."" (p.328) [Comment: Discussed gender- and racial composition of different replication samples.]; ""MTurk subjects are more diverse than college student samples, across various demographic variables."""
High replicability of newly discovered social-behavioural findings is achievable,2023,10.1038/s41562-023-01749-9,Joris Frese; Rachel Heyard,Social Sciences and Humanities,"Interdisciplinary [Comment: psychology, marketing, advertising, political science, communication, and judgement and decision-making]","Many Phenomena, Many studies","""In this Article, we report the results of a prospective replication study [...] examining whether low replicability and declining effects are inevitable when using proposed rigour-enhancing practices."" (pp. 1-2); ""an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency.""",FALSE,NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: One way of assessing replicability is to examine whether each replication rejects the null hypothesis at P < 0.05 in the expected direction] (2) Agreement in effect size [Comment: An alternative index of replicability examines the consistency of ESs generated by the initial self-confirmatory test and its subsequent replications (within-study heterogeneity; τ̂ within). On the basis of a multilevel meta-analysis, little variation in ESs was observed beyond what would be expected by sampling variation alone.]",FALSE,NA,2,FALSE,NA,TRUE,"""Due to the limited number of participating labs, lab-level variation in the replicability of findings was incalculable; to the extent that labs vary in how they select potential replication targets, the replication rates observed in the present study may not generalize to a broader population of research groups"" (p.6)",TRUE,"""The sample providers were instructed to provide American adults drawn in a stratified way with unequal probabilities of selection from the panels so that the people who completed each survey would resemble the nation’s adult population (according to the most recently available Current Population Survey, conducted by the US Census Bureau) in terms of gender, age, education, ethnicity (Hispanic versus not), race (allowing each respondent to select more than one race), region and income."" (p.7) [Comment: Discussed racial, gender, and income-based distribution of replication samples.]"
Genotypic variability enhances the reproducibility of an ecological study,2018,10.1038/s41559-017-0434-x,Joris Frese; Rachel Heyard,Life Sciences,Biology; Ecology,"One Phenomenon, Many Studies","""A novel but controversial hypothesis postulates that stringent levels of environmental and biotic standardization in experimental studies reduce reproducibility by amplifying the impacts of laboratory-specific environmental factors not accounted for in study designs. A corollary to this hypothesis is that a deliberate introduction of controlled systematic variability (CSV) in experimental designs may lead to increased reproducibility"" (Abstract); ""To test the hypothesis that introducing CSV enhances reproducibility in an ecological context""",TRUE,"""Reproducibility—the ability to duplicate a study and its findings.""",Different data - same analysis,"(1) Agreement in statistical significance [Comment: ""Statistically significant differences among the 14 laboratories 
were considered an indication of irreproducibility."" (p.2)] (2) Meta-analysis of original and replication study/studies",TRUE,"To answer the question of how many laboratories produced results that were statistically indistinguishable from one another (that is, reproduced the same finding), we used Tukey’s post-hoc honest significant difference test for the laboratory effect.",2.5,FALSE,NA,TRUE,"""Arguably, our use of statistical significance tests of effect sizes to determine reproducibility might be viewed as overly restrictive and better suited to assessing the reproducibility of parameter esti- mates rather than the generality of the hypothesis under test27. We used this approach because no generally accepted alternative framework is available to assess how close the multivariate results from multiple laboratories need to be to conclude that they reproduced the same finding."" (p.7)",TRUE,In a sense: they introduce controlled systematic variability and investigated its impact on reproducibility [Comment: referring to 10.1080/00031305.2016.1154108]
Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy: survey of studies published in The BMJ and PLOS Medicine,2018,10.1136/bmj.k400,Joris Frese; Rachel Heyard,Life Sciences,Clinical trials; Medicine,"Many Phenomena, One Study","""To explore the effectiveness of data sharing by randomized controlled trials (RCTs) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes."" (Abstract)",FALSE,NA,Same data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size (3) Subjective assessment [Comment: clinical judgment],FALSE,NA,3,FALSE,NA,FALSE,NA,FALSE,NA
Eleven years of student replication projects provide evidence on the correlates of replicability in psychology,2023,10.1098/rsos.231240,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology;Experimental psychology,"Many Phenomena, One Study","""our contribution is a new dataset of 176 replications of experimental studies from the social sciences, primarily psychology [...] We use this dataset to investigate the rate of replicability for such projects as well as the correlates of replication success."" (p.2)",TRUE,"Replicability - section on ""What are we estimating when we measure replicability?""",Different data - same analysis,"(1) Agreement in statistical significance [p-original, the p-value on the null hypothesis that the original and replication statistics are from the same distribution] (2) Agreement in effect size [Comment: prediction interval, a binary measure of whether the replication statistic fell within the prediction interval of the original statistic] (3) Subjective assessment [Comment: a subjective replication score (coded by two independent raters—one typically at the time of project completion—with discrepancies resolved by discussion)] ",FALSE,NA,3,TRUE,10.1111/rssa.12572 [Comment: prediction interval and p-original],TRUE,"""Unlike statistical measures, subjective replication success accommodates studies with multiple important outcome measures that together define the pattern of interest. Further, this measure was applicable across the diverse range of statistical measures and reporting practices present in the sample."" [Comment: statistical measures]",FALSE,NA
A many-analysts approach to the relation between religiosity and well-being,2023,10.1080/2153599X.2022.2070255,Helena Hartmann; Joris Frese,Social Sciences and Humanities,Psychology; Religion; Wellbeing,"One Phenomenon, Many Studies","""assess the robustness of the relation between relgiosity and wellbeing using a multi-analyst approach""",TRUE,"""we believe that the more consistent the results from diﬀerent analysis teams are, the more conﬁdent we can be in the conclusions we draw from the results.""",Same data - different analysis,"(1) Agreement in statistical significance (2) Agreement in effect size [""Note that our request for beta coeﬃcients as eﬀect size metrics may have aﬀected the teams’ choice of statistical model and encouraged them to use regression models that generate beta coeﬃcients."" (p.247/248)]",FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"""Although most teams indicated that eﬀort was (very) high, the majority also reported that frustration was (very) low and that they spent as much time as anticipated (see Figure 10). That is, in stage 1, 55% of the teams reported (very) high eﬀort, 17% were neutral, and 28% reported (very) low eﬀort. For stage 2, 48% of the teams reported (very) high eﬀort, 18% were neutral, and 34% reported (very) low eﬀort. In stage 1, 17% of the teams reported (very) high frustration, 23% were neutral, and 60% reported (very) low frustration. In stage 2, 18% of the teams reported (very) high frustration, 17% were neutral, and 65% reported (very) low frustration. The median time spent on the analyzes was 8 hours for both stages, although the range was quite wide: 1 to 80 hours for stage 1 and 30 minutes to 140 hours for stage 2. Most teams anticipated as much time as they spent: 51% for stage 1 and 52% for stage 2. In stage 1, 36% spent (much) more time than anticipated and 13% spent (much) less time. In stage 2, 33% spent (much) more time than anticipated and 15% spent (much) less time."" (p.255) [Comment: Maybe not EDI, but they had a section on the difficulties, frustrations, and time usage of the research teams regarding their replication tasks.]"
Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability,2020,10.1177/2515245920958687,Louise Townsin; Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies","""This design is particularly well suited for testing the strong hypothesis that many, if not most, failures to replicate are due to design errors that could have been caught by a domain expert (Gilbert et al., 2016). If this hypothesis is correct, then the new, peer-reviewed protocols would be expected to improve replicability and increase effect sizes to be closer to those of the original studies. This would not necessarily mean that all failures to replicate are due to poor design. After all, our sample of studies was chosen because they are among the most likely published replications to have faulty designs. However, such an outcome would suggest that published replicability rates are overly pessimistic.""; ""The purpose of this investigation was to test whether protocols resulting from formal peer review would produce stronger evidence for replicability than protocols that had not received formal peer review""",TRUE,"""A replication study is an attempt to reproduce a previously observed finding with no a priori expectation for a different outcome (see Nosek & Errington, 2017, 2020; Zwaan, Etz, Lucas, & Donnellan, 2018).""","Different data - different analysis [Comment: We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3–9; median total sample = 1,279.5, range = 276–3,512) for high-powered tests of each original finding with both protocols.]; Different data - same analysis [Comment: Same analysis, but for small deviations]","(1) Agreement in effect size [Comment: Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (Δr = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00–.15) were 78% smaller, on average, than the original effect sizes (median r = .37, range = .19–.50).] (2) Meta-analysis of original and replication study/studies [Comment: ""We conducted a multilevel random- effects meta-analysis of the 101 effect sizes, with a random intercept of data-collection site (varying from 3 to 9 depending on study) nested within study (10 studies).""; ""we conducted a random-effects meta-analysis on the estimates of the effect of protocol within each replication study.""]",TRUE,"""In exploratory analyses, we considered several other measures of replicability that directly assessed (a) statistical consistency between the replications and the original studies and (b) the strength of evidence provided by the replications for the scientific effects under investigation (Mathur & VanderWeele, 2020). These analyses also accounted for potential heterogeneity in the replications and for the sample sizes in both the replications and the original studies. Accounting for these sources of variability avoids potentially misleading conclusions regarding replication success that can arise from metrics that do not account for these sources of variability, such as agreement in statistical significance.""",2,TRUE,10.1111/rssa.12572 [Comment: statistical consistency],FALSE,NA,TRUE,"""the studies were not selected to be representative of any particular population. The extent to which our findings will generalize is unknown. It is possible that our findings are unique to this sample of studies, or to psychology studies that are conducted in good faith but fail to be endorsed by original authors, as in the RP:P (Open Science Collaboration, 2015). A more expansive possibility is that the findings will be generalizable to occasions in which original authors or other experts dismiss a failed replication for having design flaws that are then addressed and tested again."""
Variability in the analysis of a single neuroimaging dataset by many teams,2020,10.1038/s41586-020-2314-9,Helena Hartmann; Rachel Heyard,Life Sciences,Neuroscience; Neuroimaging,"Many Phenomena, Many studies","""assess the effect of this flexibility on the results of functional magnetic resonance imaging""; ""to assess the real-world variability of results across independent teams analysing the same dataset.""",FALSE,NA,Same data - different analysis,(1) Agreement in statistical significance [Comment: Rates of reported significant findings across hypotheses] (2) Subjective assessment [Comment: confirmation of hypothesis or not] [Comment: Analysis of the pipeline used by the teams],TRUE,Prediction markets,3,TRUE,10.1073/pnas.1516179112 [Comment: Prediction markets],TRUE,"""note that because some analyses were performed on the final market prices (that is, the predictions of the markets), for which there is one value per hypothesis per market, the number of observations for each of the markets was low (n = 9), leading to limited statistical power. Therefore, the results should be interpreted with caution."" [Comment: prediction markets]",FALSE,NA
The influence of hidden researcher decisions in applied microeconomics,2021,10.1111/ecin.12992,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Economics; Applied Microeconomics,"Many Phenomena, Many studies","""To measure the magnitude of variation due to researcher degrees of freedom in the context of applied microeconomics studies that attempt to isolate a causal effect""",FALSE,NA,Same data - different analysis,(1) Agreement in statistical significance (2) Agreement in effect size,TRUE,They additionally coded difference analysis step/decision in a binary manner; Comparison of sampling variation,3.5,FALSE,NA,FALSE,NA,TRUE,"""Among successful replicators, the mean and median year in which they received their PhD was 2011 and 2014, respectively. As of October 2020, the mean and median Google Scholar “cited by” count among successful replicators were 366 and 128, respectively, with a minimum of 38 and a maximum of 1291, omitting one replicator who did not have a Google Scholar profile."" (p.950) [Comment: Looked at seniority and professional success of replicators]"
Investigating the replicability of preclinical cancer biology,2021,10.7554/eLife.71601,Joris Frese; Rachel Heyard,Life Sciences,Biology; Preclinical cancer biology,"Many Phenomena, One Study",To investigate the replicability of preclinical cancer biology,FALSE,NA,Different data - same analysis,"(1) Agreement in effect size [Comment: Same direction: We evaluated whether the sign of the replication estimate agreed with that of the original effect] (2) Agreement in statistical significance [Comment: Significance agreement: We assessed whether the replication had a p-value less than 0.05 and an estimate in the same direction as the original.] (3) Agreement in effect size [Comment: We assessed whether the original and replication estimates were inside the 95% confidence intervals of the other.] (4) Agreement in effect size [Comment: We assessed whether the replication estimate was inside the 95% prediction interval of the original, and formally assessed the degree of statistical inconsistency between the replication and the original using the metric porig, which can be viewed as a p-value for the hypothesis that the original and the replication had the same population effect size.] (5) Agreement in effect size [Comment: We estimated the difference in estimates between the replication and the original after transforming all effect sizes to a comparable scale.] (6) Meta-analysis of original and replication study/studies [Comment: we calculated a pooled estimate from combining the replication and original via fixed-effects meta-analysis]",FALSE,NA,7,TRUE,10.1111/rssa.12572 [Comment: porig],TRUE,"(1) This rudimentary metric does not account for effect sizes nor statistical precision, but was useful because we could compute it for even the non-quantitative pairs such as when original experiments reported only representative images. [Comment: Same direction] (2) We chose a fixed-effect model rather than a random-effects model primarily for consistency with our assumption throughout all main analyses that there was no within-pair heterogeneity. [Comment: Meta-analysis]",FALSE,NA
How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project,2019,10.1177/0956797619831612,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology; Behavioral Science,"Many Phenomena, One Study","""to estimate the replicability of the personality-outcome literature.""",TRUE,"""the replicability of behavioral science—the likelihood that independent researchers conducting similar studies will obtain similar results."" [Comment: Replicability]",Different data - same analysis,(1) Agreement in statistical significance [Comment: defined simply as the proportion of replication attempts that yielded statistically significant results in the hypothesized direction.] (2) Agreement in effect size [Comment: we examined the frequency with which the replication attempts obtained a trait–outcome association weaker than the corresponding original effect or not in the expected direction.],FALSE,NA,2,FALSE,NA,FALSE,NA,FALSE,NA
Same data different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis,2021,10.1016/j.obhdp.2021.02.003,Joris Frese; Rachel Heyard,"Social Sciences and Humanities; STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary,"One Phenomenon, Many Studies","""Our project examined whether independent analysts would arrive at similar analyses and statistical results using the same dataset to address these questions.""",FALSE,NA,Same data - different analysis,"(1) Agreement in statistical significance [Comment: summarizes the number of analysts who obtained statistically significant support for the hypothesis, directional but non-significant support, directional results contrary to the hypothesis, and statistically significant results contrary to the initial prediction.] (2) Agreement in effect size [Comment: by computing the z-score for each statistical result’s p-value]",TRUE,"""Boba multiverse analysis: To complement the qualitative analyses based on DataExplained, we also examined underlying processes quantitatively, through a Boba multiverse analysis (Liu et al., 2020). This crossed all of the crowd of analysts’ choices with one another, removing analytic choices that did not make sense in conjunction with one another"" (p.241)",3,TRUE,10.1109/TVCG.2020.3028985 [Comment: Boba multiverse analysis],FALSE,NA,FALSE,NA
Interaction effects in econometrics,2013,10.1007/s00181-012-0604-2,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Econometrics,"Many Phenomena, Many studies","""provide practical advice for applied economists regarding robust specification and interpretation of linear regression models with interaction terms;replicate a number of prominently published results using interaction effects and examine if they are robust to reasonable speciﬁcation permutations""; ""We replicate a number of prominently published results using interaction effects and examine if they are robust to reasonable speciﬁcation permutations.""",FALSE,NA,Same data - different analysis,(1) Agreement in statistical significance [Comment: size and significance of the interactions],FALSE,NA,1,FALSE,NA,FALSE,NA,FALSE,NA
Multiple Perspectives on Inference for Two Simple Statistical Scenarios,2019,10.1080/00031305.2019.1565553,Joris Frese; Rachel Heyard,"Social Sciences and Humanities;Life Sciences; STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary; Statistics,"Many Phenomena, Many studies","""When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data?"" (Abstract); In addition to providing an empirical answer to the question “does it matter?”, we hope to highlight how the same dataset can give rise to rather different statistical treatments.",FALSE,NA,Same data - different analysis,"(1) Subjective assessment [Comment: All findings were deemed inconclusive; Comparison of the conclusions, qualitatively similar]",FALSE,NA,1,FALSE,NA,FALSE,NA,FALSE,NA
Systematic assessment of the replicability and generalizability of preclinical findings: Impact of protocol harmonization across laboratory sites,2022,10.1371/journal.pbio.3001886,Helena Hartmann; Rachel Heyard,Life Sciences,Preclinical animal research,"One Phenomenon, Many Studies",assess the influence of protocol standardization between laboratories on their replicability of preclinical results;aimed to elucidate parameters that impact the replicability of preclinical animal studies.; determine whether harmonization of study protocols across laboratories improves the replicability of the results and whether replicability can be further improved by systematic variation (heterogenization) of 2 environmental factors (time of testing and light intensity during testing) within laboratories,TRUE,"""Defining results replicability as the ability to duplicate results from a previous scientific claim supported by new data"" [Comment: Replicability]",Different data - different analysis,(1) Agreement in statistical significance (2) Agreement in effect size [Reduction in variability of results],FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,we could not confirm that diversifying the environmental conditions further reduces the variability across laboratories. The current selection of “heterogenizing” factors was rather limited by the feasibility to diversify them across all labs.; It also shows that it is possible to diversify the study sample by incorporating blocking factors like sex or introducing systematic heterogenization of conditions without the need to increase the overall sample size.
Many Analysts One Data Set: Making Transparent How Variations in Analytic Choices Affect Results,2018,10.1177/2515245917747646,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology; Statistics,"One Phenomenon, Many Studies","""In this article, we report an investigation that addressed the current lack of knowledge about how much diversity in analytic choice there can be when different researchers analyze the same data and whether such diversity results in different conclusions.""",FALSE,NA,Same data - different analysis,"(1) Agreement in statistical significance [Comment: Twenty teams (69%) found a significant positive relationship, p < .05, and nine teams (31%) found a nonsignificant relationship. No team reported a significant negative relationship.] (2) Agreement in effect size",FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"""An important question is whether the variability in the analytic choices made and results found by the teams resulted from teams with the greatest statistical expertise making different choices than the other teams. A related question is whether teams whose members had more quantitative expertise showed greater convergence in their estimated effect sizes. To answer these questions, we dichotomized the teams into two groups using latent class analysis. The first group (n = 9) was more likely to have a team member who had a Ph.D. (100% vs. 53%), was a professor at a university (100% vs. 37%), had taught a graduate statistics course more than twice (100% vs. 0%), and had at least one methodological or statistical publication (78% vs. 47%)."" (p.350) [Comment: Not really EDI, but discussed seniority and previous publication success of replicators.]; ""A demographic survey revealed that the team leaders worked in 13 different countries and came from a variety of disciplinary backgrounds, including psychology, statistics, research methods, economics, sociology, linguistics, and management. At the time that the first draft of this manuscript was written, 38 of the 61 data analysts (62%) held a Ph.D. (62%), and 17 (28%) had a master’s degree. The analysts came from various ranks and included 8 full professors (13%), 9 associate professors (15%), 13 assistant professors (21%), 8 postdocs (13%), and 17 doctoral students (28%). In addition, 27 participants (44%) had taught at least one undergraduate statistics course, 22 (36%) had taught at least one graduate statistics course, and 24 (39%) had published at least one methodological or statistical article.;a crowdsourced approach adds value in a number of ways. A globally distributed crowdsourced project will leverage skills, perspectives, and approaches to data analysis that no single analyst or research team can realistically muster alone."""
Comparison of two independent systematic reviews of trials of recombinant human bone morphogenetic protein-2 (rhBMP-2): the Yale Open Data Access Medtronic Project,2017,10.1186/s13643-017-0422-x,Louise Townsin; Rachel Heyard,Life Sciences,Health; Clinical trials; Evidence synthesis,"One Phenomenon, Many Studies","""We retrospectively compared the research methods and results of the final comprehensive publications of two meta-analyses performed in the context of full systematic reviews of recombinant human bone morphogenetic protein-2 (rhBMP-2) prepared by two independent centers, Center A [26, 27] from the University of York and Center B [28, 29] from Oregon Health & Science University, and focused on (1) meta-analysis trial inclusion criteria; (2) statistical methods; (3) summary risk estimates; and (4) conclusions.""; ""to determine if two independent centers, each of which were contracted to pursue identical research questions concurrently, with access to identical IPD, would employ identical methods in the areas of data use and statistical analysis and report identical, or at least consistent, results and conclusions.""",TRUE,"""The premise of replication efforts is that different groups, employing rigorous methods, may take different approaches and come to different conclusions on a previously addressed question""; ""Previous studies have sought to determine whether systematic reviews are replicable, with new teams performing new searches, summaries, and analyses of the literature for a particular question.""",Different data - same analysis,"(1) Agreement in statistical significance [Comment: Center A found a statistically significant increase in fusion rate at 24 months [...] Center B, reporting results for each surgical approach separately, did not find a significant increase in fusion at 24 months.] (2) Subjective assessment [Comment: In consideration of these factors, we provide a subjective comparison of the overall conclusions drawn by each center.]",TRUE,"Trial inclusion criteria were defined as study characteristics necessary for inclusion in meta-analysis. We explicitly compared, for primary and secondary endpoint meta-analyses, as well as safety analyses, the trials used by both centers for each analysis.; For methods, we compared centers’ reported outcomes at various time points as well as statistical methods.; We compared the centers’ risk estimates for all primary outcomes for efficacy as well as safety at all time points.",3,FALSE,NA,FALSE,NA,FALSE,NA
Replicability of simulation studies for the investigation of statistical methods: the RepliSims project,2024,10.1098/rsos.231003,Joris Frese; Rachel Heyard,"Social Sciences and Humanities; Life Sciences; STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary [Simulation study; Statistics; Empirical research],"Many Phenomena, One Study","""The current study aims at the following: — discussing the definitions of reproducibility and replicability in the context of simulation studies; — illustrating that replicability of simulation studies is not a given, using the replication of eight simulation studies as an example; — describing features that hinder and facilitate replicability of the original studies; and — providing preliminary recommendations for future simulation studies to facilitate replicability, in addition to available guidance for reporting of simulation studies."" (p.2)",TRUE,"Replication: writing new code to generate and analyse new data, following procedures in the original study as closely as possible [Comment: Replicability]",Different data - same analysis,"(1) Subjective assessment [Comment: assessed in a qualitative manner and involved evaluating: whether numerical values from the replication studies were comparable to those in the original studies, whether trends in the results were moving in the same direction, and whether the performance rankings of different simulation scenarios matched those in the original studies.]",FALSE,NA,1,FALSE,NA,FALSE,NA,FALSE,NA
Reproducibility via coordinated standardization: a multi-center study in a Shank2 genetic rat model for Autism Spectrum Disorders,2019,10.1038/s41598-019-47981-0,Helena Hartmann; Rachel Heyard,Life Sciences,Neuroscience; Preclinical studies; Biomedicine,"One Phenomenon, Many Studies",whether reproducibility increases through harmonization of apparatus test protocol and aligned and non-aligned environmental variables,FALSE,NA,Different data - same analysis,(1) Agreement in statistical significance,TRUE,"Three-way ANOVA [Comment: A three-way ANOVA with genotype (two levels) and site (three levels) as between-subject factors, and treatment as the repeated within-subject factor [four levels (vehicle, 3 doses)] was performed on absolute data values for each of the readouts. In the case of a main site and genotype effect in this absolute data set, normalized values (relative to vehicle treatment) were analyzed using the same three-way ANOVA design. This analysis aimed to address reproducibility across sites in terms of the phenotype evaluation as well as the pharmacological intervention.]",2,FALSE,NA,FALSE,NA,FALSE,NA
Quantitative Macroeconomics: Lessons Learned from Fourteen Replications,2023,10.1007/s10614-022-10234-w,Rachel Heyard; Samuel Pawel,Social Sciences and Humanities,Quantitative Macroeconomics; Economics,"Many Phenomena, One Study","""The focus of this paper is instead on the lessons to be learned from these replications and on providing some suggestions for best practice based on the experience of performing the replications.""; ""replicate all tables and figures from fourteen papers in Quantitative Macroeconomics""",FALSE,NA,Different data - same analysis,None of the above,TRUE,"Percentage difference between the replication and original results [Comment: It is based on all the entries of all the Tables from each paper: the absolute percentage difference between the replication value and the value in the original paper was calculated for every table entry, and the quartiles of these are reported.]",1,FALSE,NA,TRUE,The main weakness of this is that it obviously misses any Figures. [Comment: quartiles of the percentage difference],FALSE,NA
Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015,2018,10.1038/s41562-018-0399-z,Helena Hartmann; Joris Frese,Social Sciences and Humanities,Interdisciplinary,"Many Phenomena, Many studies",Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015,TRUE,"find a significant effect in the same direction as the original study, Being able to replicate scientific findings",Different data - same analysis,(1) Agreement in effect size [Comment: direction of effect] (2) Agreement in statistical significance [Comment: p </> .05],FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"Each investigation has a relatively small sample of studies with idiosyncratic inclusion criteria and unknown generalizability. However, the diversity in approaches provides some confidence that considering them in the aggregate may provide more general insight about reproducibility in the social behavioural sciences."
Reproducibility in Management Science,2024,10.31219/osf.io/mydzv,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Management sciences,"Many Phenomena, One Study",Assess the reproducibility of articles published in Management Science before and after the introduction of the 2019 policy,TRUE,"""The term “computational reproducibility” comes closest to the scope of our study, and is defined as the extent to which results in studies can be reproduced based on the same data and analysis as the original study.""",Same data - same analysis,"(1) Agreement in statistical significance (2) Agreement in effect size (3) Subjective assessment [Comment: After having reviewed a study’s replication package, reviewers are asked “What is your overall assessment of the reproducibility of this article's main results (tables, figures, other results in the main manuscript)?” with six response options.]",TRUE,More assessment on replication package quality,4,FALSE,NA,TRUE,the eventual categorization of the article remains subjective to the reviewer. [Comment: Subjective assessment],FALSE,NA
A Multilab Preregistered Replication of the Ego-Depletion Effect,2016,10.1177/1745691616652873,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies","""We proposed a set of independent replications of the ego-depletion effect using the sequential-task paradigm""",FALSE,NA,Different data - same analysis,"(1) Agreement in effect size [Comment: Averaged sample-weighted effect sizes for the mean, only three labs found mean RT values with confidence intervals that did not include the value of zero, two of which were negative] (2) Meta-analysis of original and replication study/studies [Comment: Meta analysis of 23 replications. Effect size and confidence interval of meta analysis were reported and interpreted in comparison to the original effect size.]",TRUE,"Heterogeneity in the effect sizes with Cochranes Q and I2 statistics, with a statistically significant value for Q and an I2 value greater than 25% indicative of at least moderate levels of heterogeneity in the effect size across studies.",3,FALSE,NA,FALSE,NA,FALSE,NA
Systematizing Confidence in Open Research and Evidence (SCORE),2024,10.31235/osf.io/46mnb,Helena Hartmann;Louise Townsin,Social Sciences and Humanities,"Interdisciplinary [Comment: Criminology, Economics and Finance, Education, Health, Management, Marketing and Organizational Behavior, Political Science, Psychology, Public Administration, and Sociology]","Many Phenomena, Many studies","The primary research objective for SCORE is to create accurate, scalable, automated algorithms to signal confidence in research claims [Comment: SCORE has aspirational objectives to advance scalable tools for credibility assessment, and will generate substantial research artifacts to support scholarly research on human and machine judgment, replicability and reproducibility, and the nature of research claims. This is made possible by SCORE’s greatest asset -- the participation of hundreds of researchers across the social and behavioral sciences contributing to claim extraction, credibility assessment, and reproducibility, robustness, and replication studies.]",TRUE,Replication: Testing the reliability of a prior finding with new data expected to be theoretically equivalent by comparing the outcome of an inferential test as reported in a paper with the equivalent inferential test as calculated in the new dataset.  Reproducibility: Testing the reliability of a prior finding with the same data and same analysis strategy by comparing the outcome of an inferential test as reported in a paper with a re-calculation of that inferential test from the original data.,Same data - same analysis; Same data - different analysis; Different data - same analysis;Different data - different analysis,None of the above,TRUE,"predictions, called confidence scores, from human readers about replicability of extracted claims (replicats, prediction markets); generate confidence scores using machine learning and other algorithmic approaches. [Comment: used human evaluators to provide confidence scores predicting the reproducibility or replicability of the 3,000 research claims in the annotation set. Also used machine learning methods to develop algorithms that assign confidence scores just like the human evaluators. Also created a stratified random sample of 600 of those papers to create the evidence set. Some claims from the evidence set were subjected to reproduction and replication studies.]",2,TRUE,"https://doi.org/10.31222/osf.io/2pczv, osf.io/svg3x; [Comment: Not reproducibility specifically but In discussion of work to predict/ manage human and automated scores of possible replication: Described in detail by Hanea and colleagues (2021), the aggregation models fall into three categories: (1) linear combinations of best estimates, transformed best estimates (Satopää et al., 2014) and distributions (Cooke et al., 2021); (2) Bayesian approaches, one of which incorporates characteristics of a claim directly from the paper, such as sample size and effect size; and (3) weighted linear combinations of best estimates, mainly by potential proxies for good forecasting performance, such as demonstrated breadth of reasoning, engagement in the task, openness to changing opinion and informativeness of judgments (Mellers, Stone, Atanasov, et al., 2015; Mellers, Stone, Murray, et al., 2015). Replication Markets extends work showing that markets of domain experts can accurately estimate the replicability of findings in the social and behavioral sciences (Camerer et al., 2016, 2018; Dreber et al., 2015; Ebersole et al., 2020; Forsell et al., 2018; Klein et al., 2018; Gordon et al., 2021).]",FALSE,NA,TRUE,"SCORE forecasted 3,000 highly diverse claims; Generalizable = Original claim supported across diverse samples, treatments, outcomes, and settings; Robust = Original claim supported with diverse treatments of original data;SCORE is inclusive of a substantial portion of the social-behavioral sciences to facilitate generalizability and investigation of heterogeneity in credibility and replicability across subdisciplines and methodologies. Also, with a standard identification process of discrete claims across papers, the SCORE program facilitates broad inclusion of outcome types, comparison of those outcomes across papers, and a variety of verification attempts including reproduction, robustness, and replication tests."
Evaluating replicability of laboratory experiments in economics,2016,10.1126/science.aaf0918,Joris Frese; Louise Townsin,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","""To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014."" (Abstract)",FALSE,NA,Different data - same analysis,"Agreement in statistical significance [Comment: We present results for the same replication indicators that were used in the RPP. As our first indicator of replication, we used a “significant effect in the same direction as in the original study”.] (2) Agreement in effect size [Comment: A complementary method for assessing replicability is to test whether the 95% confidence interval (CI) of the replication effect size includes the original effect size. An alternative measure, which acknowledges sampling error in both the original study and the replications, is to count how many replicated effects lie in a 95% “prediction interval”. We also tested whether replicability is correlated with two observable characteristics of published studies: the P value and the sample size (number of participants) of the original study.] (3) Meta-analysis of original and replication study/studies [Comment: The original and replication studies can also be combined in a meta-analytic estimate of the effect size.]",TRUE,"Predicition markets [To measure peer beliefs about the replicability of original results, we set up prediction markets before the 18 replications were performed.]",5,TRUE,10.1198/000313006X152649; 10.1111/j.1745-6924.2008.00079.x,TRUE,"""An alternative measure, which acknowledges sampling error in both the original study and the replications, is to count how many replicated effects lie in a 95% “prediction interval” (24).""; ""the original effect sizes in the studies that we replicated could have been inflated, a phenomenon that could stem from publication bias (28). If there is publication bias, our prospective power analyses will have overestimated the replication power.""",FALSE,NA
Reproducibility of findings in modern PET neuroimaging: insight from the NRM2018 grand challenge,2021,10.1177/0271678X211015101,Helena Hartmann; Rachel Heyard,Life Sciences,Neuroimaging,"One Phenomenon, Many Studies",assess the performance and consistency of PET pre-processing and modelling approaches on a common data set where the ground truth was known,FALSE,NA,Same data - different analysis,None of the above,TRUE,percentage root-mean-squared error (RMSE); Jaccard similarity index,2,FALSE,NA,FALSE,NA,TRUE,"The full dataset, inclusive of simulated dynamic PET images, reference kinetic parameters and areas of displacement..."
Registered Replication Report: Study 1 From Finkel Rusbult Kumashiro & Hannon (2002),2016,10.1177/1745691616664694,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies","This Registered Replication Report (RRR) meta-analytically combines the results of 16 new direct replications of the original study, all of which followed a standardized, vetted, and preregistered protocol.; this study provides the only experimental evidence that inducing changes to subjective commitment can causally affect forgiveness responses.; to provide a direct replication of this influential finding and to provide a more precise estimate of the size of the effect of this commitment prime on how people report that they would respond to betrayals from a romantic partner",TRUE,"""direct replications of the original study, all following the same vetted protocol""",Different data - same analysis,"(1) Agreement in statistical significance (2) Agreement in effect size [Comment: Comparison of original effect to meta-analytical effect from all replications. ""If the RRR precisely replicated the results of the original study, it would observe a similarly sized difference for exit and neglect, and a similar lack of a difference for voice and loyalty""]",FALSE,NA,2,FALSE,NA,FALSE,NA,FALSE,NA
Reproducibility of real-world evidence studies using clinical practice data to inform regulatory and coverage decisions,2022,10.1038/s41467-022-32310-3,Rachel Heyard; Samuel Pawel,Life Sciences,Medicine; Randomized clinical trials; Real-world evidence,"Many Phenomena, One Study","""To evaluate the independent reproducibility of results from 150 published RWE studies using the same healthcare databases and applying the same reported methods as original authors.""",TRUE,"Reproducibility is the ability to obtain the same results when reanalyzing the original data, following the original analysis strategy. [Comment: They defined it as being different from computational reproducibility, since they did not use original data and code - but simply followed the same steps to acquire the same data and analysis.]",Same data - same analysis,(1) Agreement in effect size [Comment: relative magnitude of the original effect size compared to the reproduction effect size; Pearson’s correlation coefficient between the reproduced and original effect sizes (both unweighted and inverse variance weighted); proportion of studies where the absolute difference in the coefficients for the measure of association differed by ≤0.1 or ≤0.2; proportion of comparative studies where the reproduced measure of association was closer to null than the original; the proportion where the measures were on the same side of null; the proportion with any overlap in 95% confidence intervals],TRUE,"Reproducibility of population sample size was measured by dividing the sample size of the original study by the reproduction.; Reproducibility of baseline characteristics reported in an original manuscript table describing the cohort characteristics. The reproducibility of binary and categorical baseline characteristics of the study population was measured by taking the prevalence of the characteristic reported in the original publication (within each exposure group if there was more than one) and subtracting the prevalence obtained in the reproduction.; Reproducibility of outcome risks and rates was measured by taking the reported outcome risk or rate in the original publication and subtracting the risk or rate obtained in the reproduction. For descriptive studies, the overall risk or rate was reproduced. For comparative studies, the risk or rate was reproduced for each compared group. Rates were converted to reﬂect events per 100 person-years.; Reproducibility metrics for measures of associations of interest were the relative magnitude of the original effect size compared to the reproduction effect size and the Pearson’s correlation coefﬁcient between the reproduced and original effect sizes. [Comment: They used calibration plots and bland-altman plots to represent findings and compare results.]",12,FALSE,NA,TRUE,"the proportion of studies where the effect estimate was on the same side of null can mislead in RWE studies with small effect sizes as small implementation differences could conceivably result in enough change to result in an effect estimate on the other side of null in a reproduction attempt. Further, the reproduction effort is focused on US and UK data sources frequently used in research and the generalizability may be limited to well-established and curated research databases that are accessible to independent researchers. [Comment: they refer to this: 10.1515/em-2016-0018]",FALSE,NA
Using mice from different breeding sites fails to improve replicability of results from single-laboratory studies,2024,10.1038/s41684-023-01307-w,Helena Hartmann; Rachel Heyard,Life Sciences,Neuroscience; Animal studies; Preclinical research,"Many Phenomena, Many studies","In this Article, we therefore tested whether systematic hetero- genization of study populations, by using mice from different breeding sites to introduce the genetic and environmental variation that normally exists between independent study populations, would increase the external validity of the results sufficiently to guarantee replicability.",FALSE,NA,Different data - different analysis,None of the above,TRUE,"multivariate analysis of variance (MANOVA); Levene tests for equal variances; To investigate whether HET designs led to lower between-laboratory variation than STA designs, we ran for each outcome variable two separate mixed models with the outcome as a dependent variable, laboratory as fixed effect and cage ID as a random factor—one for the HET design and one for the STA design. We then compared the marginal R2 estimates. [Comment: For comparing the variance of each Levene: outcome variable between STA and HET cohorts within each of the laboratory]",1,FALSE,NA,FALSE,NA,True [Comment: not really the diveristy we mean I think],"the diversity of the mice within breeding sites may have been greater than expected, thereby limiting the scope for variation between breeding sites. This could, for example, be due to variation in age (the age of mice may vary by several days) and origin from different colony rooms."
Estimating the reproducibility of psychological science,2015,10.1126/science.aac4716,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, One Study",To estimate the reproducibility of psychological science,TRUE,"Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding (7, 8) and is the means of establishing reproducibility of a finding with new data.",Different data - same analysis,"(1) Agreement in statistical significance [Comment: original studies that interpreted nonsignificant P values as significant were coded as significant (four cases, all with P values < 0.06). Using only the nonsignificant P values of the replication studies and applying Fisher’s method (26), we tested the hypothesis that these studies had “no evidential value” (the null hypothesis of zero-effect holds for all these studies).] (2) Agreement in effect size [Comment: We compared effect sizes using four tests. We compared the central tendency of the effect size distributions of original and replication studies using both a paired two-sample t test and the Wilcoxon signed-rank test. Third, we computed the proportion of study-pairs in which the effect of the original study was stronger than in the replication study and tested the hypothesis that this proportion is 0.5. For this test, we included findings for which effect size measures were available but no correlation coefficient could be computed (for example, if a regression coefficient was reported but not its test statistic). Fourth, we calculated “coverage,” or the proportion of study-pairs in which the effect of the original study was in the CI of the effect of the replication study, and compared this with the expected pro- portion using a goodness-of-fit c2 test.] (3) Meta-analysis of original and replication study/studies [Comment: fixed-effect meta-analyses using the R package metafor. The number of times the CI of all these meta-analyses contained 0 was calculated.] (4) Subjective assessment [Comment: For subjective assessment, replication teams answered “yes” or “no” to the question, “Did your results replicate the original effect?”]",FALSE,NA,6,FALSE,NA,TRUE,These data provide information about reproducibility in general but little precision about individual effects in particular.,FALSE,NA
Comparison of Model Averaging Techniques: Assessing Growth Determinants,2012,10.1002/jae.2288,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Economics; Econometrics,"Many Phenomena, One Study","""This paper set out to replicate several important studies focused on model uncertainty in the modeling of economic growth.""",FALSE,NA,Same data - different analysis,Agreement in effect size [Comment: Comparison of coefficient estimates],TRUE,Comparison of posterior inclusion probability,2,FALSE,NA,FALSE,NA,FALSE,NA
Registered Replication Report: Strack Martin & Stepper (1988),2016,10.1177/1745691616674458,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies",to measure the reliability and size of an effect.,TRUE,replicated directly using the same design and the same dependent variable. [Comment: Replicability],Different data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size [Comment: the 95% confidence interval was narrower than the one estimated for Study 1 from SMS] (3) Agreement in effect size [Comment: the 95% confidence interval overlapped the mean effect size from SMS Study 1] (4) Agreement in effect size [Comment: Random-effects meta-analysis of all replications vs. original],TRUE,"""In addition to this primary analysis, we report two Bayes factor analyses for each study."" (p. 921)",5,FALSE,NA,FALSE,NA,FALSE,NA
Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses,2023,10.1177/25152459231162567,Helena Hartmann; Rachel Heyard,"Social Sciences and Humanities;Life Sciences;STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary; Linguistics; Speech production research;Acoustic research,"One Phenomenon, Many Studies",assess flexibility in fields in which the primary data lend themselves to a variety of possible operationalizations; The primary aim of this analysis was to assess the degree of between-team variability.,TRUE,(a) produce data that can be replicated using the original methods and (b) arrive at robust conclusions substantiated by such data.,Same data - different analysis,"(1) Meta-analysis of original and replication study/studies [Comment: We investigated the variability in reported effect sizes using Bayesian meta-analytic techniques. As the measure of variability, we took the meta-analytic group-level standard deviation (σαt; see below), where each analysis team represents a group.] (2) Agreement in effect size",TRUE,"Descriptive summary statistics [Comment: describing variation among analyses, including (a) the nature and number of acoustic measures (e.g., f0 or dura- tion), (b) the operationalization and the temporal domain of measurement (e.g., mean of an interval or value at a specified point in time), (c) the nature and number of model parameters for both fixed and random effects (if applicable), (d) the nature and reasoning behind inferential assessments (e.g., dichotomous decision based on p values, ordinal decision based on a Bayes factor), as well as the (e) mean, (f) standard deviation, and (g) range of the standardized effect sizes (see the next section for the standardization procedure).]",3,FALSE,NA,FALSE,NA,FALSE,NA
Leveraging real-world data to assess treatment sequences in health economic evaluations: a study protocol for emulating target trials using the English Cancer Registry and US Electronic Health Records-Derived Database,2024,NA,Joris Frese; Rachel Heyard,"Life Sciences;STEM, e.g. Engineering, Mathematics, Physics; Social Sciences and Humanities",Medicine,"Many Phenomena, One Study","""to assess the feasibility of harnessing data from the English Cancer Registry and Flatiron electronic health record (EHR)-derived database to derive unbiased effect estimates for comparing oncology treatment sequences.""",TRUE,"""replicate the designs and results of existing clinical trials (i.e., benchmark trials) through emulating Target Trials using RWD, including applying the same (or as far as possible) patient inclusion/exclusion criteria and analytical methods to achieve the emulation"" [Comment: Emulation]",Different data - same analysis,"(1) Agreement in statistical significance [Comment: Regulatory agreement: This component assesses whether the RWE replicates its benchmark’s results (such as hazard ratio (HR) and risk ratio (RR)) in terms of both direction and statistical significance observed in the benchmark trials. Endpoints with non-significant effects in RCTs should also show no significant effect in RWE.] (2) Agreement in effect size [Comment: Estimate agreement: examines whether the point estimate of RWE’s effect sizes falls within the 95% confidence intervals (CIs) of the benchmark trial. Furthermore, we added an extra procedure to include the comparison of non-relative effect estimates for timeto-event outcomes. For example, it examines whether the point effect of median survival estimates falls within the 95% confidence interval (CI) of the trial.] (3) Agreement in effect size [Comment: Exploratory - standardised differences: It involves computing the standardised difference to compare the relative effect estimates from the benchmark and the RWE, to determine whether there is a statistically significant difference in the estimated effects.]",TRUE,Exploratory - survival curve comparison: The key aspect here is assessing whether the point estimates of the RWE survival curve for each treatment-sequence group fall within the 95% CI of the benchmark trial.,4,TRUE,10.1002/cpt.1633 [Comment: Agreement measures],TRUE,"""Considering the potentially disproportionately large sample size of RWE, achieving statistical significance might be easier compared to the benchmarks"" (and thus not easier to meet the first criteria). [Comment: Agreement in statistical significance]",FALSE,NA
A rise by any other name? Sensitivity of growth regressions to data source,2008,10.1016/j.jmacro.2007.08.015,Helena Hartmann; Louise Townsin,Social Sciences and Humanities,Economics,"Many Phenomena, One Study",Replication of several recent studies of growth determinants,FALSE,NA,Different data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size,FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"Table 8 are based on the “full conditioning set of variables” that includes the variables of interest plus initial years of schooling, government size, inflation rate, black market premium, openness to trade, number of revolutions and coups, political assassinations and ethnic diversity.; As of version 6.1 PWT contains 115 benchmark countries ( i.e. countries included in the ICP) and 53 additional nonbenchmark countries."
Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology,2020,10.1016/j.jpsychores.2020.110211,Joris Frese; Rachel Heyard,Social Sciences and Humanities;Life Sciences,Psychology; Medicine; Statistics [Comment: Investigate experience sampling methodology],"One Phenomenon, Many Studies",evaluate how much researchers vary in their analytical approach towards these individual time-series data and to what degree outcomes vary based on analytical choices.,FALSE,"""we crowdsourced the analysis of one individual patient's ESM data to 12 prominent research teams, asking them what symptom(s) they would advise the treating clinician to target in subsequent treatment."" (Abstract)",Same data - different analysis,(1) Subjective assessment,TRUE,"Description of differences in analytical approaches including variable selection, preprocessing, clustering and statistical analyses.; Description of differences in results, e.g. selected targets, treatment selection",3,FALSE,NA,FALSE,NA,TRUE,"None of the analytic approaches were inherently invalid. Instead, the multiplicity of plausible processing steps implies that there could be several sensible statistical results based on the same original dataset [Comment: Referencing to this 10.1177/1745691616658637]"
Estimating the Reproducibility of Experimental Philosophy,2021,10.1007/s13164-018-0400-9,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Philosophy; Experimental Philosophy,"Many Phenomena, One Study","""Responding to recent concerns about the reliability of the published literature in psychology and other disciplines, we formed the X-Phi Replicability Project (XRP) to estimate the reproducibility of experimental philosophy"" (Abstract)",TRUE,"""To answer this question, ‘direct’ replications are needed (Doyen et al. 2014). Direct replications—often contrasted with ‘conceptual’ replications—are replications that attempt to follow the design and methods of an original study as closely as possible in order to confirm its reported findings"" (p.14)",Different data - same analysis,"(1) Agreement in statistical significance [Comment: Were the replication results statistically significant? For the present research, we defined ‘statistically significant’ as a p-value less than .05, following the currently conventional default standards for Null Hypothesis Significance Testing (NHST).] (2) Agreement in effect size [Comment: Comparison of the original and replication effect size: Because sample sizes of replication studies were typically larger than those of original ones, and because calculation of confidence intervals (CIs) for original effect sizes were not always possible (due to a lack of information), we decided to draw this comparison by investigating whether the original effect size fell within the 95% CI of the replication effect size.] (3) Subjective assessment [Comment: Subjective assessment of the replicating team: By asking our researchers to register their overall subjective judgment about whether the effect replicated, therefore, they were able to take into consideration the ‘wider picture’.]",FALSE,NA,3,TRUE,10.1521/soco.2014.32.supp.12,TRUE,"(1) the use of p-values as a criterion for success is especially dubious when applied to studies reporting null results [Comment: Agreement in statistical significance](2) sample sizes of replication studies were typically larger than those of original ones, and calculation of confidence intervals (CIs) for original effect sizes were not always possible (due to a lack of information), [Comment: Agreement in effect size]",FALSE,NA
Many Labs 3: Evaluating participant pool quality across the academic semester via replication,2016,10.1016/j.jesp.2015.10.012,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Psychology [Comment: social psychology; cognitive psychology],"Many Phenomena, Many studies",we investigated the extent to which 10 psychological effects and multiple individual difference variables varied across the academic semester.,FALSE,NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: For each phenomenon, the effect aggregated for all replication participants was compared to the original effect] (2) Agreement in effect size [Comment: For each phenomenon, the effect aggregated for all replication participants was compared to the original effect]",FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"In addition, for the collected set of effects and measures we sought: (1) diversity of represented research domains, (2) diversity of known or presumed likelihood of variation across the semester, and (3) diversity of “classic” well-established effects and contemporary effects that have untested replicability.;These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample.; ""These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample.;Given this, one strategy would have been to only select classic, well-established effects for replication. However, it is possible that these effects are well established because they are resistant to contextual variation."""
Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement,2022,10.1525/collabra.35271,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Social psychology,"One Phenomenon, Many Studies",To investigate whether author involvement increases replicability,FALSE,NA,Different data - same analysis,"(1) Agreement in effect size [Comment: Original vs. meta-analysis of all replications; No clear explanation of metric used, but they did a random-effects meta-analysis on all the replications and then compared this with the original results]",FALSE,NA,1,TRUE,https://doi.org/10.3389/fpsyg.2014.01521,FALSE,NA,FALSE,NA
Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say “Often Not”,2022,10.1561/104.00000053,Helena Hartmann; Joris Frese,Social Sciences and Humanities,Economics; Macroeconomics,"Many Phenomena, Many studies",to replicate 67 macroeconomic papers published in 13 well-regarded economics journals using author-provided replication ﬁles that included both data and code by following a preanalysis plan.,TRUE,"""We deﬁned a successful replication as when the authors or journal provided data and code ﬁles that allowed us to qualitatively reproduce the key results of the paper."" (p.8)",Same data - same analysis,None of the above,TRUE,Reproduce Figures and Tables; produce the key qualitative conclusions of the original paper,2,TRUE,"(1) 10.1111/joes.12139 [Comment: Replication-Verification by Clemens (2017)](2) No DOI [Comment: Replication Accuracy Rating System by Glandon (2010)](3) 10.1353/mcb.2006.0061 [Comment: Partially Successful Replication (""replicating enough of the results that the conclusions of the article remain intact"") by McCollough et al. (2006)]",TRUE,"""We deﬁned success using this extremely loose deﬁnition to get an upper bound on what the replication success rate could be."" (p.8) [Comment: Replication Verification; Replication Accuracy System; Partially Successful Replication]",FALSE,NA
Investigating Variation in Replicability,2014,10.1027/1864-9335/a000178,Joris Frese; Rachel Heyard,Social Sciences and Humanities;Life Sciences,Psychology,"Many Phenomena, Many studies","to examine the heterogeneity of effect sizes by the wide variety of samples and settings, and to provide an example of a paradigm for testing such variation.",TRUE,"Replication is a central tenet of science; its purpose is to confirm the accuracy of empirical findings, clarify the conditions under which an effect can be observed, and estimate the true effect size. Successful replication of an experiment requires the recreation of the essential conditions of the initial experiment. [Comment: Replicability]",Different data - same analysis,(1) Agreement in statistical significance [Comment: Proportion of samples that rejected the null hypothesis in the expected and unexpected direction.] (2) Agreement in effect size [Comment: intra-class correlation of samples across effects + heterogeneity of effect sizes using Cochran's Q and I2 statistics],FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"""The replication sites included in this project cannot capture all possible cultural variation,and most societies sampled were relatively Western,Educated, Industrialized, Rich, and Democratic (WEIRD)"" (p.151); All replication studies were translated into the dominant language of the country of data collection"
Data Models Coefficients: The Case of United States Military Expenditure,2007,10.1080/07388940601102845,Joris Frese; Louise Townsin,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","""The purpose of this article is to demonstrate the obvious: that use of the “wrong” data leads one to obtain improper results. Even if one employs the “right” (or at least “better”) data, one can obtain results that are inconsistent when the sample is varied."" (p.55)",FALSE,NA,Different data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size,FALSE,NA,2,FALSE,NA,FALSE,NA,FALSE,NA
Emulation of Randomized Clinical Trials With Nonrandomized Database Analyses: Results of 32 Clinical Trials,2023,10.1001/jama.2023.4221,Helena Hartmann; Rachel Heyard,Life Sciences,Medicine; Clinical trials,"Many Phenomena, One Study",To emulate the design of 30 completed and 2 ongoing randomized clinical trials (RCTs) of medications with database studies using observational analogues of the RCT design parameters (population intervention comparator outcome time [PICOT]) and to quantify agreement in RCT-database study pairs,TRUE [Comment: Emulation],"We aimed to emulate RCT designs under the best possible circumstances by identifying and implementing observational analogues of RCT design parameters that define the research question (population, intervention, comparator, outcome, time-frame [PICOT]11), apply confounding adjustment methods, and then compare the results of RCT–database study pairs",Different data - same analysis,"(1) Agreement in statistical significance [Comment: full statistical significance agreement, defined by estimates and CIs on the same side of the null] (2) Agreement in effect size [Comment: estimate agreement, defined by whether estimates for the trial emulation fell within the 95% CI for the trial results] (3) Agreement in effect size [Comment: standardized difference agreement between treatment effect estimates from trials and emulations, defined by standardized differences |z|<1.96]",TRUE,partial significance agreement was defined as meeting the prespecified noninferiority criteria even though the database study may have indicated superiority,4,FALSE,NA,TRUE,"Second, apparent agreement between RCT and database study results could occur if the effects of multiple factors (chance, emulation differences, bias) cancel each other out.",FALSE,NA
Registered Replication Report: Rand Greene and Nowak (2012),2017,10.1177/1745691617693624,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities,Economics,"One Phenomenon, Many Studies",assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate preregistered replications of the critical conditions from Study 7 of the original article (Rand et al. 2012),FALSE,NA,Different data - same analysis,"(1) Agreement in effect size [Comment: estimate the effect size meta-analytically across studies, and compare it with the original.]",FALSE,NA,1,FALSE,NA,FALSE,NA,FALSE,NA
The challenge of mapping the human connectome based on diffusion tractography,2017,10.1038/s41467-017-01285-x,Helena Hartmann; Rachel Heyard,Life Sciences,Neuroscience; Medical imaging,"One Phenomenon, Many Studies","systematically validate tractography studies, to assess how well the algorithms were able to reproduce the known connectivity, our study is mainly about accuracy with respect to the reference, rather than reproducibility or robustness of tractography.",FALSE,NA,Same data - different analysis,None of the above,TRUE,"Comparison of final results, as counts, proportions compared to ground truth (instead of original)",1,FALSE,NA,FALSE,NA,FALSE,NA
Many Labs 2: Investigating Variation in Replicability Across Samples and Settings,2018,10.1177/2515245918810225,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies","""We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were  peer reviewed in advance, to examine variation in effect magnitudes across samples and settings"" (Abstract)",FALSE,NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: Significance criterion with alpha 0.05 and 0.001, and alpha 0.05 combined with different sample sizes] (2) Agreement in effect size",FALSE,NA,2,FALSE,NA,FALSE,NA,TRUE,"""Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized,  rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness  scores, respectively)."" (Abstract)"
The Brazilian Reproducibility Initiative,2019,10.7554/eLife.41602,Helena Hartmann; Rachel Heyard,Life Sciences,Biomedical Science [Comment: Published by researchers based in Brazil],"Many Phenomena, Many studies","to estimate the level of reproducibility of biomedical science in Brazil, and to investigate what aspects of the published literature might help to predict whether a finding is reproducible.",FALSE,NA,Different data - same analysis,(1) Agreement in statistical significance [Comment: the statistical significance of the pooled replication studies] (2) Meta-analysis of original and replication study/studies [Comment: the significance of the effect in a meta-analysis including the original result and replication attempts] (3) Agreement in effect size [Comment: statistical comparison between the pooled effect sizes of the replications and the original result.],TRUE,the primary outcome of our analysis will be the percentage of original studies with effect sizes falling within the 95% prediction interval of a meta-analysis of the three replications.,3,FALSE,NA,TRUE,"(1) Reproducibility of the general conclusions on the existence of an effect (e.g. two results finding a statistically significant difference in the same direction) might not be accompanied by reproducibility of the effect size; conversely, studies with effect sizes that are similar to each other might have different outcomes in significance tests. Moreover, if non-replication occurs, it is hard to judge whether the original study or the replication is closer to the true result. [Comment: Significance and effect size, referencing 10.1177/0956797614567341](2) if inter-laboratory variability is high, prediction intervals can be wide, leading a large amount of results to be considered “reproducible”. Thus, replication estimates obtained by these methods are likely to be optimistic. On the other hand, failed replications will be more likely to reflect true biases, errors or deficiencies in the original experiments. [Comment: Primary outcome: referencing 10.1177/1745691616646366](3) incomplete reporting in the original study might increase inter-laboratory variation and artificially improve our primary outcome. [Comment: Primary outcome]",FALSE,NA
The REPRISE project: protocol for an evaluation of REProducibility and Replicability In Syntheses of Evidence,2021,10.1186/s13643-021-01670-0,Helena Hartmann; Rachel Heyard,Social Sciences and Humanities; Life Sciences,Health; Evidence synthesis; Methodological research [Comment: In medicine and psychology],"Many Phenomena, Many studies; Many Phenomena, One Study","aim to explore various aspects relating to the transparency reproducibility and replicability of several components of systematic reviews with meta-analysis of the effects of health social behavioural and educational interventions;evaluate the completeness of reporting and sharing of review data, analytic code and other materials",TRUE,"reproducibility = ""evaluating the extent of variation in results when we independently reproduce meta-analyses using the same computational steps and analytic code (if available) as used in the original review.""; replicability = ""we will adopt the non-procedural definitions of replication advocated by Nosek and Errington [5] and Machery [6]; that is, replicators will not need to follow every single step exactly as reported in the original systematic review, but they will be constrained by the original review question and must avoid making changes to the methods and concepts that might be reasonably judged to violate an attempt to answer that question.""",Different data - same analysis; Different data - different analysis; Same data - same analysis,"(1) Agreement in effect size [Comment: ""Results fully reproducible/replicable’ (i.e. no difference [with allowance for trivial discrepancies such as those due to computational algorithms] is observed between the original and recalculated meta-analytic effect estimate, its 95% confidence interval and inferences about heterogeneity reported in the original review). [...] We will calculate agreement between the original and recalculated meta-analytic effects, displayed using Bland-Altman plots, and tabulate discordance between P values for the meta-analytic effects, by categorising the P values based on commonly used levels of statistical significance, namely P < 0.01; 0.01 ≤ P < 0.05; 0.05 ≤ P < 0.1; P ≥ 0.1. [...] For the 30 systematic reviews replicated by one team each, we will assess agreement between the original and replicated review in the number of citations yielded from each database, in total and once duplicates were removed, by calculating the weighted Kappa statistic and percentage agreement""] (2) Subjective assessment [Comment: will also independently specify whether they believe the observed difference between the original and recalculated summary estimate and its precision was meaningful, that is, would lead to a change in the interpretation of the results.]",TRUE,indicators of transparency in the systematic reviews using descriptive statistics (e.g. frequency and percentage for categorical items and mean and standard deviation for continuous items); risk ratios with 95% confidence intervals to examine differences in percentages of each indicator; frequency and percentage for each response option,6,TRUE,(1) https://doi.org/10.1016/S0140-6736(86)90837-8 [Comment: Agreement in effect size (Bland-Altman plot)](2) https://doi.org/10.2307/2529310 [Comment: Agreement in effect size (Kappa statistic)],FALSE,NA,TRUE,"For the two reviews replicated by 15 teams, we will strive to select reviews with multiplicity of results in the included studies, diversity in the study characteristics, and diversity in the risk of bias in the included studies and extent of missing evidence (e.g. unpublished studies), and which match the content expertise of the majority of replicators."
Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty,2022,10.1073/pnas.2203150119,Joris Frese; Rachel Heyard,Social Sciences and Humanities,Political Science; Sociology,"One Phenomenon, Many Studies","""This study explores how researchers’ analytical choices affect the reliability of scientiﬁc ﬁndings"" (Abstract)",FALSE,NA,Same data - different analysis,(1) Agreement in statistical significance (2) Agreement in effect size,TRUE,Agreement in team's subjective conclusions,3,FALSE,NA,FALSE,NA,TRUE,"First, we do not know the generalizability of our study to different topics, disciplines, or even datasets."
Predicting the replicability of social and behavioural science claims from the COVID-19 Preprint Replication Project with structured expert and novice groups,2024,10.31222/osf.io/xdsjf,Helena Hartmann; Joris Frese,Social Sciences and Humanities; Life Sciences,Interdisciplinary; Social science; Behavioral science; Covid 19 research,"Many Phenomena, Many studies",eliciting predictions from experts or novices could accelerate credibility assessment and improve allocation of replication resources for important and uncertain findings. We elicited judgments from experts and novices on 100 claims from preprints about an emerging area of research (COVID-19 pandemic) using a new interactive structured elicitation protocol and we conducted 35 new replications,TRUE,"""The process of replications was conducted following the same methodology used in the rest of the SCORE program and resulted in two ‘types’ depending on the source of the new  data. [...]  New data replications were those that focused on collecting new data as part of the replication  attempt [...] Secondary data replications were those that focused on identifying another existing data source  that was not the same as the one used in the original study"" (p.4)",Different data - same analysis,(1) Agreement in statistical significance (2) Agreement in effect size,TRUE,Expert and novice prediction and accuracy,3,TRUE,10.31235/osf.io/46mnb,FALSE,NA,"True [Comment: DEI not discusses regarding replicability, but regarding the inclusion of less educated ""novices"" for the assessment of replicability.]","""if novices can match experts’ performance, the viable pool of assessors is much  larger, potentially enhancing efficiency and reducing the cost of eliciting predictions. In teaching  contexts, the findings suggest more class assignments, dissertations, and projects could be tailored  to offer not only educational opportunities for students but to also contribute to scientific  “credibility control,” with students becoming involved in research evaluation as part of their  training"" (p.14)"