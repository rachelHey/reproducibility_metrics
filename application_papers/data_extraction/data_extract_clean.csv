Title,Year,Doi,InvestigatorName,Field of research,Discipline,Type of project,Research question or aim,Did the authors define the type of reproducibility that is investigated?,Aspect of reproducibility investigated,"Even if defined by the authors, infer the aspect of reproducibility investigated",Did the authors measure reproducibility,Did they measure reproducibility using measures not in the previous list?,Paste description of each of the measures used,Did the paper refer to other papers for more information on the metric(s) used?,Paste the doi of all paper(s),Did the authors discuss limitations or assumptions of the metric(s) used?,Paste text on limitation/assumptions,"Did the authors discuss equity, diversity, and/or inclusion at any point?",Paste text on EDI dimension discussion
Law and Psychology Grows Up Goes Online and Replicates,2018,10.1111/jels.12180,Joris Frese,Social Sciences and Humanities,Psychology; Law,"Many Phenomena, Many studies","""Using three canonical law and psychology findings, we document the challenges and the feasibility of reproducing results across platforms."" (Abstract)",True,"""We evaluate the extent to which we are able to reproduce the original findings with contemporary subject pools (Amazon Mechanical Turk, other national online platforms, and in-person labs)."" (Abstract)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,True,"""we did ﬁnd, consistent with the literature previously reviewed, that MTurk participants were more attentive to study materials.68 By contrast, we found that participants from SSI were markedly inattentive, and the results from that platform were the most divergent from both in-person and other online samples. We note that it is understood that SSI has provided subjects for the inhouse subject pool at Qualtrics, the popular survey platform that also offers researchers a source of subjects. This result suggests that researchers might take care in using these fee-for-service subject recruitment platforms when subject motivation is particularly obscure. Given the attentiveness and reliability of our MTurk respondents (along with the similar Proliﬁc Academic sample), together with the now quite large body of literature from cognate ﬁelds on the replicability of treatment effects using MTurk, we would urge peer-review publications in our ﬁeld to put aside some of the latent skepticism that they may have held for that platform, at the very least in comparison to paid survey ﬁrms."" (p.344) [Comment: Compared attentiveness of respondents across replication samples.]",False,NA,False,NA,True,"""The gender split identiﬁed by PCI has balanced out in recent years.30 Respecting other demographic characteristics, results vary. One study, for example, concludes that workers are “wealthier, younger, more educated, less racially diverse, and more Demo- cratic than national samples” and less religiously afﬁliated.31 Others are more equivocal, concluding that apart from workers’ relative youth and liberalism, they closely approxi- mate a nationally representative sample."" (p.328) [Comment: Discussed gender- and racial composition of different replication samples.]"
Law and Psychology Grows Up Goes Online and Replicates,2018,10.1111/jels.12180,Samuel Pawel,Social Sciences and Humanities,law;psychology,"Many Phenomena, One Study [Comment: this could maybe also be coded as ""many phenomena, many studies"" because there were three different types of subjects (MTurk, SSI, lab), but since the authors described it as 3 replication studies and not 9, I decided to code as ""many phenomena, one study""]","""we would like to show, through replication of canonical legal experiments on a variety of survey platforms, the potential and the pitfalls of replication in law and psychology.""",False,NA,Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size [Comment: relatively informal comparison of effect estimates](3) Subjective assessment,False,NA,True,"10.1177/0956797614567341 [Comment: this was for the sample size determination, rather than the analysis]",False,NA,True,"""MTurk subjects are more diverse than college student samples, across various demographic variables."""
High replicability of newly discovered social-behavioural findings is achievable,2023,10.1038/s41562-023-01749-9,Joris Frese,Social Sciences and Humanities,Interdisciplinary,"Many Phenomena, Many studies","""In this Article, we report the results of a prospective replication study [...] examining whether low replicability and declining effects are inevitable when using proposed rigour-enhancing practices."" (pp. 1-2)",True,"""This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency."" (Abstract)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""Due to the limited number of participating labs, lab-level variation in the replicability of findings was incalculable; to the extent that labs vary in how they select potential replication targets, the replication rates observed in the present study may not generalize to a broader population of research groups"" (p.6)",True,"""The sample providers were instructed to provide American adults drawn in a stratified way with unequal probabilities of selection from the panels so that the people who completed each survey would resemble the nation’s adult population (according to the most recently available Current Population Survey, conducted by the US Census Bureau) in terms of gender, age, education, ethnicity (Hispanic versus not), race (allowing each respondent to select more than one race), region and income."" (p.7) [Comment: Discussed racial, gender, and income-based distribution of replication samples.]"
High replicability of newly discovered social-behavioural findings is achievable,2023,10.1038/s41562-023-01749-9,Rachel Heyard,Social Sciences and Humanities,"Interdisciplinary;Social-behavioural research [Comment: psychology, marketing, advertising, political science, communication, and judgement and decision-making]","Many Phenomena, Many studies","an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency.","False [Comment: No formal definition, but referring to replicability]",NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: One way of assessing replicability is to examine whether each replication rejects the null hypothesis at P < 0.05 in the expected direction](2) Agreement in effect size [Comment: An alternative index of replicability examines the consistency of ESs generated by the initial self-confirmatory test and its subsequent replications (within-study heterogeneity; τ̂within). On the basis of a multilevel meta-analysis, little variation in ESs was observed beyond what would be expected by sampling variation alone.]",False,NA,False,NA,False,NA,True,"All samples for the self-confirmatory tests and replications were drawn from online panels of American adults. These 16 findings do not characterize a representative sample of any methodology or discipline, although they do represent common methodologies, samples and research questions from the social-behavioural sciences;The sample providers were instructed to provide American adults drawn in a stratified way with unequal probabilities of selection from the panels so that the people who completed each survey would resemble the nation’s adult population (according to the most recently available Current Population Survey, conducted by the US Census Bureau) in terms of gender, age, education, ethnicity (Hispanic versus not), race (allowing each respondent to select more than one race), region and income."
Genotypic variability enhances the reproducibility of an ecological study,2018,10.1038/s41559-017-0434-x,Joris Frese,"STEM, e.g. Engineering, Mathematics, Physics",Biology; Ecology,"One Phenomenon, Many Studies","""A novel but controversial hypothesis postulates that stringent levels of environmental and biotic standardization in experimental studies reduce reproducibility by amplifying the impacts of laboratory-specific environmental factors not accounted for in study designs. A corollary to this hypothesis is that a deliberate introduction of controlled systematic  variability (CSV) in experimental designs may lead to increased reproducibility"" (Abstract)",True,"""To test the hypothesis that introducing CSV enhances reproducibility in an ecological context, we had 14 European laboratories  simultaneously run a simple microcosm experiment using grass  (Brachypodium distachyon L.) monocultures and grass and legume  (Medicago truncatula Gaertn.) mixtures. As part of the reproducibility experiment, the 14 laboratories independently tested the  hypothesis that the presence of the legume species M. truncatula in  mixtures would lead to higher total plant productivity in the microcosms and enhanced growth of the non-legume B. distachyon via  rhizobia-mediated nitrogen fertilization and/or nitrogen-sparing  effects"" (p.2)",Different data - same analysis,"(1) Agreement in statistical significance [Comment: ""Statistically significant differences among the 14 laboratories 
were considered an indication of irreproducibility."" (p.2)](2) Meta-analysis of original and replication study/studies",False,NA,False,NA,True,""" Arguably, our use of statistical significance tests of effect sizes  to determine reproducibility might be viewed as overly restrictive  and better suited to assessing the reproducibility of parameter estimates rather than the generality of the hypothesis under test27. We  used this approach because no generally accepted alternative framework is available to assess how close the multivariate results from  multiple laboratories need to be to conclude that they reproduced  the same finding"" (p.7)",False,NA
Genotypic variability enhances the reproducibility of an ecological study,2018,10.1038/s41559-017-0434-x,Rachel Heyard,Life Sciences,Ecology,"One Phenomenon, Many Studies",To test the hypothesis that introducing CSV enhances reproducibility in an ecological context,True,Reproducibility—the ability to duplicate a study and its findings.,Different data - same analysis [Comment: While they also introduced controlled systematic variability],"Agreement in statistical significance [Comment: these results suggest that the effect size or direction of the net legume effect was significantly different (that is, not reproducible) in some laboratories and that the introduced CSV treatment affected reproducibility.]",True,"To answer the question of how many laboratories produced results that were statistically indistinguishable from one another (that is, reproduced the same finding), we used Tukey’s post-hoc honest significant difference test for the laboratory effect.",False,NA,True,"Arguably, our use of statistical significance tests of effect sizes to determine reproducibility might be viewed as overly restrictive and better suited to assessing the reproducibility of parameter esti- mates rather than the generality of the hypothesis under test27. We used this approach because no generally accepted alternative framework is available to assess how close the multivariate results from multiple laboratories need to be to conclude that they reproduced the same finding.",True,"In a sense, they introduce controlled systematic variability and investigated its impact on reproducibility [Comment: referring to 10.1080/00031305.2016.1154108]"
Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy: survey of studies published in The BMJ and PLOS Medicine,2018,10.1136/bmj.k400,Joris Frese,Life Sciences,Biology; Medicine,"Many Phenomena, Many studies","""To explore the effectiveness of data sharing by  randomized controlled trials (RCTs) in journals with  a full data sharing policy and to describe potential  difficulties encountered in the process of performing  reanalyses of the primary outcomes."" (Abstract)",True,"""The primary outcome was data availability, defined  as the eventual receipt of complete data with clear  labelling. Primary outcomes were reanalyzed to assess  to what extent studies were reproduced."" (Abstract)",Same data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Subjective assessment,False,NA,False,NA,False,NA,False,NA
Data sharing and reanalysis of randomized controlled trials in leading biomedical journals with a full data sharing policy: survey of studies published in The BMJ and PLOS Medicine,2018,10.1136/bmj.k400,Rachel Heyard,Life Sciences,Medicine;Clinical trials,"Many Phenomena, One Study",To explore the effectiveness of data sharing by randomized controlled trials (RCTs) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes.,"False [Comment: No formal definition, but referring to reproducibility]",NA,Same data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Subjective assessment [Comment: clinical judgment],False,NA,False,NA,False,NA,False,NA
Eleven years of student replication projects provide evidence on the correlates of replicability in psychology,2023,10.1098/rsos.231240,Joris Frese,Social Sciences and Humanities;Life Sciences,Psychology,"Many Phenomena, Many studies","""our contribution is a new dataset of 176 replications of experimental studies from the social sciences, primarily psychology [...] We use this dataset to investigate the rate of replicability for such projects as well as the correlates of replication success."" (p.2)",True,"""We introduce a new dataset of 176 replications from students in a graduate-level methods course."" (Abstract)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Subjective assessment,False,NA,True,10.1111/rssa.12572,True,"""We do not interpret our non-replications as indicating the original results were false positives: presumably some were and some were not. There are many possible reasons for the non-replications in this sample. In some cases, the problem may be with the replication, such as too few participants, many exclusions for failed attention checks, or participants speeding through the study. When these issues are diagnosed, they can suggest possible ways to ‘rescue’ the replication by increasing the sample or changing the interface, without altering the underlying experiment; thus, while the replication did not succeed, after some troubleshooting, students may still be able to extend the work in the future. In other cases, there were a priori reasons to distrust the original study, such as exclusion criteria that seemed post hoc or high-order interaction terms with a small sample. That said, not all scientists recognize the same factors as potential indications of low power or questionable research practices; students conducting these replications generally expected them to succeed. In many—perhaps most—non-replications in our sample, it was unclear why the results failed to replicate. Our results are limited by the sample of studies we included, which are limited in number and may not be representative of the studies of interest to psychologists as a whole. Further, our predictor variables were not manipulated, so they cannot be interpreted as causing (non-)replication, but only as correlational markers. Some of the correlates are most easily interpreted as being about the original study, and others reflect the closeness of the replication to the original. For instance, while within-participants designs are more likely to replicate than between-participants designs, this predictor could also be related to the types of experiments that tend to be run in each design. Additionally, as [24] notes, within-participants designs can lead to practice effects, carry-over of treatments, and critically, sensitization effects, where participants reason about the contrast between conditions and respond differently due to that reasoning. Sensitization effects are a threat to internal validity, and could be replicable even in the absence of the effect itself being replicable. Given the strong relationship of within-participants designs to replicability, slightly more scepticism and critical reading of between-participants designs may be warranted, but this correlation, by itself, does not mean scientists should prefer within-participants designs."" (p.9)",False,NA
Eleven years of student replication projects provide evidence on the correlates of replicability in psychology,2023,10.1098/rsos.231240,Rachel Heyard,Social Sciences and Humanities,Psychology;Experimental psychology,"Many Phenomena, One Study",to investigate the rate of replicability for such projects as well as the correlates of replication success,True,"Replicability - section on ""What are we estimating when we measure replicability?""",Different data - same analysis,"(1) Subjective assessment [Comment: a subjective replication score (coded by two independent raters—one typically at the time of project completion—with discrepancies resolved by discussion)](2) Agreement in effect size [Comment: prediction interval, a binary measure of whether the replication statistic fell within the prediction interval of the original statistic]",True,"p-original, the p-value on the null hypothesis that the original and replication statistics are from the same distribution",True,10.1111/rssa.12572 [Comment: prediction interval and p-original],True,"Unlike statistical measures, subjective replication success accommodates studies with multiple important outcome measures that together define the pattern of interest. Further, this measure was applicable across the diverse range of statistical measures and reporting practices present in the sample. [Comment: statistical measures]",False,NA
A many-analysts approach to the relation between religiosity and well-being,2023,10.1080/2153599X.2022.2070255,Helena Hartmann,Social Sciences and Humanities,Religion;Wellbeing,"One Phenomenon, Many Studies",assess the robustness of the relation between relgiosity and wellbeing using a multi-analyst approach,True,"we believe that the more consistent the results from diﬀerent analysis teams are, the more conﬁdent we can be in the conclusions we draw from the results.",Same data - different analysis,Agreement in effect size,False,NA,False,NA,False,NA,False,NA
A many-analysts approach to the relation between religiosity and well-being,2023,10.1080/2153599X.2022.2070255,Joris Frese,Social Sciences and Humanities;Life Sciences,Psychology,"One Phenomenon, Many Studies","Analyzing ""The relation between religiosity and well-being"" (Abstract)",True,"""In the current project, we aim to shed light on the association between religion and well-being and the extent to which diﬀerent theoretically- or methodologically-motivated analytic choices aﬀect the results. To this end, we initiated a many-analysts project, in which several independent analysis teams analyze the same dataset in order to answer a speciﬁc research question"" (p.241)",Same data - different analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""Note that our request for beta coeﬃcients as eﬀect size metrics may have aﬀected the teams’ choice of statistical model and encouraged them to use regression models that generate beta coeﬃcients."" (p.247/248)",True,"""Although most teams indicated that eﬀort was (very) high, the majority also reported that frustration was (very) low and that they spent as much time as anticipated (see Figure 10). That is, in stage 1, 55% of the teams reported (very) high eﬀort, 17% were neutral, and 28% reported (very) low eﬀort. For stage 2, 48% of the teams reported (very) high eﬀort, 18% were neutral, and 34% reported (very) low eﬀort. In stage 1, 17% of the teams reported (very) high frustration, 23% were neutral, and 60% reported (very) low frustration. In stage 2, 18% of the teams reported (very) high frustration, 17% were neutral, and 65% reported (very) low frustration. The median time spent on the analyzes was 8 hours for both stages, although the range was quite wide: 1 to 80 hours for stage 1 and 30 minutes to 140 hours for stage 2. Most teams anticipated as much time as they spent: 51% for stage 1 and 52% for stage 2. In stage 1, 36% spent (much) more time than anticipated and 13% spent (much) less time. In stage 2, 33% spent (much) more time than anticipated and 15% spent (much) less time."" (p.255) [Comment: Maybe not EDI, but they had a section on the difficulties, frustrations, and time usage of the research teams regarding their replication tasks.]"
Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability,2020,10.1177/2515245920958687,Louise Townsin,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies","This design is particularly well suited for testing the strong hypothesis that many, if not most, failures to replicate are due to design errors that could have been caught by a domain expert (Gilbert et al., 2016). If this hypothesis is correct, then the new, peer-reviewed protocols would be expected to improve replicability and increase effect sizes to be closer to those of the original studies. This would not necessarily mean that all failures to replicate are due to poor design. After all, our sample of studies was chosen because they are among the most likely published replications to have faulty designs. However, such an outcome would suggest that published replicability rates are overly pessimistic.",True,"A replication study is an attempt to reproduce a previously observed finding with no a priori expectation for a different outcome (see Nosek & Errington, 2017, 2020; Zwaan, Etz, Lucas, & Donnellan, 2018).","Different data - different analysis [Comment: We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3–9; median total sample = 1,279.5, range = 276–3,512) for high-powered tests of each original finding with both protocols.]","Agreement in effect size [Comment: Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (Δr = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00–.15) were 78% smaller, on average, than the original effect sizes (median r = .37, range = .19–.50).]",True,"In exploratory analyses, we considered several other measures of replicability that directly assessed (a) statistical consistency between the replications and the original studies and (b) the strength of evidence provided by the replications for the scientific effects under investigation (Mathur & VanderWeele, 2020). These analyses also accounted for potential heterogeneity in the replications and for the sample sizes in both the replications and the original studies. Accounting for these sources of variability avoids potentially misleading conclusions regarding replication success that can arise from metrics that do not account for these sources of variability, such as agreement in statistical significance.","True [Comment: Mathur & VanderWeele, 2020]","doi:10.1111/rssa.12572 [Comment: an original study can be considered statistically consistent with a set of replications if the original study and the replications came from the same distribution of potentially heterogeneous effects—that is, if the original study was not an anomaly (Mathur & VanderWeele, 2020). We assessed statistical consistency using the met- ric Porig. Analogous to a p value for the null hypothesis of consistency, this metric characterizes the probability that the original study would have obtained a point estimate at least as extreme as was observed if in fact the original study was consistent with the replications.]",True,"the studies were not selected to be representative of any particular population. The extent to which our findings will generalize is unknown. It is possible that our findings are unique to this sample of studies, or to psychology studies that are conducted in good faith but fail to be endorsed by original authors, as in the RP:P (Open Science Collaboration, 2015). A more expansive possibility is that the findings will be generalizable to occasions in which original authors or other experts dismiss a failed replication for having design flaws that are then addressed and tested again.",False,NA
Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability,2020,10.1177/2515245920958687,Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies",The purpose of this investigation was to test whether protocols resulting from formal peer review would produce stronger evidence for replicability than protocols that had not received formal peer review,"False [Comment: No formal definition, but refer to replicability]",NA,"Different data - same analysis [Comment: Same analysis, but for small deviations]","(1) Meta-analysis of original and replication study/studies [Comment: We conducted a multilevel random- effects meta-analysis of the 101 effect sizes, with a random intercept of data-collection site (varying from 3 to 9 depending on study) nested within study (10 studies).](2) Meta-analysis of original and replication study/studies [Comment: we conducted a random-effects meta-analysis on the estimates of the effect of protocol within each replication study.]",True,The strength of evidence provided by the replications for the scientific effects under investigation;The statistical consistency between the replications and the original studies,True,10.1111/rssa.12572 [Comment: statistical consistency],False,NA,False,NA
Variability in the analysis of a single neuroimaging dataset by many teams,2020,10.1038/s41586-020-2314-9,Helena Hartmann,Life Sciences,Neuroscience;Neuroimaging,"One Phenomenon, Many Studies",assess the effect of this flexibility on the results of functional magnetic resonance imaging,False,NA,Same data - different analysis,(1) Agreement in statistical significance(2) Subjective assessment [Comment: confirmation of hypothesis or not],True,Prediction market,False,NA,True,"For prediction markets: note that because some analyses were performed on the final market prices (that is, the predictions of the markets), for which there is one value per hypothesis per market, the number of observations for each of the markets was low (n = 9), leading to limited statistical power. Therefore, the results should be interpreted with caution.",False,NA
Variability in the analysis of a single neuroimaging dataset by many teams,2020,10.1038/s41586-020-2314-9,Rachel Heyard,Life Sciences,Neuroscience,"Many Phenomena, Many studies [Comment: 9 hypotheses were tested by 70 teams on the same data]",to assess the real-world variability of results across independent teams analysing the same dataset.,"False [Comment: No, but they were interested in analytical variability]",NA,Same data - same analysis;Same data - different analysis,Agreement in statistical significance [Comment: Rates of reported significant findings across hypotheses],True [Comment: Analysis of the pipeline used by the teams; Prediction markets],Analysis of the pipeline used by the teams;Prediction markets,True,10.1073/pnas.1516179112 [Comment: Prediction markets],False,NA,False,NA
The influence of hidden researcher decisions in applied microeconomics,2021,10.1111/ecin.12992,Joris Frese,Social Sciences and Humanities,Economics; Applied Microeconomics,"Many Phenomena, Many studies [Comment: 2 phenomena, 7 replications each]","""Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions."" (Abstract)",True,"""replicators are told to use any statistics package, and that they should use assistants if they would normally use assistants in their work. They are also told that their analysis should be independent, and should not attempt to identify the original study, or to match (or mismatch) with fellow replicators. The goal is to uncover “how you would estimate this effect, if you'd had this question, this idea for identification, and had chosen this particular sample.” (p.948)","Same data - same analysis;Same data - different analysis [Comment: replicators were mostly free to choose their approach, using the same data]",(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""It is not surprising that different researchers would carry out an analysis in different ways. Replicators were asked after completing their replication about their reasoning for the analytic and data cleaning choices that were not covered by the instructions and differed among replicators. The most common reasons included familiarity with a given model, differing intuitive or technical ideas about which control variables are appropriate or whether linear probability models are appropriate, and differing preferences for parsimony. There is nothing inherently wrong about these choices or reasons, although the fact that researchers do not seem to agree on these issues implies additional sources of uncertainty in estimates. These differences only rise to a real cause for concern when they are about things that either would be unlikely to be reported in the resulting study, or would be reported but paid little attention by reviewers and readers. If invisible researcher choices are different and consequential, that means that empirical results in applied microeconomics reflect variation in sample and methods, as expected, but also reflect variation in researcher choice."" (p.958)",True,"""Among successful replicators, the mean and median year in which they received their PhD was 2011 and 2014, respectively. As of October 2020, the mean and median Google Scholar “cited by” count among successful replicators were 366 and 128, respectively, with a minimum of 38 and a maximum of 1291, omitting one replicator who did not have a Google Scholar profile."" (p.950) [Comment: Looked at seniority and professional success of replicators]"
The influence of hidden researcher decisions in applied microeconomics,2021,10.1111/ecin.12992,Rachel Heyard,Social Sciences and Humanities,Microeconomics,"One Phenomenon, Many Studies [Comment: While two phenomena were replicated.]",To measure the magnitude of variation due to researcher degrees of freedom in the context of applied microeconomics studies that attempt to isolate a causal effect,False,NA,Same data - same analysis;Same data - different analysis,(1) Agreement in statistical significance(2) Agreement in effect size,True,They additionally coded difference analysis step/decision in a binary manner;Comparison of sampling variation,False,NA,False,NA,False,NA
Investigating the replicability of preclinical cancer biology,2021,10.7554/eLife.71601,Joris Frese,"STEM, e.g. Engineering, Mathematics, Physics",Biology,"Many Phenomena, Many studies","""provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from highimpact papers."" (abstract)",True,"""In the Reproducibility Project: Cancer Biology, we sought to acquire evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from 53 high-impact papers published in 2010, 2011, and 2012"" (p.2)",Different data - same analysis,(1) Agreement in effect size(2) Meta-analysis of original and replication study/studies,False,NA,False,NA,True,"""There is no single method for assessing the success or failure of replication attempts (Mathur and VanderWeele, 2019; Open Science Collaboration, 2015; Valentine et al., 2011), so we used seven different methods to compare the effect reported in the original paper and the effect observed in the replication attempt (see Results). Six of these methods were dichotomous (i.e., replication success/failure) and one was not."" (p.3)",False,NA
Investigating the replicability of preclinical cancer biology,2021,10.7554/eLife.71601,Rachel Heyard,Life Sciences,Preclinical cancer biology,"Many Phenomena, One Study",To investigate the replicability of preclinical cancer biology,"False [Comment: No formal definition, but refer to replicability]",NA,Different data - same analysis,"(1) Agreement in effect size [Comment: Same direction: We evaluated whether the sign of the replication estimate agreed with that of the original effect](2) Agreement in statistical significance [Comment: Significance agreement: We assessed whether the replication had a p-value less than 0.05 and an estimate in the same direction as the original.](3) Agreement in effect size [Comment: We assessed whether the original and replication estimates were inside the 95% confidence intervals of the other.](4) Agreement in effect size [Comment: We assessed whether the replication estimate was inside the 95% prediction interval of the original, and formally assessed the degree of statistical inconsistency between the replication and the original using the metric porig, which can be viewed as a p-value for the hypothesis that the original and the replication had the same population effect size.](5) Agreement in effect size [Comment: We estimated the difference in estimates between the replication and the original after transforming all effect sizes to a comparable scale.](6) Meta-analysis of original and replication study/studies [Comment: we calculated a pooled estimate from combining the replication and original via fixed-effects meta-analysis]",False,NA,True,10.1111/rssa.12572 [Comment: porig],True,"(1) This rudimentary metric does not account for effect sizes nor statistical precision, but was useful because we could compute it for even the non-quantitative pairs such as when original experiments reported only representative images. [Comment: Same direction](2) We chose a fixed-effect model rather than a random-effects model primarily for consistency with our assumption throughout all main analyses that there was no within-pair heterogeneity. [Comment: Meta-analysis]",False,NA
How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project,2019,10.1177/0956797619831612,Joris Frese,Social Sciences and Humanities;Life Sciences,Psychology; Behavioral Science,"Many Phenomena, Many studies","""Do personality characteristics reliably predict consequential life outcomes?"" (p.711)",True,"""I conducted preregistered, high-powered (median N = 1,504) replications of 78 previously published trait–outcome associations."" (Abstract)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""Strengths, limitations, and future directions The LOOPR Project had a number of important strengths, including its broad sample of life outcomes, representative samples, preregistered design, and high statistical power. However, it also had some noteworthy limitations that suggest promising directions for future research. Most notably, all of the present data come from cross-sectional, self-report surveys completed by online research panels, whereas some of the original studies used longitudinal designs or other data sources (e.g., interviews, informant reports, community samples). Indeed, our analyses of replicability predictors indicated that replication effect sizes tended to be somewhat stronger when the original study had also used a self-report survey to measure the target outcome. Thus, the present research is only a first step toward establishing the replicability of these traitoutcome associations, and future research using longitudinal designs, as well as alternative sampling and assessment methods, is clearly needed. A broader issue is that large-scale replication projects can be conducted using different approaches (McShane, Tackett, Bockenholt, & Gelman, 2017). Any particular approach will have advantages and disadvantages, and the choice of an optimal approach will depend on the goals of a particular project. The main goal of the LOOPR Project was to estimate the overall replicability of the personality-outcome literature. We therefore adopted an approach that attempted to replicate a large number of original effects from many studies, with one replication attempt per effect and relatively brief outcome measures (Camerer et al., 2016; Cova et al., 2018; Open Science Collaboration, 2015). An alternative approach would be to replicate a smaller number of effects with lengthier measures or multiple replication attempts per effect (i.e., a many-labs approach; Ebersole et al., 2016; Hagger et al., 2016; Klein et al., 2014). Such an approach would be less well suited for estimating the overall replicability of a literature but better suited for achieving other goals. For example, future research can complement the LOOPR Project by testing individual trait–outcome associations more robustly and by directly investigating factors—such as location, sampling method, mode of administration, measures, and analytic methodthat might moderate these associations."" (p.725)",False,NA
How Replicable Are Links Between Personality Traits and Consequential Life Outcomes? The Life Outcomes of Personality Replication Project,2019,10.1177/0956797619831612,Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, One Study",to estimate the replicability of the personality-outcome literature.,True,the replicability of behavioral science—the likelihood that independent researchers conducting similar studies will obtain similar results. [Comment: Replicability],Different data - same analysis,(1) Agreement in statistical significance [Comment: defined simply as the proportion of replication attempts that yielded statistically significant results in the hypothesized direction.](2) Agreement in effect size [Comment: we examined the frequency with which the replication attempts obtained a trait–outcome association weaker than the corresponding original effect or not in the expected direction.],False,NA,False,NA,False,NA,False,NA
Same data different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis,2021,10.1016/j.obhdp.2021.02.003,Joris Frese,Social Sciences and Humanities,Interdisciplinary,"Many Phenomena, Many studies","""test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings"" (Abstract)",True,"""independent analysts used the same dataset to test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings. Not only the analytic approach but also the operationalizations of key variables were left unconstrained and up to individual analysts."" (Abstract)",Same data - same analysis;Same data - different analysis,Agreement in statistical significance,True,"""Boba multiverse analysis: To complement the qualitative analyses based on DataExplained, we also examined underlying processes quantitatively, through a Boba multiverse analysis (Liu et al., 2020). This crossed all of the crowd of analysts’ choices with one another, removing analytic choices that did not make sense in conjunction with one another"" (p.241)",True,10.1109/TVCG.2020.3028985,True,"Too much to paste, but section 5.2 (starting on p.244) includes a detailed discussion of various limitations.",False,NA
Same data different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis,2021,10.1016/j.obhdp.2021.02.003,Rachel Heyard,"Social Sciences and Humanities;STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary;Data science,"One Phenomenon, Many Studies [Comment: same dataset to test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings]",Our project examined whether independent analysts would arrive at similar analyses and statistical results using the same dataset to address these questions.,"False [Comment: No formal definition, but refer to variability]",NA,Same data - different analysis,"(1) Agreement in statistical significance [Comment: summarizes the number of analysts who obtained statistically significant support for the hypothesis, directional but non-significant support, directional results contrary to the hypothesis, and statistically significant results contrary to the initial prediction.](2) Agreement in effect size [Comment: by computing the z-score for each statistical result’s p-value]",True,to identify the extent of heterogeneity in researchers’ choices of analytic methods;Boba multiverse analysis,True,10.1109/TVCG.2020.3028985 [Comment: Boba multiverse analysis],False,NA,False,NA
Interaction effects in econometrics,2013,10.1007/s00181-012-0604-2,Helena Hartmann,Social Sciences and Humanities,Econometrics,"Many Phenomena, One Study",provide practical advice for applied economists regarding robust specification and interpretation of linear regression models with interaction terms;replicate a number of prominently published results using interaction effects and examine if they are robust to reasonable speciﬁcation permutations,False,NA,Same data - different analysis ;Different data - different analysis [Comment: at least that what it sounds like they are doing],Agreement in statistical significance [Comment: size and significance of the interactions],False,NA,False,NA,False,NA,False,NA
Interaction effects in econometrics,2013,10.1007/s00181-012-0604-2,Rachel Heyard,Social Sciences and Humanities,Econometrics,"Many Phenomena, One Study",We replicate a number of prominently published results using interaction effects and examine if they are robust to reasonable speciﬁcation permutations.,"False [Comment: No formal definition, but interested in robustness]",NA,"Same data - same analysis;Same data - different analysis [Comment: The same data was used, either directly from the source of the original paper, or they followed the steps/source outlined in the paper.]",None of the above,True,Robustness analysis robustness analysis with respect to the functional form should be standard when one uses non-linear speciﬁcations,False,NA,False,NA,False,NA
Multiple Perspectives on Inference for Two Simple Statistical Scenarios,2019,10.1080/00031305.2019.1565553,Joris Frese,Social Sciences and Humanities;Life Sciences,Interdisciplinary,"Many Phenomena, Many studies [Comment: 2 phenomena, 4 studies of each]","""When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data?"" (Abstract)",True,"""we selected from the literature two simple scenarios—involving a comparison of two proportions and a Pearson correlation—and asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome"" (Abstract)",Same data - different analysis,Subjective assessment [Comment: All findings were deemed inconclusive],False,NA,False,NA,False,NA,False,NA
Multiple Perspectives on Inference for Two Simple Statistical Scenarios,2019,10.1080/00031305.2019.1565553,Rachel Heyard,"STEM, e.g. Engineering, Mathematics, Physics",Statistics,"Many Phenomena, Many studies","In addition to providing an empirical answer to the question “does it matter?”, we hope to highlight how the same dataset can give rise to rather different statistical treatments.",False [Comment: No definition - also they don't name the type of reproducibility],NA,Same data - different analysis,None of the above,True,"Comparison of the conclusions, qualitatively similar",False,NA,False,NA,False,NA
Systematic assessment of the replicability and generalizability of preclinical findings: Impact of protocol harmonization across laboratory sites,2022,10.1371/journal.pbio.3001886,Helena Hartmann,Life Sciences,Preclinical animal research,"One Phenomenon, Many Studies",assess the influence of protocol standardization between laboratories on their replicability of preclinical results;aimed to elucidate parameters that impact the replicability of preclinical animal studies.;determine whether harmonization of study protocols across laboratories improves the replicability of the results and whether replicability can be further improved by systematic variation (heterogenization) of 2 environmental factors (time of testing and light intensity during testing) within laboratories,True,ability to duplicate results from a previous scientific claim supported by new data,Different data - same analysis;Different data - different analysis [Comment: three levels of protocol harmonization],(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,False,NA,True,we could not confirm that diversifying the environmental conditions further reduces the variability across laboratories. The current selection of “heterogenizing” factors was rather limited by the feasibility to diversify them across all labs.;It also shows that it is possible to diversify the study sample by incorporating blocking factors like sex or introducing systematic heterogenization of conditions without the need to increase the overall sample size.
Systematic assessment of the replicability and generalizability of preclinical findings: Impact of protocol harmonization across laboratory sites,2022,10.1371/journal.pbio.3001886,Rachel Heyard,Life Sciences,Preclinical research,"One Phenomenon, Many Studies",to determine whether harmonization of study protocols across laboratories improves the replicability of the results and whether replicability can be further improved by systematic variation (heterogenization) of 2 environmental factors (time of testing and light intensity during testing) within laboratories,True,Defining results replicability as the ability to duplicate results from a previous scientific claim supported by new data [Comment: Replicability],Different data - same analysis,None of the above,True,Reduction in variability of results,False,NA,False,NA,False,NA
Many Analysts One Data Set: Making Transparent How Variations in Analytic Choices Affect Results,2018,10.1177/2515245917747646,Joris Frese,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies","""Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players."" (Abstract)",True,"""Twenty-nine teams involving 61 analysts used the same data set to address the same research question"" (Abstract)",Same data - same analysis;Same data - different analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,""" number of significant limitations of the dataset were discussed during the project, and are worth further elaborating on. Given the correlational nature of the available field data, the present research cannot identify causal relationships between variables. Most teams observed a significant relationship between player skin tone and referee red card decisions, but this correlation could be driven by referee biases, player behavior (e.g., due to national differences in playing styles), or unmeasured third variables. Another major limitation is that data on explicit and implicit skin tone preferences (the focus of research questions 2a and 2b) were only available for referees’ country of origin, not for the individual referees themselves. Referees may or may not have skin tone preferences similar to those of the average person in their home country. This could be one reason why our analysis teams converged on the conclusion that skin tone preferences did not predict referee decisions, and that the dataset was not adequate to answer the question effectively (see S7). Another explanation, of course, is that neither explicit nor implicit attitudes exhibit significant predictive validity in this particular field context. To address these issues, it will be productive to directly measure the social attitudes of sports officials and examine whether these predict their judgments of players. More generally, to investigate the research questions more effectively, access to more detailed and fine-grained data would be ideal. The amount of time a player was on the pitch during the game, details of all other players playing that same match, whether the game was an international game or league game and if the latter in which league the game was played, as well as the importance of the particular game were all mentioned by analysts as information they would have liked to have included but that was not available."" (Supplement 9)",True,"""An important question is whether the variability in the analytic choices made and results found by the teams resulted from teams with the greatest statistical expertise making different choices than the other teams. A related question is whether teams whose members had more quantitative expertise showed greater convergence in their estimated effect sizes. To answer these questions, we dichotomized the teams into two groups using latent class analysis. The first group (n = 9) was more likely to have a team member who had a Ph.D. (100% vs. 53%), was a professor at a university (100% vs. 37%), had taught a graduate statistics course more than twice (100% vs. 0%), and had at least one methodological or statistical publication (78% vs. 47%)."" (p.350) [Comment: Not really EDI, but discussed seniority and previous publication success of replicators.]"
Many Analysts One Data Set: Making Transparent How Variations in Analytic Choices Affect Results,2018,10.1177/2515245917747646,Rachel Heyard,Social Sciences and Humanities,Psychology;Statistics,"One Phenomenon, Many Studies","In this article, we report an investigation that addressed the current lack of knowledge about how much diversity in analytic choice there can be when different researchers analyze the same data and whether such diversity results in different conclusions.","False [Comment: No formal definition, and they do not name any type of reproducibility]",NA,Same data - different analysis,"(1) Agreement in statistical significance [Comment: Twenty teams (69%) found a significant positive relationship, p < .05, and nine teams (31%) found a nonsignificant relationship. No team reported a significant negative relationship.](2) Agreement in effect size",False,NA,False,NA,False,NA,True,"A demographic survey revealed that the team leaders worked in 13 different countries and came from a variety of disciplinary backgrounds, including psychology, statistics, research methods, economics, sociology, linguistics, and management. At the time that the first draft of this manuscript was written, 38 of the 61 data analysts (62%) held a Ph.D. (62%), and 17 (28%) had a master’s degree. The analysts came from various ranks and included 8 full professors (13%), 9 associate professors (15%), 13 assistant professors (21%), 8 postdocs (13%), and 17 doctoral students (28%). In addition, 27 participants (44%) had taught at least one undergraduate statistics course, 22 (36%) had taught at least one graduate statistics course, and 24 (39%) had published at least one methodological or statistical article.;a crowdsourced approach adds value in a number of ways. A globally distributed crowdsourced project will leverage skills, perspectives, and approaches to data analysis that no single analyst or research team can realistically muster alone."
Comparison of two independent systematic reviews of trials of recombinant human bone morphogenetic protein-2 (rhBMP-2): the Yale Open Data Access Medtronic Project,2017,10.1186/s13643-017-0422-x,Louise Townsin,Life Sciences,Health,"One Phenomenon, Many Studies","We retrospectively compared the research methods and results of the final comprehensive publications of two meta-analyses performed in the context of full systematic reviews of recombinant human bone morphogenetic protein-2 (rhBMP-2) prepared by two independent centers, Center A [26, 27] from the University of York and Center B [28, 29] from Oregon Health & Science University, and focused on (1) meta-analysis trial inclusion criteria; (2) statistical methods; (3) summary risk estimates; and (4) conclusions.",True,"The premise of replication efforts is that different groups, employing rigorous methods, may take different approaches and come to different conclusions on a previously addressed question",Same data - different analysis,"(1) Agreement in statistical significance [Comment: (1) meta-analysis trial inclusion criteria; (2) statistical methods; (3) summary risk estimates; and (4) conclusions.](2) Meta-analysis of original and replication study/studies(3) Subjective assessment [Comment: In our study of two independent centers provided with identical objectives, data, resources, and time to conduct concurrent meta-analyses, we found that the centers did not report identical methods, results, and interpretations. In addition, the potential benefit of additional analyses of the same data was not limited solely to increasing confidence through replication. Separate analyses revealed nuances of differences, with potential interpretations for clinical management, which could be produced from the same data set using valid methods. These findings, even though largely similar, support the case for greater sharing and access to clinical data as a way to maximize public dialogue about the meaning of the data and to ensure that a single interpretation does not lead people to believe there is no other possible approach.]",True,"(1) meta-analysis trial inclusion criteria; (2) statistical methods; (3) summary risk estimates; and (4) conclusions. [Comment: Trial inclusion criteria were defined as study characteristics necessary for inclusion in meta-analysis. We explicitly compared, for primary and secondary endpoint meta-analyses, as well as safety analyses, the trials used by both centers for each analysis. For methods, we com- pared centers’ reported outcomes at various time points as well as statistical methods. We compared the centers’ risk estimates for all primary outcomes for efficacy as well as safety at all time points. In consideration of these factors, we provide a subjective comparison of the overall conclusions drawn by each center.Trial inclusion was largely similar with a primary difference of IPD obtained from a single published RCT. Both centers chose only to include RCTs of rhBMP-2 in spinal fusion in their meta-analysis, and both groups analyzed 11 of the RCTs.Research methodology differed primarily in the choice of stratification, with minor differences in the choice of statistical methods. Both centers studied the same primary outcomes for effectiveness and reported them at the same time points of 6 weeks and 3, 6, 12, and 24 months after surgery. Similar outcomes were also reported between the centers for harms up to 4 weeks and then up to 24 months for general adverse events, and up to 48 months for cancer and death. Neither group found evidence of an rhBMP-2 doseresponse relationship or heterogeneity in groups that received high-dose forms of rhBMP-2, so all dose formulations were combined. For harms, Center A chose to combine all trials using a generalized mixed effects model since specific adverse events were few at the trial level. Center B also used a generalized mixed effects model with stratification by surgical approach, except for cancer and death. The groups obtained similar results in summary estimates of most clinical outcomes and adverse events, although there were notable differences. Center A interpreted benefits to fusion and postoperative pain as “clinically insignificant” and increased cancer incidence as “inconclusive,” noting that “whether this increased risk is genuine is uncertain” (Table 3). Overall, by this ana- lysis alone, rhBMP-2 seemed to offer improved rates of fusion with similar clinical outcomes compared with standard techniques at the expense of increased reports of back and leg pain in the early postoperative period. In contrast to Center A’s report, Center B found “moder- ate-strength evidence of no consistent differences between rhBMP-2 and ICBG in…fusion rates.” In addition, it re- ported a statistically significant increase in cancer at the 24-month time point, while noting that “This finding should be interpreted with caution because cases were heterogeneous.” Overall conclusions from Center B seemed to indicate more strongly than those of Center A that rhBMP-2 had no additional clinical benefit.]",False,NA,"False [Comment: Center A interpreted benefits to fusion and postoperative pain as “clinically insignificant” and increased cancer incidence as “inconclusive,” noting that “whether this increased risk is genuine is uncertain” (Table 3). Overall, by this ana- lysis alone, rhBMP-2 seemed to offer improved rates of fusion with similar clinical outcomes compared with standard techniques at the expense of increased reports of back and leg pain in the early postoperative period. In contrast to Center A’s report, Center B found “moder- ate-strength evidence of no consistent differences between rhBMP-2 and ICBG in…fusion rates.” In addition, it re- ported a statistically significant increase in cancer at the 24-month time point, while noting that “This finding should be interpreted with caution because cases were heterogeneous.” Overall conclusions from Center B seemed to indicate more strongly than those of Center A that rhBMP-2 had no additional clinical benefit]",NA,False,NA
Comparison of two independent systematic reviews of trials of recombinant human bone morphogenetic protein-2 (rhBMP-2): the Yale Open Data Access Medtronic Project,2017,10.1186/s13643-017-0422-x,Rachel Heyard,Life Sciences,Clinical trials;Evidence synthesis,"One Phenomenon, Many Studies","to determine if two independent centers, each of which were contracted to pursue identical research questions concurrently, with access to identical IPD, would employ identical methods in the areas of data use and statistical analysis and report identical, or at least consistent, results and conclusions.",True,"Previous studies have sought to determine whether systematic reviews are replicable, with new teams performing new searches, summaries, and analyses of the literature for a particular question.",Different data - same analysis;Different data - different analysis [Comment: Select new data to answer the same research question - the analysis might be slightly different.],"(1) Agreement in statistical significance [Comment: Center A found a statistically significant increase in fusion rate at 24 months [...] Center B, reporting results for each surgical approach separately, did not find a significant increase in fusion at 24 months.](2) Subjective assessment [Comment: In consideration of these factors, we provide a subjective comparison of the overall conclusions drawn by each center.]",True,"Trial inclusion criteria were defined as study characteristics necessary for inclusion in meta-analysis. We explicitly compared, for primary and secondary endpoint meta-analyses, as well as safety analyses, the trials used by both centers for each analysis.;For methods, we com- pared centers’ reported outcomes at various time points as well as statistical methods.;We compared the centers’ risk estimates for all primary outcomes for efficacy as well as safety at all time points.",False,NA,False,NA,False,NA
Replicability of simulation studies for the investigation of statistical methods: the RepliSims project,2024,10.1098/rsos.231003,Joris Frese,"Social Sciences and Humanities;Life Sciences;STEM, e.g. Engineering, Mathematics, Physics",Interdisciplinary,"Many Phenomena, Many studies","""The current study aims at the following: — discussing the definitions of reproducibility and replicability in the context of simulation studies; — illustrating that replicability of simulation studies is not a given, using the replication of eight simulation studies as an example; — describing features that hinder and facilitate replicability of the original studies; and — providing preliminary recommendations for future simulation studies to facilitate replicability, in addition to available guidance for reporting of simulation studies."" (p.2)",True,"""Eight highly cited statistical simulation studies were selected, and their replicability was assessed by teams of replicators with formal training in quantitative methodology. The teams used information in the original publications to write simulation code with the aim of replicating the results. The primary outcome was to determine the feasibility of replicability based on reported information in the original publications and supplementary materials."" (Abstract)",Other [Add as comment] [Comment: Trying to recreate simulated data based on original code.],Subjective assessment,False,NA,False,NA,False,NA,False,NA
Replicability of simulation studies for the investigation of statistical methods: the RepliSims project,2024,10.1098/rsos.231003,Rachel Heyard,"STEM, e.g. Engineering, Mathematics, Physics",Simulation study;Statistics;Empirical research,"Many Phenomena, One Study","The current study aims at the following: — discussing the definitions of reproducibility and replicability in the context of simulation studies; — illustrating that replicability of simulation studies is not a given, using the replication of eight simulation studies as an example; — describing features that hinder and facilitate replicability of the original studies; and — providing preliminary recommendations for future simulation studies to facilitate replicability, in addition to available guidance for reporting of simulation studies.",True,"Replication: writing new code to generate and analyse new data, following procedures in the original study as closely as possible [Comment: Replicability]",Different data - same analysis,"Subjective assessment [Comment: assessed in a qualitative manner and involved evaluating: whether numerical values from the replication studies were comparable to those in the original studies, whether trends in the results were moving in the same direction, and whether the performance rankings of different simulation scenarios matched those in the original studies.]",False,NA,False,NA,False,NA,True,"tacit knowledge about a particular field or method could have enhanced replicability. For instance, the simulations by Fritz & MacKinnon [36] and MacKinnon et al. [37] were replicated by researchers who specialize in mediation analysis, and these were two of the more replicable studies."
Reproducibility via coordinated standardization: a multi-center study in a Shank2 genetic rat model for Autism Spectrum Disorders,2019,10.1038/s41598-019-47981-0,Helena Hartmann,Life Sciences,Neuroscience;Preclinical studies;Biomedicine,"One Phenomenon, Many Studies",whether reproducibility increases through harmonization of apparatus test protocol and aligned and non-aligned environmental variables,False,NA,Different data - different analysis,Agreement in statistical significance,False,NA,False,NA,False,NA,False,NA
Reproducibility via coordinated standardization: a multi-center study in a Shank2 genetic rat model for Autism Spectrum Disorders,2019,10.1038/s41598-019-47981-0,Rachel Heyard,Life Sciences,Preclinical research,"One Phenomenon, Many Studies",to investigate whether these previously reported results could be reproduced and replicated across three study sites by following the same experimental protocol for behavioral evaluation with automated video scoring analysis and drug testing.,"False [Comment: No definition, but refer to reproducibility]",NA,"Different data - same analysis [Comment: Although the aim of this study was to explore the reproducibility of the results, it was not intended to fully reproduce the original methodology; standardized phenotyping equipment was used, and small changes were made to the protocol for this study.]",None of the above,True,"Three-way ANOVA [Comment: A three-way ANOVA with genotype (two levels) and site (three levels) as between-subject factors, and treatment as the repeated within-subject factor [four levels (vehicle, 3 doses)] was performed on absolute data values for each of the readouts. In the case of a main site and genotype effect in this absolute data set, normalized values (relative to vehicle treatment) were analyzed using the same three-way ANOVA design. This analysis aimed to address reproducibility across sites in terms of the phenotype evaluation as well as the pharmacological intervention.]",False,NA,False,NA,False,NA
Quantitative Macroeconomics: Lessons Learned from Fourteen Replications,2023,10.1007/s10614-022-10234-w,Rachel Heyard,Social Sciences and Humanities,Quantitative Macroeconomics;Economics,"Many Phenomena, One Study",The focus of this paper is instead on the lessons to be learned from these replications and on providing some suggestions for best practice based on the experience of performing the replications.,True,"Replication necessarily involves writing new code as simply running existing codes includes replicating all the errors made in the original when treating the data and writing the code. [Comment: Replicability, also refers to it as quasi-replications, saying ""I do not attempt to use the same numerical methods to solve the model as the original authors, and I (only) replicate all ﬁgures and tables relating to the model""]",Different data - same analysis,None of the above,True,"quartiles of the percentage difference between the replication and original results [Comment: It is based on all the entries of all the Tables from each paper: the absolute percentage difference between the replication value and the value in the original paper was calculated for every table entry, and the quartiles of these are reported.]",False,NA,True,The main weakness of this is that it obviously misses any Figures. [Comment: quartiles of the percentage difference],False,NA
Quantitative Macroeconomics: Lessons Learned from Fourteen Replications,2023,10.1007/s10614-022-10234-w,Samuel Pawel,"Social Sciences and Humanities;STEM, e.g. Engineering, Mathematics, Physics",Quantitative Macroeconomics,"Many Phenomena, One Study","""replicate all tables and figures from fourteen papers in Quantitative Macroeconomics""",False,NA,"Other [Add as comment] [Comment: the author replicates computational studies, hard to say whether these replications are ""new data"" and ""new analyses"" or not]",None of the above,True,Percentage Difference between Numbers in Replication and Original Paper,False,NA,False,NA,False,NA
Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015,2018,10.1038/s41562-018-0399-z,Helena Hartmann,Social Sciences and Humanities,social science,"Many Phenomena, Many studies",Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015,True,"find a significant effect in the same direction as the original study, Being able to replicate scientific findings",Different data - same analysis,(1) Agreement in effect size [Comment: direction of effect](2) Agreement in statistical significance [Comment: p </> .05],False,NA,False,NA,False,NA,True,"Each investigation has a relatively small sample of studies with idiosyncratic inclusion criteria and unknown generalizability. However, the diversity in approaches provides some confidence that considering them in the aggregate may provide more general insight about reproducibility in the social behavioural sciences."
Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015,2018,10.1038/s41562-018-0399-z,Joris Frese,Social Sciences and Humanities,Interdisciplinary,"Many Phenomena, Many studies","""Being able to replicate scientific findings is crucial for sci- entific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015"" (Abstract)",True,"""We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies."" (Abstract)",Different data - same analysis,Agreement in effect size,True,"""In additional Bayesian analyses, we use an errors-in-variables mixture model49 to estimate the true-positive rate in the total sam- ple"" (p.639)",False,NA,False,NA,False,NA
Reproducibility in Management Science,2024,10.31219/osf.io/mydzv,Joris Frese,Social Sciences and Humanities,Management Science,"Many Phenomena, Many studies","""With the help of more than 700 reviewers, we assess the reproducibility of nearly 500 articles published in the journal Management Science before and after the introduction of a new Data and Code Disclosure policy in 2019"" (Abstract)",True,"""In addition to individual reproducibility assessments of tables, figures, and other results, we asked reviewers for an overall assessment of their reproduction attempt. The guidelines given to reviewers stated the following assess-ment classifications: •An assessment of “Fully reproduced” means that the output of the reproduction analysis shows the exact same results as reported in the article, for all results reported in the main manuscript.•“Largely reproduced, with minor issues” means that there may be small differences in the reproduction output compared with the results in the original article, but the article’s conclusions and learnings stay the same.•“Largely not reproduced, with major issues” means that there are major differences in the output compared with the results in the article, such that the reproduction results could not be used to support the conclusions of the original article.•An assessment of “Not reproduced” means that the results from the reproduction cannot support the conclusions drawn in the paper, either because the out-put is different, or because the results cannot be pro-duced at all because of missing data or non-recoverable code."" (p.1348)",Same data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Subjective assessment,False,NA,False,NA,False,NA,False,NA
Reproducibility in Management Science,2024,10.31219/osf.io/mydzv,Rachel Heyard,Social Sciences and Humanities,Management sciences,"Many Phenomena, One Study [Comment: But they tried to have two reproductions for as many original papers as possible]",Assess the reproducibility of articles published in Management Science before and after the introduction of the 2019 policy,True,"""The term “computational reproducibility” comes closest to the scope of our study, and is defined as the extent to which results in studies can be reproduced based on the same data and analysis as the original study.""",Same data - same analysis,"Subjective assessment [Comment: After having reviewed a study’s replication package, reviewers are asked “What is your overall assessment of the reproducibility of this article's main results (tables, figures, other results in the main manuscript)?” with six response options.]",True,More assessment on content of replication package and its quality,False,NA,True,the eventual categorization of the article remains subjective to the reviewer. [Comment: Subjective assessment],False,NA
A Multilab Preregistered Replication of the Ego-Depletion Effect,2016,10.1177/1745691616652873,Joris Frese,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies","""Understanding the mechanisms by which self-control predicts behavior [...] Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequentialtask experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion."" (abstract)",True,"""We proposed a set of independent replications of the ego-depletion effect using the sequential-task paradigm, as advocated by Carter and McCullough (2013b, 2014) and Hagger and Chatzisarantis (2014)."" p. 548",Different data - same analysis,Meta-analysis of original and replication study/studies [Comment: Meta analysis of 23 replications. Effect size and confidence interval of meta analysis were reported and interpreted in comparison to the original effect size.],False,NA,False,NA,False,NA,False,NA
A Multilab Preregistered Replication of the Ego-Depletion Effect,2016,10.1177/1745691616652873,Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies",We proposed a set of independent replications of the ego-depletion effect using the sequential-task paradigm,"False [Comment: No formal definition, but they refer to it a replicability]",NA,Different data - same analysis,"(1) Agreement in effect size [Comment: Averaged sample-weighted effect sizes for the mean, only three labs found mean RT values with confidence intervals that did not include the value of zero, two of which were negative](2) Meta-analysis of original and replication study/studies [Comment: Forest plots showing the means of the target dependent variables (mean RTV and RT for the MSIT) in both conditions for each lab]",True,"Heterogeneity in the effect sizes with Cochranes Q and I2 statistics, , with a statistically significant value for Q and an I2 value greater than 25% indicative of at least moderate levels of heterogeneity in the effect size across studies.",False,NA,False,NA,True,"Participating labs were in Australia, Belgium, Canada, France, Germany, Indonesia, the Netherlands, New Zealand, Sweden, Switzerland, and the United States. Coordinated and systematic translation efforts were undertaken to prepare study materials in labs recruiting participants whose native language was not English."
Systematizing Confidence in Open Research and Evidence (SCORE),2024,10.31235/osf.io/46mnb,Helena Hartmann,Social Sciences and Humanities,"Social science;Behavioral science [Comment: Criminology, Economics and Finance, Education, Health, Management, Marketing and Organizational Behavior, Political Science, Psychology, Public Administration, and Sociology]","Many Phenomena, Many studies",creating and validating algorithms to provide confidence scores for research claims at scale. To investigate the viability of scalable tools,True,"assess claims to determine their credibility including evaluating reliability, validity, generalizability, and applicability.","Same data - same analysis;Same data - different analysis ;Different data - same analysis;Different data - different analysis [Comment: database of claims from papers; expert and machine generated estimates of credibility; and, evidence of reproducibility, robustness, and replicability to validate the estimates]",None of the above,True,"predictions, called confidence scores, from human readers about replicability of extracted claims (replicats, prediction markets);generate confidence scores using machine learning and other algorithmic approaches",True,"https://doi.org/10.31222/osf.io/2pczv, osf.io/svg3x",False,NA,True,"SCORE forecasted 3,000 highly diverse claims;Generalizable = Original claim supported across diverse samples, treatments, outcomes, and settings;Robust = Original claim supported with diverse treatments of original data;SCORE is inclusive of a substantial portion of the social-behavioral sciences to facilitate generalizability and investigation of heterogeneity in credibility and replicability across subdisciplines and methodologies. Also, with a standard identification process of discrete claims across papers, the SCORE program facilitates broad inclusion of outcome types, comparison of those outcomes across papers, and a variety of verification attempts including reproduction, robustness, and replication tests."
Systematizing Confidence in Open Research and Evidence (SCORE),2024,10.31235/osf.io/46mnb,Louise Townsin,Social Sciences and Humanities,Interdisciplinary [Comment: social and behavioral sciences],"Many Phenomena, Many studies","The primary research objective for SCORE is to create accurate, scalable, automated algorithms to signal confidence in research claims [Comment: SCORE has aspirational objectives to advance scalable tools for credibility assessment, and will generate substantial research artifacts to support scholarly research on human and machine judgment, replicability and reproducibility, and the nature of research claims. This is made possible by SCORE’s greatest asset -- the participation of hundreds of researchers across the social and behavioral sciences contributing to claim extraction, credibility assessment, and reproducibility, robustness, and replication studies.]",True,Replication: Testing the reliability of a prior finding with new data expected to be theoretically equivalent by comparing the outcome of an inferential test as reported in a paper with the equivalent inferential test as calculated in the new dataset. [Comment: Reproducibility: Testing the reliability of a prior finding with the same data and same analysis strategy by comparing the outcome of an inferential test as reported in a paper with a re-calculation of that inferential test from the original data.],Same data - different analysis,"None of the above [Comment: used human evaluators to provide confidence scores predicting the reproducibility or replicability of the 3,000 research claims in the annotation set. Also used machine learning methods to develop algorithms that assign confidence scores just like the human evaluators. Also created a stratified random sample of 600 of those papers to create the evidence set. Some claims from the evidence set were subjected to reproduction and replication studies.]",False,NA,True,"n/a [Comment: Not reproducibility specifically but In discussion of work to predict/ manage human and automated scores of possible replication: Described in detail by Hanea and colleagues (2021), the aggregation models fall into three categories: (1) linear combinations of best estimates, transformed best estimates (Satopää et al., 2014) and distributions (Cooke et al., 2021); (2) Bayesian approaches, one of which incorporates characteristics of a claim directly from the paper, such as sample size and effect size; and (3) weighted linear combinations of best estimates, mainly by potential proxies for good forecasting performance, such as demonstrated breadth of reasoning, engagement in the task, openness to changing opinion and informativeness of judgments (Mellers, Stone, Atanasov, et al., 2015; Mellers, Stone, Murray, et al., 2015). Replication Markets extends work showing that markets of domain experts can accurately estimate the replicability of findings in the social and behavioral sciences (Camerer et al., 2016, 2018; Dreber et al., 2015; Ebersole et al., 2020; Forsell et al., 2018; Klein et al., 2018; Gordon et al., 2021).]","False [Comment: There is no definitive criterion for deciding whether a finding is successfully replicated or reproduced (Nosek et al., 2021)]",NA,False,NA
Evaluating replicability of laboratory experiments in economics,2016,10.1126/science.aaf0918,Joris Frese,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","""The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014."" (Abstract)",True,"""Our sample consists of all 18 between-subject laboratory experimental papers published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. The most important statistically significant finding as emphasized by the authors of each paper, was chosen for replication (see section 1 of the supplementary materials and tables S1 and S2). We used replication sample sizes with at least 90% power (mean = 92%; median = 91%) to detect the original effect size at the 5% significance level"" (pp.1433-1434)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Meta-analysis of original and replication study/studies,False,NA,True,10.1198/000313006X152649; 10.1111/j.1745-6924.2008.00079.x,True,"""The results of the replications are shown in Fig. 1A and table S1. We found a significant effect in the same direction as in the original study for 11 replications (61.1%). This is considerably lower than the replication rate of 92% (mean power) that would be expected if all original effects were true and accurately estimated (one-sample binomial test, P < 0.001). A complementary method for assessing replicability is to test whether the 95% confidence interval (CI) of the replication effect size includes the original effect size (19) [Cumming (21) discusses the interpretation of CIs for replications]. This is the case in 12 of our replications (66.7%). If we also include the study in which the entire 95% CI exceeds the original effect size, the number of replicable studies increases to 13 (72.2%). An alternative measure, which acknowledges sampling error in both the original study and the replications, is to count how many replicated effects lie in a 95% “prediction interval” (24). This count is higher (83.3%) and increases to 88.9% if we also include the replication whose effect size exceeds the upper bound of the prediction interval (fig. S2 and supplementary materials, section 2). The mean standardized effect size (correlation coefficient, r) of the replications is 0.279, compared with 0.474 in the original studies (fig. S3). This difference is significant [Wilcoxon signedrank test; z = –2.98, P = 0.003, n = 18]. The replicated effect sizes tend to be of the same sign as the original ones but not as large. The mean relative effect size of the replications is 65.9%. The original and replication studies can also be combined in a meta-analytic estimate of the effect size (19). As shown in Fig. 1B, in the metaanalysis, 14 studies (77.8%) have a significant effect in the same direction as in the original study. These results should be interpreted cautiously, because the estimates assume that the results of the original studies do not have publication or reporting biases",False,NA
Evaluating replicability of laboratory experiments in economics,2016,10.1126/science.aaf0918,Louise Townsin,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","Our sample consists of all 18 between-subject laboratory experimental papers published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. The most important statistically significant finding, as emphasized by the authors of each paper, was chosen for replication (see section 1 of the supplementary materials and tables S1 and S2). We used replication sample sizes with at least 90% power (mean = 92%; median = 91%) to detect the original effect size at the 5% significance level. All of the replication and analysis plans were made public on the project website (supplementary materials, section 1) and were also sent to the original authors for verification.",False,NA,Same data - different analysis,"Agreement in statistical significance [Comment: We present results for the same replication indicators that were used in the RPP. As our first indicator of replication, we used a “significant effect in the same direction as in the original study”. A complementary method for assessing replicability is to test whether the 95% confidence interval (CI) of the replication effect size includes the original effect size. An alternative measure, which acknowledges sampling error in both the original study and the replications, is to count how many replicated effects lie in a 95% “prediction interval”. We also tested whether replicability is correlated with two observable characteristics of published studies: the P value and the sample size (number of participants) of the original study. The original and replication studies can also be combined in a meta-analytic estimate of the effect size. To measure peer beliefs about the replicability of original results, we set up prediction markets before the 18 replications were performed.]",False,NA,False,NA,True,"Acknowledging the limits of this two-study comparison, and particularly our small sample of 18 replications, there appears to be some difference in replication success between these fields. However, it is premature to draw strong conclusions about disciplinary differences; other methodological factors potentially could explain why the replication rates differed. [Comment: the original effect sizes in the studies that we replicated could have been inflated, a phenomenon that could stem from publication bias (28). If there is publication bias, our prospective power analyses will have overestimated the replication power.]",False,NA
Reproducibility of findings in modern PET neuroimaging: insight from the NRM2018 grand challenge,2021,10.1177/0271678X211015101,Helena Hartmann,Life Sciences,Neuroimaging,"One Phenomenon, Many Studies",assess the performance and consistency of PET pre-processing and modelling approaches on a common data set where the ground truth was known,True,ability to replicate experimental results,Same data - different analysis,"(1) Agreement in statistical significance [Comment: the capacity of returning the correct BPND values both in terms of single maps and in terms of magnitude of change in the displacement regions,](2) Subjective assessment [Comment: the spatial identification of the simulated displaced regions.  For the second criterion, i.e. the identification of the displacement regions, the Jaccard similarity index was used to compare the areas of displacement]",False,NA,False,NA,True,a limited subset of quantification methods was implemented [Comment: not sure if relevant],True,"The full dataset, inclusive of simulated dynamic PET images, reference kinetic parameters and areas of displacement..."
Reproducibility of findings in modern PET neuroimaging: insight from the NRM2018 grand challenge,2021,10.1177/0271678X211015101,Rachel Heyard,Life Sciences,Neuroscience,"One Phenomenon, Many Studies",This paper investigates the reproducibility of findings in a PET neuroimaging study by looking at the consistency of results provided by 14 imaging groups who took part in the PET Grand Challenge during the NeuroReceptor Mapping (NRM) conference in London 2018.,"False [Comment: Reproducibility, but definition provided]",NA,Same data - different analysis,None of the above,True,percentage root-mean-squared error (RMSE);Jaccard similarity index,False,NA,False,NA,False,NA
Registered Replication Report: Study 1 From Finkel Rusbult Kumashiro & Hannon (2002),2016,10.1177/1745691616664694,Helena Hartmann,Social Sciences and Humanities,Social psychology [Comment: inferred],"One Phenomenon, Many Studies","This Registered Replication Report (RRR) meta-analytically combines the results of 16 new direct replications of the original study, all of which followed a standardized, vetted, and preregistered protocol.;this study provides the only experimental evidence that inducing changes to subjective commitment can causally affect forgiveness responses.",False,NA,Different data - different analysis,(1) Meta-analysis of original and replication study/studies(2) Agreement in statistical significance(3) Agreement in effect size,False,NA,False,NA,False,NA,False,NA
Registered Replication Report: Study 1 From Finkel Rusbult Kumashiro & Hannon (2002),2016,10.1177/1745691616664694,Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies [Comment: Registered replication report]",to provide a direct replication of this influential finding and to provide a more precise estimate of the size of the effect of this commitment prime on how people report that they would respond to betrayals from a romantic partner,True,"direct replications of the original study, all following the same vetted protocol",Different data - same analysis,None of the above,True,"Comparison of original effect to meta-analytical effect from all replications [Comment: If the RRR precisely replicated the results of the original study, it would observe a similarly sized difference for exit and neglect, and a similar lack of a difference for voice and loyalty]",False,NA,False,NA,False,NA
Reproducibility of real-world evidence studies using clinical practice data to inform regulatory and coverage decisions,2022,10.1038/s41467-022-32310-3,Rachel Heyard,Life Sciences,Medicine;Clinical trials;Real-world evidence,"Many Phenomena, One Study",To evaluate the independent reproducibility of results from 150 published RWE studies using the same healthcare databases and applying the same reported methods as original authors.,True,"Reproducibility is the ability to obtain the same results when reanalyzing the original data, following the original analysis strategy. [Comment: They defined it as being different from computational reproducibility, since they did not use original data and code - but simply followed the same steps to acquire the same data and analysis.]",Same data - same analysis,None of the above,True,"Reproducibility of population sample size was measured by dividing the sample size of the original study by the reproduction.;Reproducibility of baseline characteristics reported in an original manuscript table describing the cohort characteristics. The reproducibility of binary and categorical baseline characteristics of the study population was measured by taking the prevalence of the characteristic reported in the original publication (within each exposure group if there was more than one) and subtracting the prevalence obtained in the reproduction.;Reproducibility of outcome risks and rates was measured by taking the reported outcome risk or rate in the original publication and subtracting the risk or rate obtained in the reproduction. For descriptive studies, the overall risk or rate was reproduced. For comparative studies, the risk or rate was reproduced for each compared group. Rates were converted to reﬂect events per 100 person-years.;Reproducibility metrics for measures of associations of interest were the relative magnitude of the original effect size compared to the reproduction effect size and the Pearson’s correlation coefﬁcient between the reproduced and original effect sizes. [Comment: They used calibration plots and bland-altman plots to represent findings and compare results.]",False,NA,True,"the proportion of studies where the effect estimate was on the same side of null can mislead in RWE studies with small effect sizes as small implementation differences could conceivably result in enough change to result in an effect estimate on the other side of null in a reproduction attempt. Further, the reproduction effort is focused on US and UK data sources frequently used in research and the generalizability may be limited to well-established and curated research databases that are accessible to independent researchers. [Comment: they refer to this: 10.1515/em-2016-0018]",False,NA
Reproducibility of real-world evidence studies using clinical practice data to inform regulatory and coverage decisions,2022,10.1038/s41467-022-32310-3,Samuel Pawel,Life Sciences,healthcare;randomized clinical trials;real-world evidence,"Many Phenomena, One Study","""evaluate the independent reproducibility of results from 150 published RWE studies using the same healthcare databases and applying the same reported methods as original authors.""",True,"Reproducibility is the ability to obtain the same results when reanalyzing the original data, following the original analysis strategy",Same data - different analysis [Comment: by different analysis I mean that the replicators/reproducers tried to reimplement the analysis without just rerunning the original code],Agreement in effect size [Comment: relative magnitude of the original effect size compared to the reproduction effect size; Pearson’s correlation coefficient between the reproduced and original effect sizes (both unweighted and inverse variance weighted); proportion of studies where the absolute difference in the coefficients for the measure of association differed by ≤0.1 or ≤0.2; proportion of comparative studies where the reproduced measure of association was closer to null than the original; the proportion where the measures were on the same side of null; the proportion with any overlap in 95% confidence intervals],False,NA,True,"10.1515/em-2016-0018 [Comment: This paper was cited next to the statement ""Any single metric to characterize reproducibility is imperfect""]",True,"Any single metric to characterize reproducibility is imperfect 38 . For example, the proportion of studies where the effect estimate was on the same side of null can mislead in RWE studies with small effect sizes as small implementation differences could conceivably result in enough change to result in an effect estimate on the other side of null in a reproduction attempt.",False,NA
Using mice from different breeding sites fails to improve replicability of results from single-laboratory studies,2024,10.1038/s41684-023-01307-w,Helena Hartmann,Life Sciences,Neuroscience;Animal studies,"Many Phenomena, Many studies",we systematically tested whether heterogenization of study populations by including animals from different breeding sites is effective in improving the replicability of findings from single-laboratory animal studies,False,NA,Different data - same analysis,Agreement in effect size [Comment: not sure if significance or effect size],False,NA,False,NA,False,NA,True [Comment: not really the diveristy we mean I think],"the diversity of the mice within breeding sites may have been greater than expected, thereby limiting the scope for variation between breeding sites. This could, for example, be due to variation in age (the age of mice may vary by several days) and origin from different colony rooms."
Using mice from different breeding sites fails to improve replicability of results from single-laboratory studies,2024,10.1038/s41684-023-01307-w,Rachel Heyard,Life Sciences,preclinical research,"One Phenomenon, Many Studies [Comment: We used male C57BL/6J mice from six different breeding sites to test a standardized against a heterogenized (HET) study design in six independent replicate test laboratories]","In this Article, we therefore tested whether systematic hetero- genization of study populations, by using mice from different breeding sites to introduce the genetic and environmental variation that normally exists between independent study populations, would increase the external validity of the results sufficiently to guarantee replicability.",False [Comment: Replicability],NA,Different data - same analysis;Different data - different analysis [Comment: They used two sets of protocols],None of the above,True,"multivariate analysis of variance (MANOVA);Levene tests for equal variances;To investigate whether HET designs led to lower between-laboratory variation than STA designs, we ran for each outcome variable two separate mixed models with the outcome as a dependent variable, laboratory as fixed effect and cage ID as a random factor—one for the HET design and one for the STA design. We then compared the marginal R2 estimates. [Comment: For comparing the variance of each Levene: outcome variable between STA and HET cohorts within each of the laboratory]",False,NA,False,NA,False,NA
Estimating the reproducibility of psychological science,2015,10.1126/science.aac4716,Helena Hartmann,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies","large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science",False,NA,Different data - different analysis,"Agreement in statistical significance [Comment: Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes.]",False,NA,False,NA,True,These data provide information about reproducibility in general but little precision about individual effects in particular.,False,NA
Estimating the reproducibility of psychological science,2015,10.1126/science.aac4716,Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, One Study",To estimate the reproducibility of psychological science,True,"Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding (7, 8) and is the means of establishing reproducibility of a finding with new data.",Different data - same analysis,"(1) Agreement in statistical significance [Comment: original studies that interpreted nonsignificant P values as significant were coded as significant (four cases, all with P values < 0.06). Using only the nonsignificant P values of the replication studies and applying Fisher’s method (26), we tested the hypothesis that these studies had “no evidential value” (the null hypothesis of zero-effect holds for all these studies).](2) Agreement in effect size [Comment: We compared effect sizes using four tests. We compared the central tendency of the effect size distributions of original and replication studies using both a paired two-sample t test and the Wilcoxon signed-rank test. Third, we computed the proportion of study-pairs in which the effect of the original study was stronger than in the replication study and tested the hypothesis that this proportion is 0.5. For this test, we included findings for which effect size measures were available but no correlation coefficient could be computed (for example, if a regression coefficient was reported but not its test statistic). Fourth, we calculated “coverage,” or the proportion of study-pairs in which the effect of the original study was in the CI of the effect of the replication study, and compared this with the expected pro- portion using a goodness-of-fit c2 test.](3) Meta-analysis of original and replication study/studies [Comment: fixed-effect meta-analyses using the R package metafor. The number of times the CI of all these meta-analyses contained 0 was calculated.](4) Subjective assessment [Comment: For subjective assessment, replication teams answered “yes” or “no” to the question, “Did your results replicate the original effect?”]",False,NA,False,NA,False,NA,False,NA
Comparison of Model Averaging Techniques: Assessing Growth Determinants,2012,10.1002/jae.2288,Joris Frese,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","""This paper investigates the replicability of three important studies on growth theory uncertainty that employed Bayesian model averaging tools."" (Abstract)",True,"""Our basis for comparison of BMA, WALS and MMA techniques are three recent studies of growth theory uncertainty: Fernandez et al. (2001b), Masanjala and Papageorgiou (2008) and Doppelhofer and Weeks (2009). All three datasets are publicly available on the Journal of Applied Econometrics online data archive [...] For the following analyses we construct BMA, WALS, and MMA estimators using the three datasets FLS, MP, and DW, and compare the findings"" (p. 873)",Same data - different analysis,Agreement in effect size,False,NA,False,NA,False,NA,False,NA
Comparison of Model Averaging Techniques: Assessing Growth Determinants,2012,10.1002/jae.2288,Rachel Heyard,Social Sciences and Humanities,Econometrics,"Many Phenomena, One Study",This paper set out to replicate several important studies focused on model uncertainty in the modeling of economic growth.,"False [Comment: No formal definition, but refer to replicability]",NA,Same data - different analysis,Agreement in effect size [Comment: Comparison of coefficient estimates],True,Comparison of posterior inclusion probability,False,NA,False,NA,False,NA
Registered Replication Report: Strack Martin & Stepper (1988),2016,10.1177/1745691616674458,Joris Frese,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies","""Could smiling make us happier?"" (p. 917)",True,"""17 laboratories each conducted a direct replication study of Study 1 from SMS using a vetted protocol. By combining the results of these direct replications meta-analytically, we can provide a more precise estimate of the size of this important effect."" (p. 918)",Different data - same analysis,Meta-analysis of original and replication study/studies [Comment: Meta analysis of 17 replication studies (but not the original study)],True,"""In addition to this primary analysis, we report two Bayes factor analyses for each study."" (p. 921)",False,NA,False,NA,False,NA
Registered Replication Report: Strack Martin & Stepper (1988),2016,10.1177/1745691616674458,Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies [Comment: Registered Replication Report]",to measure the reliability and size of an effect.,True,replicated directly using the same design and the same dependent variable. [Comment: Replicability],Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size [Comment: the 95% confidence interval was narrower than the one estimated for Study 1 from SMS](3) Agreement in effect size [Comment: the 95% confidence interval overlapped the mean effect size from SMS Study 1],True,Random-effects meta-analysis of all replications vs. original,False,NA,False,NA,False,NA
Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses,2023,10.1177/25152459231162567,Helena Hartmann,Social Sciences and Humanities,Speech production research;Acoustic research,"One Phenomenon, Many Studies",assess flexibility in fields in which the primary data lend themselves to a variety of possible operationalizations,True,(a) produce data that can be replicated using the original methods and (b) arrive at robust conclusions substantiated by such data.,Same data - different analysis,(1) Meta-analysis of original and replication study/studies(2) Agreement in effect size,True,"Descriptive summary statistics [Comment: describing variation among analyses, including (a) the nature and number of acoustic measures (e.g., f0 or dura- tion), (b) the operationalization and the temporal domain of measurement (e.g., mean of an interval or value at a specified point in time), (c) the nature and number of model parameters for both fixed and random effects (if applicable), (d) the nature and reasoning behind inferential assessments (e.g., dichotomous decision based on p values, ordinal decision based on a Bayes factor), as well as the (e) mean, (f) standard deviation, and (g) range of the standardized effect sizes (see the next section for the standardization procedure).]",False,NA,False,NA,False,NA
Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses,2023,10.1177/25152459231162567,Rachel Heyard,"Social Sciences and Humanities;Life Sciences;STEM, e.g. Engineering, Mathematics, Physics",Linguistics;Interdisciplinary,"One Phenomenon, Many Studies",The primary aim of this analysis was to assess the degree of between-team variability.,"False [Comment: No formal definition, but refer to reproducibility]",NA,Same data - same analysis,"Meta-analysis of original and replication study/studies [Comment: We investigated the variability in reported effect sizes using Bayesian meta-analytic techniques. As the measure of variability, we took the meta-analytic group-level standard deviation (σαt; see below), where each analysis team represents a group.]",False,NA,False,NA,False,NA,False,NA
Leveraging real-world data to assess treatment sequences in health economic evaluations: a study protocol for emulating target trials using the English Cancer Registry and US Electronic Health Records-Derived Database,2024,NA,Joris Frese,"Life Sciences;STEM, e.g. Engineering, Mathematics, Physics",Medicine,"Many Phenomena, Many studies","""We aim to assess the capability of two  oncology databases: the US-based Flatiron electronic health record and the National Cancer  Registration and Analysis Service (NCRAS) database of England"" (Abstract)",True,"""We aim to emulate existing RCTs that compare the effect of different treatment sequences by  constructing the study design and analysis plan following the TTE framework. Specifically, the  following case studies are planned: (1) Prostate cancer case study 1 (PC1) - US direct proof-of-concept study (method direct  validation): replicating the GUTG-001 trial using Flatiron data  (2) Prostate cancer case study 2 (PC2) - US-England bridging study (method extension): emulating Target Trials that compare treatment sequences that have been common in  England using Flatiron data  (3) Prostate cancer case study 3 (PC3) - English indirect proof-of-concept study (method  indirect validation): emulating the same Target Trial in PC2 using English NCRAS data (4) Renal cell carcinoma case study (RCC) - method direct validation in a single-arm setting:  emulating the sunitinib followed by everolimus arm in the RECORD-3 trial using English  NCRAS data 2. We will compare results of the emulated Target Trials with those from the benchmark trials.  3. We plan to compare different advanced causal inference methods (e.g. marginal structural  models using IPW and other g-methods) in estimating the effect of treatment sequences in  RWD."" (p.8)",Different data - different analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""Considering the potentially disproportionately large sample size of RWE,  achieving statistical significance might be easier compared to the benchmarks"" (p.43)",False,NA
Leveraging real-world data to assess treatment sequences in health economic evaluations: a study protocol for emulating target trials using the English Cancer Registry and US Electronic Health Records-Derived Database,2024,NA,Rachel Heyard,Social Sciences and Humanities;Life Sciences,Health economics,"Many Phenomena, One Study",to assess the feasibility of harnessing data from the English Cancer Registry and Flatiron electronic health record (EHR)-derived database to derive unbiased effect estimates for comparing oncology treatment sequences.,True,"replicate the designs and results of existing clinical trials (i.e., benchmark trials) through emulating Target Trials using RWD, including applying the same (or as far as possible) patient inclusion/exclusion criteria and analytical methods to achieve the emulation [Comment: Emulation]",Different data - same analysis;Different data - different analysis,"(1) Agreement in statistical significance [Comment: Regulatory agreement: This component assesses whether the RWE replicates its benchmark’s results (such as hazard ratio (HR) and risk ratio (RR)) in terms of both direction and statistical significance observed in the benchmark trials. Endpoints with non-significant effects in RCTs should also show no significant effect in RWE.](2) Agreement in effect size [Comment: Estimate agreement: examines whether the point estimate of RWE’s effect sizes falls within the 95% confidence intervals (CIs) of the benchmark trial. Furthermore, we added an extra procedure to include the comparison of non-relative effect estimates for timeto-event outcomes. For example, it examines whether the point effect of median survival estimates falls within the 95% confidence interval (CI) of the trial.](3) Agreement in effect size [Comment: Exploratory - standardised differences: It involves computing the standardised difference to compare the relative effect estimates from the benchmark and the RWE, to determine whether there is a statistically significant difference in the estimated effects.]",True,Exploratory - survival curve comparison: The key aspect here is assessing whether the point estimates of the RWE survival curve for each treatment-sequence group fall within the 95% CI of the benchmark trial.,True,10.1002/cpt.1633 [Comment: Agreement measures],True,"Considering the potentially disproportionately large sample size of RWE, achieving statistical significance might be easier compared to the benchmarks (and thus not easier to meet the first criteria). [Comment: Agreement in statistical significance]",False,NA
A rise by any other name? Sensitivity of growth regressions to data source,2008,10.1016/j.jmacro.2007.08.015,Helena Hartmann,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies",Replication of several recent studies of growth determinants,False,NA,Same data - same analysis;Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,False,NA,True,"Table 8 are based on the “full conditioning set of variables” that includes the variables of interest plus initial years of schooling, government size, inflation rate, black market premium, openness to trade, number of revolutions and coups, political assassinations and ethnic diversity.;As of version 6.1 PWT contains 115 benchmark countries ( i.e. countries included in the ICP) and 53 additional nonbenchmark countries."
A rise by any other name? Sensitivity of growth regressions to data source,2008,10.1016/j.jmacro.2007.08.015,Louise Townsin,Social Sciences and Humanities,Economics,"One Phenomenon, Many Studies",Measured rates of growth in real per capita income differ drastically depending on the data source. This phenomenon occurs largely because data sets differ in whether and how they adjust for changes in relative prices across countries. Replication of several recent studies of growth determinants shows that results are sensitive in important ways to the choice of data. Previous warnings against using data adjusted to increase cross-country comparability to study within-country patterns over time (growth rates) have been largely ignored at the cost of possibly contaminating the conclusions.,True [Comment: Replication of several recent studies of growth determinants shows that results are sensitive in important ways to the choice of data.],We selected four studies published in major journals since 2000 and requested the original data from the authors.9 In each case we selected a basic equation using relatively simple econometric techniques.10 We first replicated the results reported in the original paper and then replaced the dependent variable (growth rate) in the original data with growth rates calculated from own-country data as reported in the IFS data base and the income level variable on the right-hand side of the estimated equation with cross-country comparable PPP-adjusted data from the Penn World Tables.,Same data - same analysis,"Agreement in statistical significance [Comment: We have also replicated simple results from four recent studies of determinants of differences in long-term growth across countries. In each case, we retained the specification and all data from the original study except for initial income levels and measures of growth used as the dependent variable, which we calculated own-country data for growth rates and PPP adjusted cross-country comparable data for initial income levels. When these alternative sources resulted in a reduced sample size, we also reestimated the relationship using the original data but smaller sample. In each case, the results could most charitably be described as “fragile.” Key relationships change in size and significance, frequently leading to fundamentally different conclusions were the analysis to be based on seemingly simple changes of data set.]",False [Comment: No],NA,False [Comment: No],NA,False [Comment: No],NA,False [Comment: No],NA
Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology,2020,10.1016/j.jpsychores.2020.110211,Joris Frese,Life Sciences,Psychology; Medicine,"One Phenomenon, Many Studies","""we crowdsourced the analysis of one individual patient's ESM data to 12 prominent research teams, asking them what symptom(s) they would advise the treating clinician to target in subsequent treatment."" (Abstract)",True,"""we crowdsourced the analysis of one individual patient's ESM data to 12 prominent research teams, asking them what symptom(s) they would advise the treating clinician to target in subsequent treatment."" (Abstract)",Same data - same analysis;Same data - different analysis,Subjective assessment,True,(Not pasted) Number and type of items selected for a follow-up analysis based on ESM.,True,10.1002/wps.20513 [Comment: Citation for Experience Sampling Methodology],True,"Too much to paste but various discussions of limitations in chapter 4, starting on p.10",False,NA
Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology,2020,10.1016/j.jpsychores.2020.110211,Rachel Heyard,Social Sciences and Humanities;Life Sciences,Medicine;Psychology;Statistics [Comment: Investigate experience sampling methodology],"One Phenomenon, Many Studies",evaluate how much researchers vary in their analytical approach towards these individual time-series data and to what degree outcomes vary based on analytical choices.,"False [Comment: No formal definition, but interested in variability of methods and results]",NA,Same data - same analysis;Same data - different analysis [Comment: Teams had to answer the same research question but without following a standardized protocol],None of the above,True,"Description of differences in analytical approaches including variable selection, preprocessing, clustering and statistical analyses.;Description of differences in results, e.g. selected targets, treatment selection",False,NA,False,NA,True,"None of the analytic approaches were inherently invalid. Instead, the multiplicity of plausible processing steps implies that there could be several sensible statistical results based on the same original dataset [Comment: Referencing to this 10.1177/1745691616658637]"
Estimating the Reproducibility of Experimental Philosophy,2021,10.1007/s13164-018-0400-9,Joris Frese,Social Sciences and Humanities,Philosophy; Experimental Philosophy,"Many Phenomena, Many studies","""Responding to recent concerns about the reliability of the published literature in psychology and other disciplines, we formed the X-Phi Replicability Project (XRP) to estimate the reproducibility of experimental philosophy"" (Abstract)",True,"""To answer this question, ‘direct’ replications are needed (Doyen et al. 2014). Direct replications—often contrasted with ‘conceptual’ replications—are replications that attempt to follow the design and methods of an original study as closely as possible in order to confirm its reported findings"" (p.14)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size(3) Subjective assessment,False,NA,True,10.1521/soco.2014.32.supp.12,True,"Too much to paste, but section 4 of the article discusses in great length the potential reasons why findings in experimental philosophy are so much more replicable than e.g., findings in psychology",False,NA
Estimating the Reproducibility of Experimental Philosophy,2021,10.1007/s13164-018-0400-9,Rachel Heyard,Social Sciences and Humanities,Experimental Philosophy;Philosophy,"Many Phenomena, One Study",Estimating the Reproducibility of Experimental Philosophy,True,"Replicability = ""Direct replications—often contrasted with ‘conceptual’ replications—are replications that attempt to follow the design and methods of an original study as closely as possible in order to confirm its reported findings.""",Different data - same analysis,"(1) Agreement in statistical significance [Comment: Were the replication results statistically significant? For the present research, we defined ‘statistically significant’ as a p-value less than .05, following the currently conventional default standards for Null Hypothesis Significance Testing (NHST).](2) Agreement in effect size [Comment: Comparison of the original and replication effect size: Because sample sizes of replication studies were typically larger than those of original ones, and because calculation of confidence intervals (CIs) for original effect sizes were not always possible (due to a lack of information), we decided to draw this comparison by investigating whether the original effect size fell within the 95% CI of the replication effect size.](3) Subjective assessment [Comment: Subjective assessment of the replicating team: By asking our researchers to register their overall subjective judgment about whether the effect replicated, therefore, they were able to take into consideration the ‘wider picture’.]",False,NA,False,NA,True,"(1) the use of p-values as a criterion for success is especially dubious when applied to studies reporting null results [Comment: Agreement in statistical significance](2) sample sizes of replication studies were typically larger than those of original ones, and calculation of confidence intervals (CIs) for original effect sizes were not always possible (due to a lack of information), [Comment: Agreement in effect size]",False,NA
Many Labs 3: Evaluating participant pool quality across the academic semester via replication,2016,10.1016/j.jesp.2015.10.012,Helena Hartmann,Social Sciences and Humanities,Social psychology;Cognitive psychology,"Many Phenomena, Many studies",examined time of semester variation in 10 known effects 10 individual differences and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737),True [Comment: only indirectly],finding the same findings in multiple samples,Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,False,NA,True,"In addition, for the collected set of effects and measures we sought: (1) diversity of represented research domains, (2) diversity of known or presumed likelihood of variation across the semester, and (3) diversity of “classic” well-established effects and contemporary effects that have untested replicability.;These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample."
Many Labs 3: Evaluating participant pool quality across the academic semester via replication,2016,10.1016/j.jesp.2015.10.012,Rachel Heyard,Social Sciences and Humanities,Psychology [Comment: social psychology; cognitive psychology],"Many Phenomena, Many studies",we investigated the extent to which 10 psychological effects and multiple individual difference variables varied across the academic semester.,"False [Comment: No formal definition, but refer to replication and replicability]",NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: For each phenomenon, the effect aggregated for all replication participants was compared to the original effect](2) Agreement in effect size [Comment: For each phenomenon, the effect aggregated for all replication participants was compared to the original effect]",False,NA,False,NA,False,NA,True,"These participants came from a wide range of institutions, producing a relatively diverse undergraduate sample.;Given this, one strategy would have been to only select classic, well-established effects for replication. However, it is possible that these effects are well established because they are resistant to contextual variation."
Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement,2022,10.1525/collabra.35271,Helena Hartmann,Social Sciences and Humanities,Social psychology,"One Phenomenon, Many Studies",experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory,False,NA,Different data - same analysis [Comment: similar analysis protocol as original study],Meta-analysis of original and replication study/studies,False,NA,True,https://doi.org/10.3389/fpsyg.2014.01521,False,NA,False,NA
Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement,2022,10.1525/collabra.35271,Rachel Heyard,Social Sciences and Humanities,Psychology,"One Phenomenon, Many Studies",To investigate whether author involvement increases replicability,"False [Comment: No formal definition, but refer to replicability]",NA,Different data - same analysis,None of the above,True,"Original vs. meta-analysis of all replications [Comment: No clear explanation of metric used, but they did a random-effects meta-analysis on all the replications and then compared this with the original results]",False,NA,False,NA,False,NA
Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say “Often Not”,2022,10.1561/104.00000053,Helena Hartmann,Social Sciences and Humanities,Macroeconomics,"Many Phenomena, Many studies",to replicate 67 macroeconomic papers published in 13 well-regarded economics journals using author-provided replication ﬁles that included both data and code by following a preanalysis plan.,True,ability to use the author-provided data and code ﬁles to produce the key qualitative conclusions of the original paper,Same data - same analysis,None of the above,True,Reproduce Figures and Tables;produce the key qualitative conclusions of the original paper;qualitatively reproduce the key results of the paper,False,NA,False,NA,False,NA
Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say “Often Not”,2022,10.1561/104.00000053,Joris Frese,Social Sciences and Humanities,Economics; Macroeconomics,"Many Phenomena, Many studies","""We attempted to replicate 67 macroeconomic papers published in 13 well-regarded economics journals using author-provided replication ﬁles that included both data and code"" (Abstract)",True,"""We deﬁned a successful replication as when the authors or journal provided data and code ﬁles that allowed us to qualitatively reproduce the key results of the paper."" (p.8)",Same data - same analysis,None of the above,True,"Their ""deﬁnition corresponds to replication - veriﬁcation by Clemens (2015)’s Table 1 with the added condition of the authors providing us data and code ﬁles, a rating of three out of ﬁve, “minor discrepancies,” or better by Glandon (2010), and “partially successful replication” or better by McCullough, McGeary, and Harrison (2006)."" (p.8, footnote 15)",True,"(1) 10.1111/joes.12139 [Comment: Replication-Verification by Clemens (2017)](2) No DOI [Comment: Replication Accuracy Rating System by Glandon (2010)](3) 10.1353/mcb.2006.0061 [Comment: Partially Successful Replication (""replicating enough of the results that the conclusions of the article remain intact"") by McCollough et al. (2006)]",True,"""We deﬁned success using this extremely loose deﬁnition to get an upper bound on what the replication success rate could be."" (p.8) [Comment: Replication Verification; Replication Accuracy System; Partially Successful Replication]",False,NA
Investigating Variation in Replicability,2014,10.1027/1864-9335/a000178,Joris Frese,Social Sciences and Humanities;Life Sciences,Psychology,"Many Phenomena, Many studies","""This research tested variation in thereplicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. [...] We compared whether theconditions such as lab versus online or US versus international sample predicted effect magnitudes."" (Abstract)",True,"""This research tested variation in thereplicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants."" (Abstract)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,False,NA,True,"""he replication sites included inthis project cannot capture all possible cultural variation,and most societies sampled were relatively Western,Educated, Industrialized, Rich, and Democratic (WEIRD)"" (p.151)"
Investigating Variation in Replicability,2014,10.1027/1864-9335/a000178,Rachel Heyard,Social Sciences and Humanities,Psychology,"Many Phenomena, Many studies","to examine the heterogeneity of effect sizes by the wide variety of samples and settings, and to provide an example of a paradigm for testing such variation.",True,"Replication is a central tenet of science; its purpose is to confirm the accuracy of empirical findings, clarify the conditions under which an effect can be observed, and estimate the true effect size. Successful replication of an experiment requires the recreation of the essential conditions of the initial experiment. [Comment: Replicability]",Different data - same analysis,(1) Agreement in statistical significance [Comment: Proportion of samples that rejected the null hypothesis in the expected and unexpected direction.](2) Agreement in effect size [Comment: intra-class correlation of samples across effects + heterogeneity of effect sizes using Cochran's Q and I2 statistics],False,NA,False,NA,False,NA,True,All replication studies were translated into the dominant language of the country of data collection
Data Models Coefficients: The Case of United States Military Expenditure,2007,10.1080/07388940601102845,Joris Frese,Social Sciences and Humanities,Economics,"Many Phenomena, Many studies","""The purpose of this article is to demonstrate the obvious: that use of the “wrong” data leads one to obtain improper results. Even if one employs the “right” (or at least “better”) data, one can obtain results that are inconsistent when the sample is varied."" (p.55)",True,"""The article replicates two published models of the effect of military expenditure on the economy of the United States. To study variations in the relevant estimated parameters, it applies two different military expenditure data sets to the models (budget-based vs. National Income and Product Accounts [NIPA]-based data). In an extension, the article examines coefficient stability when the economically preferred NIPA-based data are applied across varying time-periods."" (p.55)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,False,NA,False,NA
Data Models Coefficients: The Case of United States Military Expenditure,2007,10.1080/07388940601102845,Louise Townsin,Social Sciences and Humanities,Economics,"One Phenomenon, Many Studies","It is part of a tradition of a small body of methodological work investigating the (im)proper use of models and data in defense and peace economics [Comment: The article replicates two published models of the effect of military expenditure on the economy of the United States. To study variations in the relevant estimated parameters, it applies two different military expenditure data sets to the models (budget-based vs. National Income and Product Accounts [NIPA]-based data). In an extension, the article examines coefﬁcient stability when the economically preferred NIPA-based data are applied across varying time-periods.]",False,NA,Different data - same analysis [Comment: the exercise investigates what happens to the parameter estimates in these models when two different data sets—both purporting to measure United States military expenditure—are used.],Agreement in statistical significance,False,NA,False,NA,False,NA,False,NA
Emulation of Randomized Clinical Trials With Nonrandomized Database Analyses: Results of 32 Clinical Trials,2023,10.1001/jama.2023.4221,Helena Hartmann,Life Sciences,Medicine,"Many Phenomena, Many studies",To emulate the design of 30 completed and 2 ongoing randomized clinical trials (RCTs) of medications with database studies using observational analogues of the RCT design parameters (population intervention comparator outcome time [PICOT]) and to quantify agreement in RCT-database study pairs,False,NA,Different data - same analysis,(1) Agreement in effect size(2) Agreement in statistical significance,False,NA,False,NA,True,"Second, apparent agreement between RCT and database study results could occur if the effects of multiple factors (chance, emulation differences, bias) cancel each other out.",False,NA
Emulation of Randomized Clinical Trials With Nonrandomized Database Analyses: Results of 32 Clinical Trials,2023,10.1001/jama.2023.4221,Rachel Heyard,Life Sciences,Clinical trials;Medicine,"Many Phenomena, One Study","To emulate the design of 30 completed and 2 ongoing randomized clinical trials (RCTs) of medications with database studies using observational analogues of the RCT design parameters (population, intervention, comparator, outcome, time [PICOT]) and to quantify agreement in RCT-database study pairs.",True [Comment: Emulation],"We aimed to emulate RCT designs under the best possible circumstances by identifying and implementing observational analogues of RCT design parameters that define the research question (population, intervention, comparator, outcome, time-frame [PICOT]11), apply confounding adjustment meth- ods, and then compare the results of RCT–database study pairs",Different data - same analysis,"(1) Agreement in statistical significance [Comment: full statistical significance agreement, defined by estimates and CIs on the same side of the null](2) Agreement in effect size [Comment: estimate agreement, defined by whether estimates for the trial emulation fell within the 95% CI for the trial results](3) Agreement in effect size [Comment: standardized difference agreement between treatment effect estimates from trials and emulations, defined by standardized differences |z|<1.96]",True,partial significance agreement was defined as meeting the prespecified noninferiority criteria even though the database study may have indicated superiority,False,NA,False,NA,False,NA
Registered Replication Report: Rand Greene and Nowak (2012),2017,10.1177/1745691617693624,Helena Hartmann,Social Sciences and Humanities,Economics,"One Phenomenon, Many Studies",assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate preregistered replications of the critical conditions from Study 7 of the original article (Rand et al. 2012),False,NA,Different data - same analysis,Meta-analysis of original and replication study/studies,True,percentage point difference,False,NA,False,NA,False,NA
Registered Replication Report: Rand Greene and Nowak (2012),2017,10.1177/1745691617693624,Rachel Heyard,Social Sciences and Humanities,Economics,"One Phenomenon, Many Studies [Comment: Registered Replication Report]","This Registered Replication Report (RRR) assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate, preregistered replications of the critical conditions from Study 7 of the original article","False [Comment: No formal analysis, but refer to replication (and replicability)]",NA,Different data - same analysis,"Meta-analysis of original and replication study/studies [Comment: estimate the effect size meta-analytically across studies, and compare it with the original.]",False,NA,False,NA,False,NA,False,NA
The challenge of mapping the human connectome based on diffusion tractography,2017,10.1038/s41467-017-01285-x,Helena Hartmann,Life Sciences,Neuroscience,"One Phenomenon, Many Studies","systematically validate tractography studies, to assess how well the algorithms were able to reproduce the known connectivity, our study is mainly about accuracy with respect to the reference, rather than reproducibility or robustness of tractography.",False,NA,Same data - different analysis [Comment: one dataset which was differently analyzed],None of the above,True,"production of ground truth bundles, which were compared between studies, separated into valid and invalid bundles",False,NA,False,NA,False,NA
The challenge of mapping the human connectome based on diffusion tractography,2017,10.1038/s41467-017-01285-x,Rachel Heyard,Life Sciences,Medical imaging,"One Phenomenon, Many Studies",to assess how well the algorithms were able to reproduce the known connectivity.,"False [Comment: No formal definition, but refer to reproducibility]",NA,Same data - different analysis,None of the above,True,"Comparison of final results, as counts, proportions compared to ground truth (instead of original)",False,NA,False,NA,False,NA
Many Labs 2: Investigating Variation in Replicability Across Samples and Settings,2018,10.1177/2515245918810225,Joris Frese,Social Sciences and Humanities;Life Sciences,Psychology,"Many Phenomena, Many studies","""We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were  peer reviewed in advance, to examine variation in effect magnitudes across samples and settings"" (Abstract)",True,"""We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were  peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was  administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories"" (Abstract)","Different data - same analysis;Different data - different analysis [Comment: Mostly ""Different data - same analysis"",  but some slight changes in the analyses for a few replications]",(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""Across systematic replication efforts in the socialbehavioral sciences, there is accumulating evidence that  replication of published effects is less frequent than  might be expected, and that replication effect sizes are  typically smaller than original effect sizes (Camerer et al.,  2016; Camerer et al., 2018; Ebersole et al., 2016; Klein  et al., 2014a; Open Science Collaboration, 2015). For  example, Camerer et al. (2018) successfully replicated  13 of 21 social science studies published in Science and  Nature. Among the failures to replicate, the average  effect size was approximately 0, but even among the  successful replications, the average effect size was about  75% of what was observed in the original experiments.  Failures to replicate can be due to errors in the replication or to unanticipated moderation by changes in sample and setting, as we investigated in the project reported  here. They can also occur because of pervasive lowpowered research plus publication bias that favors positive over negative results (Button et al., 2013; Cohen,  1962; Greenwald, 1975; Rosenthal, 1979) and because of  questionable research practices, such as p-hacking, that  can inflate the likelihood of obtaining false positives  (John, Loewenstein, & Prelec, 2012; Simmons, Nelson,  & Simonsohn, 2011). These other reasons for failure to  replicate, which can also contribute to replication effect  sizes being weaker than those originally observed, were  not investigated directly in the present research."" (p.447)",True,"""Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized,  rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness  scores, respectively)."" (Abstract)"
Many Labs 2: Investigating Variation in Replicability Across Samples and Settings,2018,10.1177/2515245918810225,Rachel Heyard,Social Sciences and Humanities,social psychology;cognitive psychology,"Many Phenomena, Many studies","We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings.",False [Comment: Replicability without clear definition],NA,Different data - same analysis,"(1) Agreement in statistical significance [Comment: Significance criterion with alpha 0.05 and 0.001, and alpha 0.05 combined with different sample sizes](2) Agreement in effect size",False,NA,False,NA,False,NA,"False [Comment: Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively)]",NA
The Brazilian Reproducibility Initiative,2019,10.7554/eLife.41602,Helena Hartmann,Life Sciences,Biomedical science,"Many Phenomena, Many studies",to assess the reproducibility of findings in biomedical science published by researchers based in Brazil,True,"many ways to define a successful replication, all of which have caveats. Reproducibility of the general conclusions on the existence of an effect (e.g. two results finding a statistically significant difference in the same direction) might not be accompanied by reproducibility of the effect size; conversely, studies with effect sizes that are similar to each other might have different outcomes in significance tests, percentage of original studies with effect sizes falling within the 95% prediction interval of a meta-analysis of the three replications, include other ways to define reproducibility as secondary outcomes, such as the statistical significance of the pooled replication studies, the significance of the effect in a meta-analysis including the original result and replication attempts, and a statistical comparison between the pooled effect sizes of the replications and the original result",Other [Add as comment] [Comment: two results finding a statistically significant difference in the same direction],"None of the above [Comment: describes plans to do things, but not results yet]",False,NA,False,NA,False,NA,False,NA
The Brazilian Reproducibility Initiative,2019,10.7554/eLife.41602,Rachel Heyard,Life Sciences,Biomedical Science [Comment: Published by researchers based in Brazil],"Many Phenomena, Many studies","to estimate the level of reproducibility of biomedical science in Brazil, and to investigate what aspects of the published literature might help to predict whether a finding is reproducible.","False [Comment: No formal definition, but refer to reproducibility]",NA,Different data - same analysis,(1) Agreement in statistical significance [Comment: the statistical significance of the pooled replication studies](2) Meta-analysis of original and replication study/studies [Comment: the significance of the effect in a meta-analysis including the original result and replication attempts](3) Agreement in effect size [Comment: statistical comparison between the pooled effect sizes of the replications and the original result.],True,the primary outcome of our analysis will be the percentage of original studies with effect sizes falling within the 95% prediction interval of a meta-analysis of the three replications.,False,NA,True,"(1) Reproducibility of the general conclusions on the existence of an effect (e.g. two results finding a statistically significant difference in the same direction) might not be accompanied by reproducibility of the effect size; conversely, studies with effect sizes that are similar to each other might have different outcomes in significance tests. Moreover, if non-replication occurs, it is hard to judge whether the original study or the replication is closer to the true result. [Comment: Significance and effect size, referencing 10.1177/0956797614567341](2) if inter-laboratory variability is high, prediction intervals can be wide, leading a large amount of results to be considered “reproducible”. Thus, replication estimates obtained by these methods are likely to be optimistic. On the other hand, failed replications will be more likely to reflect true biases, errors or deficiencies in the original experiments. [Comment: Primary outcome: referencing 10.1177/1745691616646366](3) incomplete reporting in the original study might increase inter-laboratory variation and artificially improve our primary outcome. [Comment: Primary outcome]",False,NA
The REPRISE project: protocol for an evaluation of REProducibility and Replicability In Syntheses of Evidence,2021,10.1186/s13643-021-01670-0,Helena Hartmann,Social Sciences and Humanities,Health;Meta science,"Many Phenomena, Many studies","aim to explore various aspects relating to the transparency reproducibility and replicability of several components of systematic reviews with meta-analysis of the effects of health social behavioural and educational interventions;evaluate the completeness of reporting and sharing of review data, analytic code and other materials",True,"come up with the same answer to the same research question, obtain the same results when reanalysing the data collected in a study using the same computational steps and analytic code as the ori- ginal study",Different data - same analysis;Different data - different analysis,Meta-analysis of original and replication study/studies,True,indicators of transparency in the systematic reviews using descriptive statistics (e.g. frequency and percentage for categorical items and mean and standard deviation for continuous items);risk ratios with 95% confidence intervals to examine differences in percentages of each indicator;frequency and percentage for each response option;qualitative thematic analysis,False,NA,False,NA,True,"For the two reviews replicated by 15 teams, we will strive to select reviews with multiplicity of results in the included studies, diversity in the study characteristics, and diversity in the risk of bias in the included studies and extent of missing evidence (e.g. unpublished studies), and which match the content expertise of the majority of replicators."
The REPRISE project: protocol for an evaluation of REProducibility and Replicability In Syntheses of Evidence,2021,10.1186/s13643-021-01670-0,Rachel Heyard,Social Sciences and Humanities;Life Sciences,Evidence synthesis;Methodological research [Comment: In medicine and psychology],"Many Phenomena, One Study [Comment: One part of study 4 would be a ""one phenomena, many studies""]","How frequently methods are reported completely, and how often review data, analytic code and other materials (e.g. list of all citations screened, data collection forms) are made publicly available;Systematic reviewers’ views on sharing review data, analytic code and other materials and their understanding of and opinions about replication of reviews;The extent of variation in results when independently reproducing meta-analyses using the same computational steps and analytic code (if available) as used in the original review;The extent of variation in results when replicating the search, selection, data collection and analysis processes of an original review.",True,"reproducibility = ""evaluating the extent of variation in results when we independently reproduce meta-analyses using the same computational steps and analytic code (if available) as used in the original review.""; replicability = ""we will adopt the non-procedural definitions of replication advocated by Nosek and Errington [5] and Machery [6]; that is, replicators will not need to follow every single step exactly as reported in the original systematic review, but they will be constrained by the original review question and must avoid making changes to the methods and concepts that might be reasonably judged to violate an attempt to answer that question.""",Same data - same analysis;Different data - same analysis,"(1) Agreement in effect size [Comment: ""Results fully reproducible/replicable’ (i.e. no difference [with allowance for trivial discrepancies such as those due to computational algorithms] is observed between the original and recalculated meta-analytic effect estimate, its 95% confidence interval and inferences about heterogeneity reported in the original review). [...] We will calculate agreement between the original and recalculated meta-analytic effects, displayed using Bland-Altman plots, and tabulate discordance between P values for the meta-analytic effects, by categorising the P values based on commonly used levels of statistical significance, namely P < 0.01; 0.01 ≤ P < 0.05; 0.05 ≤ P < 0.1; P ≥ 0.1. [...] For the 30 systematic reviews replicated by one team each, we will assess agreement between the original and replicated review in the number of citations yielded from each database, in total and once duplicates were removed, by calculating the weighted Kappa statistic and percentage agreement""](2) Subjective assessment [Comment: will also independently specify whether they believe the observed difference between the original and recalculated summary estimate and its precision was meaningful, that is, would lead to a change in the interpretation of the results.]",False,NA,True,(1) https://doi.org/10.1016/S0140-6736(86)90837-8 [Comment: Agreement in effect size (Bland-Altman plot)](2) https://doi.org/10.2307/2529310 [Comment: Agreement in effect size (Kappa statistic)],False,NA,False,NA
Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty,2022,10.1073/pnas.2203150119,Joris Frese,Social Sciences and Humanities,Political Science; Sociology,"One Phenomenon, Many Studies","""This study explores how researchers’ analytical choices affect the reliability of scientiﬁc ﬁndings"" (Abstract)",True,"""We coordinated 161 researchers in 73 research teams and observed their research decisions as they used the same data to independently test the same prominent social science hypothesis"" (Abstract)",Same data - same analysis;Same data - different analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,False,NA,True,"""we note that the conclusions of this study were themselves derived from myriad seemingly minor (meta-)analytical decisions, just like those we observed among our analysts."" (p.6)",False,NA
Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty,2022,10.1073/pnas.2203150119,Rachel Heyard,Social Sciences and Humanities,Political sciences,"One Phenomenon, Many Studies",This study explores how researchers’ analytical choices affect the reliability of scientiﬁc ﬁndings.,"False [Comment: No formal definition, but refer to reliability]",NA,Same data - different analysis [Comment: used the same data to independently test the same prominent social science hypothesis],(1) Agreement in effect size(2) Agreement in statistical significance,True,Agreement in team's subjective conclusions,False,NA,False,NA,True,"First, we do not know the generalizability of our study to different topics, disciplines, or even datasets."
Predicting the replicability of social and behavioural science claims from the COVID-19 Preprint Replication Project with structured expert and novice groups,2024,10.31222/osf.io/xdsjf,Helena Hartmann,Social Sciences and Humanities,Social science;Behavioral science;Covid 19 research,"Many Phenomena, Many studies",eliciting predictions from experts or novices could accelerate credibility assessment and improve allocation of replication resources for important and uncertain findings. We elicited judgments from experts and novices on 100 claims from preprints about an emerging area of research (COVID-19 pandemic) using a new interactive structured elicitation protocol and we conducted 35 new replications,False,NA,"Different data - different analysis [Comment: Replications were defined as tests of the original claim using new data and an inferential test that was expected to be theoretically equivalent to what was reported in the preprint (Nosek & Errington, 2020).]",(1) Agreement in statistical significance(2) Agreement in effect size,True,Expert and novice prediction and accuracy,False,NA,False,NA,False,NA
Predicting the replicability of social and behavioural science claims from the COVID-19 Preprint Replication Project with structured expert and novice groups,2024,10.31222/osf.io/xdsjf,Joris Frese,Social Sciences and Humanities;Life Sciences,Interdisciplinary,"Many Phenomena, Many studies","""We elicited judgments from experts and novices on 100 claims from preprints  about an emerging area of research (COVID-19 pandemic) using a new interactive structured  elicitation protocol and we conducted 35 new replications."" (Abstract)",True,"""The process of replications was conducted following the same methodology used in the rest of the SCORE program and resulted in two ‘types’ depending on the source of the new  data. [...]  New data replications were those that focused on collecting new data as part of the replication  attempt [...] Secondary data replications were those that focused on identifying another existing data source  that was not the same as the one used in the original study"" (p.4)",Different data - same analysis,(1) Agreement in statistical significance(2) Agreement in effect size,False,NA,True,10.31235/osf.io/46mnb,False,NA,"True [Comment: DEI not discusses regarding replicability, but regarding the inclusion of less educated ""novices"" for the assessment of replicability.]","""if novices can match experts’ performance, the viable pool of assessors is much  larger, potentially enhancing efficiency and reducing the cost of eliciting predictions. In teaching  contexts, the findings suggest more class assignments, dissertations, and projects could be tailored  to offer not only educational opportunities for students but to also contribute to scientific  “credibility control,” with students becoming involved in research evaluation as part of their  training"" (p.14)"
